#!/usr/bin/env python3
"""
ğŸš€ ENHANCED MULTI-DATASET NOTEBOOK SETUP
========================================
Final setup script that integrates all improvements into the
SAMo_MultiDataset_Streamlined_CLEAN.ipynb workflow
"""

import os
import sys
from pathlib import Path
import subprocess

def print_banner():
    """Print setup banner"""
    print("ğŸš€ ENHANCED MULTI-DATASET NOTEBOOK SETUP")
    print("=" * 55)
    print("ğŸ¯ Goal: Transform simple notebook into robust scientific workflow")
    print("ğŸ“Š Baseline: 51.79% F1-macro â†’ Target: 60%+ F1-macro")
    print("ğŸ”¬ Features: Phase-based training + Scientific validation")
    print("=" * 55)

def verify_all_components():
    """Verify all components are in place"""
    print("\nğŸ” COMPONENT VERIFICATION")
    print("=" * 30)

    components = [
        # Core scripts
        ("notebooks/prepare_all_datasets.py", "Multi-dataset preparation script"),
        ("scripts/train_comprehensive_multidataset.sh", "Training wrapper script"),
        ("notebooks/scripts/train_deberta_local.py", "Main training script"),

        # Enhanced features
        ("notebooks/verify_emotion_alignment.py", "Emotion alignment validator"),
        ("notebooks/fallback_handler.py", "Fallback and error handler"),
        ("scripts/reproduce_baseline.py", "Baseline reproduction script"),
        ("scripts/phase_based_multidataset_training.py", "Phase-based training system"),
        ("scripts/validation_monitor.py", "Validation and monitoring system"),

        # This setup script
        ("setup_enhanced_notebook.py", "Setup coordination script")
    ]

    missing_components = []
    present_components = []

    for component_path, description in components:
        if os.path.exists(component_path):
            present_components.append((component_path, description))
            print(f"   âœ… {component_path}")
        else:
            missing_components.append((component_path, description))
            print(f"   âŒ {component_path}")

    print(f"\nğŸ“Š Component Status: {len(present_components)}/{len(components)} available")

    if missing_components:
        print(f"\nâš ï¸ Missing components:")
        for path, desc in missing_components:
            print(f"   - {path}: {desc}")
        return False

    print("âœ… All components verified!")
    return True

def create_execution_guide():
    """Create comprehensive execution guide"""
    print("\nğŸ“‹ CREATING EXECUTION GUIDE")
    print("=" * 35)

    guide_content = """# ğŸš€ ENHANCED MULTI-DATASET TRAINING GUIDE

## ğŸ“‹ QUICK START (Original Notebook Enhanced)

### Option A: Simple Execution (Enhanced Notebook Cells)
```bash
# Cell 1: Run enhanced data preparation
python notebooks/prepare_all_datasets.py

# Cell 2: Run comprehensive training
bash scripts/train_comprehensive_multidataset.sh

# Cell 3: Monitor progress
python scripts/validation_monitor.py
```

### Option B: Phase-Based Scientific Approach (RECOMMENDED)
```bash
# Step 1: Verify environment and data quality
python notebooks/fallback_handler.py

# Step 2: Reproduce baseline (optional but recommended)
python scripts/reproduce_baseline.py

# Step 3: Run phase-based multi-dataset training
python scripts/phase_based_multidataset_training.py

# Step 4: Comprehensive validation
python scripts/validation_monitor.py
```

## ğŸ”¬ SCIENTIFIC WORKFLOW

### Phase 1: Quick Exploration (2-3 hours)
- Tests 4 configurations: BCE, Asymmetric, Combined(0.7), Combined(0.5)
- Uses subset of data for rapid iteration
- Identifies best performing approaches

### Phase 2: Extended Training (4-6 hours)
- Full dataset training on top 2 configurations from Phase 1
- Extended epochs for better convergence
- Target: >60% F1-macro achievement

## ğŸ“Š EXPECTED RESULTS

| Configuration | Expected F1-Macro | Status |
|---------------|------------------|---------|
| Baseline (GoEmotions BCE) | 51.79% | âœ… Proven |
| Multi-dataset BCE | 55-58% | ğŸ¯ Conservative |
| Multi-dataset Combined | 58-62% | ğŸš€ Optimistic |
| Multi-dataset Asymmetric | 52-56% | ğŸ“ˆ Modest improvement |

## ğŸ” MONITORING & VALIDATION

### Real-time Monitoring
```bash
# System status
python scripts/validation_monitor.py

# Training logs
tail -f logs/train_comprehensive_multidataset.log

# GPU status
watch -n 5 'nvidia-smi'
```

### Quality Assurance
```bash
# Validate emotion mappings
python notebooks/verify_emotion_alignment.py

# Check dataset quality
python scripts/validation_monitor.py
```

## ğŸ›¡ï¸ ROBUST ERROR HANDLING

### Fallback Systems
1. **Missing datasets**: Automatic sample data generation
2. **Training failures**: Checkpoint recovery and restart
3. **GPU issues**: Single-GPU fallback mode
4. **Disk space**: Automatic cleanup of old checkpoints

### Debugging Tools
```bash
# Comprehensive system check
python notebooks/fallback_handler.py

# Training process monitoring
ps aux | grep train_deberta

# Log analysis for issues
grep -i error logs/*.log
```

## ğŸ¯ SUCCESS CRITERIA

### Primary Target: >60% F1-Macro
- **Excellent**: â‰¥60% (10x improvement over baseline)
- **Good**: 55-60% (6-16% improvement)
- **Acceptable**: 52-55% (1-6% improvement)

### Secondary Metrics
- F1-Micro â‰¥55%
- Balanced performance across emotion classes
- Stable training without stalls or crashes

## ğŸ“ OUTPUT STRUCTURE

```
project_root/
â”œâ”€â”€ checkpoints_comprehensive_multidataset/  # Main results
â”œâ”€â”€ outputs/phase1_*/                        # Phase 1 exploration results
â”œâ”€â”€ outputs/phase2_*/                        # Phase 2 extended results
â”œâ”€â”€ logs/                                     # All training logs
â”œâ”€â”€ *_report.json                           # Performance reports
â””â”€â”€ comprehensive_validation_report.json     # Full validation results
```

## ğŸš¨ TROUBLESHOOTING

### Common Issues
1. **CUDA OOM**: Reduce batch size to 2, increase gradient accumulation
2. **Training stall**: Check GPU utilization, restart if needed
3. **Poor performance**: Verify emotion alignment, check data quality
4. **Missing files**: Run fallback_handler.py to regenerate

### Support Resources
- Training logs: `logs/` directory
- System status: `python scripts/validation_monitor.py`
- Error handling: `python notebooks/fallback_handler.py`

## ğŸ‰ COMPLETION CHECKLIST

- [ ] Data preparation successful (>30K samples)
- [ ] Training completes without errors
- [ ] F1-macro â‰¥55% achieved
- [ ] Validation report generated
- [ ] Model artifacts saved
- [ ] Performance documented

---

**ğŸ”¬ This enhanced workflow transforms the simple 3-cell notebook into a robust, scientific, production-ready training pipeline while maintaining ease of use.**
"""

    with open("ENHANCED_TRAINING_GUIDE.md", "w") as f:
        f.write(guide_content)

    print("âœ… Comprehensive guide created: ENHANCED_TRAINING_GUIDE.md")

def create_notebook_patches():
    """Create patches for the original notebook"""
    print("\nğŸ”§ NOTEBOOK ENHANCEMENT PATCHES")
    print("=" * 40)

    # Enhanced Cell 2 (Data Preparation)
    enhanced_cell2 = '''# ENHANCED MULTI-DATASET PREPARATION WITH VALIDATION
import os
import subprocess
import sys

print("ğŸš€ ENHANCED MULTI-DATASET PREPARATION")
print("=" * 50)
print("ğŸ”¬ Features: Scientific validation + Robust error handling")
print("ğŸ“Š Datasets: GoEmotions + SemEval + ISEAR + MELD")
print("=" * 50)

# Step 1: Run fallback handler for robust setup
print("\\nğŸ›¡ï¸ Step 1: Robust Environment Setup")
result = subprocess.run([sys.executable, 'notebooks/fallback_handler.py'],
                       capture_output=True, text=True)
print(result.stdout)

# Step 2: Enhanced dataset preparation
print("\\nğŸ“Š Step 2: Enhanced Dataset Preparation")
result = subprocess.run([sys.executable, 'notebooks/prepare_all_datasets.py'],
                       capture_output=False, text=True)

# Step 3: Validation
print("\\nğŸ” Step 3: Dataset Quality Validation")
result = subprocess.run([sys.executable, 'notebooks/verify_emotion_alignment.py'],
                       capture_output=True, text=True)
print(result.stdout)

# Verify success
if os.path.exists('data/combined_all_datasets/train.jsonl'):
    train_count = sum(1 for line in open('data/combined_all_datasets/train.jsonl'))
    val_count = sum(1 for line in open('data/combined_all_datasets/val.jsonl'))
    print(f"\\nâœ… SUCCESS: {train_count + val_count} samples prepared")
    print(f"   Training: {train_count} samples")
    print(f"   Validation: {val_count} samples")
    print("\\nğŸš€ Ready for enhanced training! Run next cell.")
else:
    print("\\nâŒ FAILED: Dataset preparation unsuccessful")
    print("ğŸ’¡ Check logs and try again")'''

    # Enhanced Cell 4 (Training)
    enhanced_cell4 = '''# ENHANCED MULTI-DATASET TRAINING WITH MONITORING
import os
import subprocess
import sys
from pathlib import Path

print("ğŸš€ ENHANCED MULTI-DATASET TRAINING")
print("=" * 50)
print("ğŸ”¬ Features: Phase-based + Scientific validation + Monitoring")
print("ğŸ¯ Target: >60% F1-macro (vs 51.79% baseline)")
print("=" * 50)

# Change to project directory
os.chdir('/home/user/goemotions-deberta')

# Choose training approach
print("\\nğŸ“‹ TRAINING APPROACH OPTIONS:")
print("   A) Simple Training (Original approach, enhanced)")
print("   B) Phase-Based Training (Scientific approach, RECOMMENDED)")

approach = input("\\nChoose approach (A/B): ").upper()

if approach == 'B':
    print("\\nğŸ”¬ PHASE-BASED SCIENTIFIC TRAINING SELECTED")
    print("â±ï¸ Duration: ~6-8 hours (2 phases)")
    print("ğŸ“Š Methodology: Systematic configuration exploration + Extended training")

    # Run phase-based training
    result = subprocess.run([sys.executable, 'scripts/phase_based_multidataset_training.py'],
                           capture_output=False, text=True)

    # Check results
    if os.path.exists('multidataset_training_report.json'):
        import json
        with open('multidataset_training_report.json', 'r') as f:
            report = json.load(f)

        best_result = report.get('best_result', {})
        if best_result:
            f1_macro = best_result.get('f1_macro', 0)
            print(f"\\nğŸ† BEST RESULT: F1-macro = {f1_macro:.4f}")
            if f1_macro >= 0.60:
                print("ğŸ‰ TARGET ACHIEVED: >60% F1-macro!")
            elif f1_macro >= 0.55:
                print("âœ… EXCELLENT: >55% F1-macro achieved!")
            else:
                print("ğŸ“ˆ PROGRESS: Above baseline, consider further tuning")

else:
    print("\\nâš¡ SIMPLE TRAINING SELECTED")
    print("â±ï¸ Duration: ~3-4 hours")

    # Run simple training with monitoring
    import threading

    def run_monitoring():
        import time
        time.sleep(300)  # Start monitoring after 5 minutes
        while True:
            subprocess.run([sys.executable, 'scripts/validation_monitor.py'],
                          capture_output=True)
            time.sleep(1800)  # Monitor every 30 minutes

    # Start monitoring in background
    monitor_thread = threading.Thread(target=run_monitoring, daemon=True)
    monitor_thread.start()

    # Run training
    result = subprocess.run(['bash', 'scripts/train_comprehensive_multidataset.sh'],
                           capture_output=False, text=True)

print("\\nâœ… ENHANCED TRAINING COMPLETE!")
print("ğŸ“Š Check results: Run validation cell next")'''

    # Enhanced Cell 6 (Results)
    enhanced_cell6 = '''# ENHANCED RESULTS ANALYSIS WITH COMPREHENSIVE VALIDATION
import json
import sys
import subprocess
from pathlib import Path

print("ğŸ“Š COMPREHENSIVE RESULTS ANALYSIS")
print("=" * 50)

# Run comprehensive validation
print("ğŸ” Running comprehensive validation...")
result = subprocess.run([sys.executable, 'scripts/validation_monitor.py'],
                       capture_output=True, text=True)
print(result.stdout)

# Load baseline comparison
baseline = {
    'f1_macro': 0.5179,
    'f1_micro': 0.5975,
    'model': 'GoEmotions BCE (Published HF Model)'
}

print("\\nğŸ† BASELINE PERFORMANCE:")
print(f"   F1 Macro: {baseline['f1_macro']:.4f} ({baseline['f1_macro']*100:.1f}%)")
print(f"   Model: {baseline['model']}")

# Check multiple result locations
result_locations = [
    ("checkpoints_comprehensive_multidataset/eval_report.json", "Simple Training"),
    ("multidataset_training_report.json", "Phase-Based Training"),
    ("outputs/phase2_best_extended/eval_report.json", "Extended Training")
]

best_f1 = 0.0
best_config = None

for result_path, config_name in result_locations:
    if os.path.exists(result_path):
        try:
            with open(result_path, 'r') as f:
                data = json.load(f)

            if 'best_result' in data:
                # Phase-based training report
                f1_macro = data['best_result']['f1_macro']
                config_name = f"Phase-Based: {data['best_result']['config']}"
            else:
                # Direct evaluation report
                f1_macro = data.get('f1_macro', 0.0)

            if f1_macro > best_f1:
                best_f1 = f1_macro
                best_config = config_name

            print(f"\\nğŸ¯ {config_name.upper()}:")
            print(f"   F1-macro: {f1_macro:.4f} ({f1_macro*100:.1f}%)")

            improvement = ((f1_macro - baseline['f1_macro']) / baseline['f1_macro']) * 100
            print(f"   Improvement: {improvement:+.1f}% over baseline")

        except Exception as e:
            print(f"   âŒ Error reading {result_path}: {e}")

# Final assessment
print(f"\\nğŸ† BEST OVERALL RESULT:")
print(f"   Configuration: {best_config}")
print(f"   F1-macro: {best_f1:.4f}")

if best_f1 >= 0.60:
    print("\\nğŸ‰ OUTSTANDING SUCCESS!")
    print("   âœ… Target >60% F1-macro achieved!")
    print("   ğŸš€ Multi-dataset training HIGHLY SUCCESSFUL!")
elif best_f1 >= 0.55:
    print("\\nâœ… EXCELLENT SUCCESS!")
    print("   âœ… Strong >55% F1-macro achieved!")
    print("   ğŸ“ˆ Significant improvement from multi-dataset approach")
elif best_f1 > baseline['f1_macro']:
    print("\\nğŸ‘ SUCCESSFUL IMPROVEMENT!")
    print("   âœ… Beat baseline performance")
    print("   ğŸ”§ Consider extended training for even better results")
else:
    print("\\nâš ï¸ NEEDS INVESTIGATION")
    print("   ğŸ” Check data quality and training logs")
    print("   ğŸ’¡ Consider baseline reproduction first")

print(f"\\nğŸ“‹ TARGET ACHIEVEMENT:")
print(f"   >60% F1-macro: {'âœ…' if best_f1 >= 0.60 else 'âŒ'}")
print(f"   >55% F1-macro: {'âœ…' if best_f1 >= 0.55 else 'âŒ'}")
print(f"   Beat baseline: {'âœ…' if best_f1 > baseline['f1_macro'] else 'âŒ'}")

print(f"\\nğŸ“„ COMPREHENSIVE REPORT:")
print(f"   Available at: comprehensive_validation_report.json")'''

    # Save patches
    patches = {
        "enhanced_cell_2_data_preparation.py": enhanced_cell2,
        "enhanced_cell_4_training.py": enhanced_cell4,
        "enhanced_cell_6_results.py": enhanced_cell6
    }

    patch_dir = "notebook_patches"
    os.makedirs(patch_dir, exist_ok=True)

    for filename, content in patches.items():
        with open(f"{patch_dir}/{filename}", "w") as f:
            f.write(content)

    print(f"âœ… Notebook patches created in: {patch_dir}/")
    print("   Use these to enhance the original notebook cells")

def create_quick_start_script():
    """Create a quick start script for immediate use"""
    print("\nğŸš€ CREATING QUICK START SCRIPT")
    print("=" * 35)

    quick_start = '''#!/bin/bash

# ğŸš€ QUICK START: ENHANCED MULTI-DATASET TRAINING
# ===============================================

echo "ğŸš€ ENHANCED MULTI-DATASET TRAINING - QUICK START"
echo "================================================"
echo "ğŸ¯ Goal: Achieve >60% F1-macro using multi-dataset approach"
echo "ğŸ“Š Baseline: 51.79% F1-macro (GoEmotions BCE)"
echo "================================================"

# Step 1: Environment setup and validation
echo ""
echo "ğŸ”§ Step 1: Environment Setup & Validation"
echo "----------------------------------------"
python3 notebooks/fallback_handler.py

# Step 2: Dataset preparation
echo ""
echo "ğŸ“Š Step 2: Multi-Dataset Preparation"
echo "------------------------------------"
python3 notebooks/prepare_all_datasets.py

# Step 3: Emotion alignment validation
echo ""
echo "ğŸ” Step 3: Emotion Alignment Validation"
echo "---------------------------------------"
python3 notebooks/verify_emotion_alignment.py

# Step 4: Training approach selection
echo ""
echo "ğŸš€ Step 4: Training Approach Selection"
echo "--------------------------------------"
echo "Choose training approach:"
echo "  A) Simple Training (3-4 hours, good for testing)"
echo "  B) Phase-Based Training (6-8 hours, recommended for best results)"
echo ""

read -p "Enter choice (A/B): " choice

if [[ "$choice" == "B" || "$choice" == "b" ]]; then
    echo ""
    echo "ğŸ”¬ Running Phase-Based Scientific Training..."
    echo "â±ï¸ Duration: ~6-8 hours"
    python3 scripts/phase_based_multidataset_training.py
else
    echo ""
    echo "âš¡ Running Simple Training..."
    echo "â±ï¸ Duration: ~3-4 hours"
    bash scripts/train_comprehensive_multidataset.sh
fi

# Step 5: Comprehensive validation and results
echo ""
echo "ğŸ“Š Step 5: Comprehensive Results Analysis"
echo "-----------------------------------------"
python3 scripts/validation_monitor.py

echo ""
echo "âœ… ENHANCED MULTI-DATASET TRAINING COMPLETE!"
echo "ğŸ“„ Check comprehensive_validation_report.json for full results"
echo "ğŸ¯ Expected improvement: 51.79% â†’ 55-65% F1-macro"
'''

    with open("quick_start_multidataset.sh", "w") as f:
        f.write(quick_start)

    os.chmod("quick_start_multidataset.sh", 0o755)
    print("âœ… Quick start script created: quick_start_multidataset.sh")

def main():
    """Main setup execution"""
    print_banner()

    # Verify components
    if not verify_all_components():
        print("\nâŒ Setup incomplete - missing components detected")
        return False

    # Create documentation and guides
    create_execution_guide()
    create_notebook_patches()
    create_quick_start_script()

    # Final summary
    print("\nğŸ‰ ENHANCED NOTEBOOK SETUP COMPLETE!")
    print("=" * 45)
    print("âœ… All components installed and verified")
    print("âœ… Execution guide created")
    print("âœ… Notebook patches generated")
    print("âœ… Quick start script ready")

    print(f"\nğŸš€ NEXT STEPS:")
    print(f"   1. IMMEDIATE: Run ./quick_start_multidataset.sh")
    print(f"   2. NOTEBOOK: Use enhanced cells from notebook_patches/")
    print(f"   3. DOCUMENTATION: Read ENHANCED_TRAINING_GUIDE.md")

    print(f"\nğŸ¯ TRANSFORMATION COMPLETE:")
    print(f"   Original: Simple 3-cell notebook (prone to failure)")
    print(f"   Enhanced: Robust scientific pipeline (production-ready)")
    print(f"   Expected: 51.79% â†’ 60%+ F1-macro improvement")

    return True

if __name__ == "__main__":
    success = main()
    if success:
        print("\nğŸš€ Ready to achieve >60% F1-macro with multi-dataset training!")
    else:
        print("\nğŸ”§ Please address setup issues before proceeding.")