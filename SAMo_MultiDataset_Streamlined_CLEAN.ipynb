{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83c\udfaf SAMo Multi-Dataset Emotion Classification Pipeline\n",
        "\n",
        "## **COMPREHENSIVE MULTI-DATASET TRAINING WITH SCIENTIFIC LOGGING**\n",
        "\n",
        "**GOAL**: Achieve >60% F1-macro on combined GoEmotions, SemEval, ISEAR, and MELD datasets\n",
        "\n",
        "**FEATURES**:\n",
        "- \u2705 **Multi-Dataset Integration**: Combines 4 emotion datasets (70K+ samples)\n",
        "- \u2705 **Scientific Logging**: Comprehensive experiment tracking and metrics\n",
        "- \u2705 **Google Drive Backup**: Automatic backup of all results and logs\n",
        "- \u2705 **Robust Training**: Error handling, progress monitoring, and recovery\n",
        "- \u2705 **Clean Workflow**: Streamlined 2-step process\n",
        "\n",
        "**DATASETS**:\n",
        "- \ud83d\udcca **GoEmotions**: 48,836 samples (28 emotions)\n",
        "- \ud83d\udcca **SemEval**: 802 samples (11 emotions) \n",
        "- \ud83d\udcca **ISEAR**: 7,500 samples (7 emotions)\n",
        "- \ud83d\udcca **MELD**: 13,708 samples (7 emotions)\n",
        "\n",
        "**TOTAL**: 70,846+ samples across 28 emotion classes\n",
        "\n",
        "---\n",
        "\n",
        "## \ud83d\ude80 **QUICK START - 2 STEP WORKFLOW**\n",
        "\n",
        "1. **Cell 1**: Environment Setup & Data Preparation\n",
        "2. **Cell 2**: Training Execution with Live Progress\n",
        "\n",
        "**Expected Training Time**: 6-8 hours on 2x RTX 3090\n",
        "**Expected Performance**: >60% F1-macro (vs 51.79% baseline)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udd27 **STEP 1: ENVIRONMENT SETUP & DATA PREPARATION**\n",
        "\n",
        "**Run this cell first to:**\n",
        "- \u2705 Verify environment and dependencies\n",
        "- \u2705 Prepare combined multi-dataset\n",
        "- \u2705 Validate data integrity\n",
        "- \u2705 Set up logging and backup systems"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# \ud83d\udd27 STEP 1: ENVIRONMENT SETUP & DATA PREPARATION\n",
        "print(\"\ud83d\ude80 SAMo Multi-Dataset Pipeline - Environment Setup\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import subprocess\n",
        "import time\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "# Set working directory\n",
        "os.chdir('/workspace')\n",
        "print(f\"\ud83d\udcc1 Working directory: {os.getcwd()}\")\n",
        "\n",
        "# Environment verification\n",
        "print(\"\\n\ud83d\udd0d Verifying Environment...\")\n",
        "print(f\"Python: {sys.executable}\")\n",
        "print(f\"Version: {sys.version}\")\n",
        "\n",
        "# Check conda environment\n",
        "conda_env = os.environ.get('CONDA_DEFAULT_ENV', 'None')\n",
        "print(f\"Conda env: {conda_env}\")\n",
        "\n",
        "if conda_env != 'deberta-v3':\n",
        "    print(\"\u26a0\ufe0f  WARNING: Switch to 'Python (deberta-v3)' kernel for best results\")\n",
        "\n",
        "# Check critical packages\n",
        "print(\"\\n\ud83d\udce6 Checking Dependencies...\")\n",
        "try:\n",
        "    import torch\n",
        "    print(f\"\u2705 PyTorch {torch.__version__}\")\n",
        "    print(f\"   CUDA Available: {torch.cuda.is_available()}\")\n",
        "    print(f\"   GPU Count: {torch.cuda.device_count()}\")\n",
        "    if torch.cuda.is_available():\n",
        "        for i in range(torch.cuda.device_count()):\n",
        "            print(f\"   GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
        "except ImportError as e:\n",
        "    print(f\"\u274c PyTorch missing: {e}\")\n",
        "\n",
        "try:\n",
        "    import transformers\n",
        "    print(f\"\u2705 Transformers {transformers.__version__}\")\n",
        "except ImportError as e:\n",
        "    print(f\"\u274c Transformers missing: {e}\")\n",
        "\n",
        "try:\n",
        "    import sklearn\n",
        "    print(f\"\u2705 Scikit-learn {sklearn.__version__}\")\n",
        "except ImportError as e:\n",
        "    print(f\"\u274c Scikit-learn missing: {e}\")\n",
        "\n",
        "# Check GPU status\n",
        "print(\"\\n\ud83d\udda5\ufe0f  GPU Status:\")\n",
        "try:\n",
        "    result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total,memory.used', '--format=csv,noheader,nounits'], \n",
        "                          capture_output=True, text=True, timeout=10)\n",
        "    if result.returncode == 0:\n",
        "        print(result.stdout.strip())\n",
        "    else:\n",
        "        print(\"\u26a0\ufe0f  nvidia-smi not available\")\n",
        "except Exception as e:\n",
        "    print(f\"\u26a0\ufe0f  GPU check failed: {e}\")\n",
        "\n",
        "# Data preparation\n",
        "print(\"\\n\ud83d\udcca Preparing Multi-Dataset...\")\n",
        "print(\"   This will combine GoEmotions, SemEval, ISEAR, and MELD datasets\")\n",
        "\n",
        "# Check if data already exists\n",
        "data_dir = Path('data/combined')\n",
        "if data_dir.exists() and (data_dir / 'train.jsonl').exists() and (data_dir / 'val.jsonl').exists():\n",
        "    print(\"\u2705 Combined dataset already exists\")\n",
        "    \n",
        "    # Show dataset statistics\n",
        "    train_samples = sum(1 for _ in open(data_dir / 'train.jsonl'))\n",
        "    val_samples = sum(1 for _ in open(data_dir / 'val.jsonl'))\n",
        "    total_samples = train_samples + val_samples\n",
        "    \n",
        "    print(f\"   \ud83d\udcc8 Training samples: {train_samples:,}\")\n",
        "    print(f\"   \ud83d\udcc8 Validation samples: {val_samples:,}\")\n",
        "    print(f\"   \ud83d\udcc8 Total samples: {total_samples:,}\")\n",
        "    \n",
        "    # Load metadata\n",
        "    if (data_dir / 'metadata.json').exists():\n",
        "        with open(data_dir / 'metadata.json', 'r') as f:\n",
        "            metadata = json.load(f)\n",
        "        print(f\"   \ud83c\udfaf Emotion classes: {metadata.get('emotion_count', 'Unknown')}\")\n",
        "        print(f\"   \ud83d\udcca Datasets: {', '.join(metadata.get('datasets_included', []))}\")\n",
        "else:\n",
        "    print(\"\ud83d\udd04 Running data preparation...\")\n",
        "    \n",
        "    # Run data preparation script\n",
        "    start_time = time.time()\n",
        "    result = subprocess.run(['python3', 'prepare_datasets_minimal.py'], \n",
        "                          capture_output=True, text=True, timeout=300)\n",
        "    \n",
        "    if result.returncode == 0:\n",
        "        print(\"\u2705 Data preparation completed successfully\")\n",
        "        print(f\"\u23f1\ufe0f  Time taken: {time.time() - start_time:.1f} seconds\")\n",
        "        \n",
        "        # Show final statistics\n",
        "        if data_dir.exists():\n",
        "            train_samples = sum(1 for _ in open(data_dir / 'train.jsonl'))\n",
        "            val_samples = sum(1 for _ in open(data_dir / 'val.jsonl'))\n",
        "            total_samples = train_samples + val_samples\n",
        "            \n",
        "            print(f\"\\n\ud83d\udcca Final Dataset Statistics:\")\n",
        "            print(f\"   \ud83d\udcc8 Training samples: {train_samples:,}\")\n",
        "            print(f\"   \ud83d\udcc8 Validation samples: {val_samples:,}\")\n",
        "            print(f\"   \ud83d\udcc8 Total samples: {total_samples:,}\")\n",
        "    else:\n",
        "        print(f\"\u274c Data preparation failed:\")\n",
        "        print(f\"   Error: {result.stderr}\")\n",
        "        raise RuntimeError(\"Data preparation failed\")\n",
        "\n",
        "# Verify training scripts exist\n",
        "print(\"\\n\ud83d\udd0d Verifying Training Scripts...\")\n",
        "scripts = [\n",
        "    'prepare_datasets_minimal.py',\n",
        "    'train_multidataset_deberta.py',\n",
        "    'train_comprehensive_multidataset.sh',\n",
        "    'backup_to_gdrive.sh'\n",
        "]\n",
        "\n",
        "for script in scripts:\n",
        "    if Path(script).exists():\n",
        "        print(f\"\u2705 {script}\")\n",
        "    else:\n",
        "        print(f\"\u274c {script} missing\")\n",
        "\n",
        "# Create necessary directories\n",
        "print(\"\\n\ud83d\udcc1 Creating Directories...\")\n",
        "directories = ['outputs', 'logs', 'models']\n",
        "for dir_name in directories:\n",
        "    Path(dir_name).mkdir(exist_ok=True)\n",
        "    print(f\"\u2705 {dir_name}/\")\n",
        "\n",
        "print(\"\\n\ud83c\udf89 ENVIRONMENT SETUP COMPLETE!\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\u2705 Environment verified\")\n",
        "print(\"\u2705 Multi-dataset prepared\")\n",
        "print(\"\u2705 Training scripts ready\")\n",
        "print(\"\u2705 Directories created\")\n",
        "print(\"\\n\ud83d\ude80 Ready for training! Proceed to Cell 2.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83c\udfc3 **STEP 2: TRAINING EXECUTION WITH LIVE PROGRESS**\n",
        "\n",
        "**Run this cell to:**\n",
        "- \u2705 Start comprehensive multi-dataset training\n",
        "- \u2705 Monitor live progress with real-time metrics\n",
        "- \u2705 Enable automatic Google Drive backup\n",
        "- \u2705 Track scientific logs and experiment data\n",
        "\n",
        "**Expected Results:**\n",
        "- \ud83c\udfaf **Target F1-macro**: >60% (vs 51.79% baseline)\n",
        "- \u23f1\ufe0f **Training Time**: 6-8 hours\n",
        "- \ud83d\udcca **Final Dataset**: 70K+ samples across 28 emotions\n",
        "- \u2601\ufe0f **Backup**: Automatic Google Drive backup every 30 minutes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# \ud83c\udfc3 STEP 2: TRAINING EXECUTION WITH LIVE PROGRESS\n",
        "print(\"\ud83d\ude80 SAMo Multi-Dataset Training - Starting...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "import subprocess\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "# Training configuration\n",
        "EXPERIMENT_NAME = f\"MultiDataset_BCE_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "print(f\"\ud83d\udcca Experiment ID: {EXPERIMENT_NAME}\")\n",
        "print(f\"\u23f0 Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(f\"\ud83c\udfaf Target: >60% F1-macro (vs 51.79% baseline)\")\n",
        "print(f\"\u23f1\ufe0f  Expected duration: 6-8 hours\")\n",
        "print()\n",
        "\n",
        "# Verify data exists\n",
        "data_dir = Path('data/combined')\n",
        "if not (data_dir / 'train.jsonl').exists() or not (data_dir / 'val.jsonl').exists():\n",
        "    print(\"\u274c Combined dataset not found! Run Cell 1 first.\")\n",
        "    raise FileNotFoundError(\"Combined dataset not found\")\n",
        "\n",
        "# Show dataset statistics\n",
        "train_samples = sum(1 for _ in open(data_dir / 'train.jsonl'))\n",
        "val_samples = sum(1 for _ in open(data_dir / 'val.jsonl'))\n",
        "total_samples = train_samples + val_samples\n",
        "\n",
        "print(f\"\ud83d\udcca Dataset Statistics:\")\n",
        "print(f\"   \ud83d\udcc8 Training samples: {train_samples:,}\")\n",
        "print(f\"   \ud83d\udcc8 Validation samples: {val_samples:,}\")\n",
        "print(f\"   \ud83d\udcc8 Total samples: {total_samples:,}\")\n",
        "print(f\"   \ud83c\udfaf Emotion classes: 28\")\n",
        "print()\n",
        "\n",
        "# Check GPU availability\n",
        "print(\"\ud83d\udda5\ufe0f  GPU Status:\")\n",
        "try:\n",
        "    result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total,memory.used', '--format=csv,noheader,nounits'], \n",
        "                          capture_output=True, text=True, timeout=10)\n",
        "    if result.returncode == 0:\n",
        "        gpu_info = result.stdout.strip().split('\\n')\n",
        "        for i, gpu in enumerate(gpu_info):\n",
        "            print(f\"   GPU {i}: {gpu}\")\n",
        "    else:\n",
        "        print(\"   \u26a0\ufe0f  nvidia-smi not available\")\n",
        "except Exception as e:\n",
        "    print(f\"   \u26a0\ufe0f  GPU check failed: {e}\")\n",
        "\n",
        "print()\n",
        "print(\"\ud83c\udfc3 Starting Training...\")\n",
        "print(\"   \ud83d\udcdd Live progress will be shown below\")\n",
        "print(\"   \ud83d\udcca Metrics will be logged every 100 steps\")\n",
        "print(\"   \ud83d\udcbe Checkpoints saved every 1000 steps\")\n",
        "print(\"   \u2601\ufe0f  Google Drive backup every 30 minutes\")\n",
        "print(\"   \u23f1\ufe0f  Evaluation every 500 steps\")\n",
        "print()\n",
        "print(\"\" + \"=\"*60)\n",
        "print(\"\ud83c\udfaf LIVE TRAINING PROGRESS\")\n",
        "print(\"\" + \"=\"*60)\n",
        "\n",
        "# Start training with visible progress\n",
        "start_time = time.time()\n",
        "\n",
        "try:\n",
        "    # Run training script with live output\n",
        "    training_result = subprocess.run(\n",
        "        ['bash', 'train_comprehensive_multidataset.sh'],\n",
        "        capture_output=False,  # Show live progress\n",
        "        text=True,\n",
        "        timeout=28800  # 8 hour timeout\n",
        "    )\n",
        "    \n",
        "    training_time = time.time() - start_time\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"\ud83c\udf89 TRAINING COMPLETED!\")\n",
        "    print(\"\" + \"=\"*60)\n",
        "    print(f\"\u23f1\ufe0f  Total training time: {training_time/3600:.2f} hours\")\n",
        "    print(f\"\ud83d\udcca Exit code: {training_result.returncode}\")\n",
        "    \n",
        "    if training_result.returncode == 0:\n",
        "        print(\"\u2705 Training completed successfully!\")\n",
        "        \n",
        "        # Check for results\n",
        "        output_dir = Path('outputs/multidataset')\n",
        "        if output_dir.exists():\n",
        "            print(f\"\\n\ud83d\udcc1 Model saved to: {output_dir}\")\n",
        "            \n",
        "            # List saved files\n",
        "            model_files = list(output_dir.glob('*'))\n",
        "            if model_files:\n",
        "                print(\"   \ud83d\udcc4 Saved files:\")\n",
        "                for file in model_files:\n",
        "                    print(f\"      - {file.name}\")\n",
        "        \n",
        "        # Check logs\n",
        "        log_dir = Path('logs')\n",
        "        if log_dir.exists():\n",
        "            log_files = list(log_dir.glob('*.log'))\n",
        "            if log_files:\n",
        "                print(f\"\\n\ud83d\udcca Logs saved to: {log_dir}\")\n",
        "                print(\"   \ud83d\udcc4 Log files:\")\n",
        "                for log_file in log_files:\n",
        "                    print(f\"      - {log_file.name}\")\n",
        "        \n",
        "        # Check Google Drive backup\n",
        "        print(\"\\n\u2601\ufe0f  Google Drive Backup:\")\n",
        "        print(\"   \ud83d\udce4 Backup completed automatically\")\n",
        "        print(\"   \ud83d\udcc1 Location: drive:00_Projects/\ud83c\udfaf TechLabs-2025/Final_Project/TRAINING/\")\n",
        "        \n",
        "        print(\"\\n\ud83c\udfaf NEXT STEPS:\")\n",
        "        print(\"   1. Check final F1-macro score in logs\")\n",
        "        print(\"   2. Verify model performance >60% F1-macro\")\n",
        "        print(\"   3. Download model from Google Drive if needed\")\n",
        "        print(\"   4. Use model for inference on new data\")\n",
        "        \n",
        "    else:\n",
        "        print(f\"\u274c Training failed with exit code: {training_result.returncode}\")\n",
        "        print(\"\\n\ud83d\udd0d Troubleshooting:\")\n",
        "        print(\"   1. Check logs/train_comprehensive_multidataset.log\")\n",
        "        print(\"   2. Verify GPU memory availability\")\n",
        "        print(\"   3. Check disk space\")\n",
        "        print(\"   4. Review error messages above\")\n",
        "        \n",
        "except subprocess.TimeoutExpired:\n",
        "    print(\"\\n\u23f0 Training timeout (8 hours exceeded)\")\n",
        "    print(\"   \ud83d\udcca Training may still be running in background\")\n",
        "    print(\"   \ud83d\udd0d Check logs for progress\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\\n\u274c Training error: {e}\")\n",
        "    print(\"   \ud83d\udd0d Check error messages above\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"\ud83c\udfc1 Training session ended at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(\"\" + \"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udcca **RESULTS ANALYSIS & MONITORING**\n",
        "\n",
        "**Use this cell to:**\n",
        "- \u2705 Monitor training progress in real-time\n",
        "- \u2705 Analyze final results and metrics\n",
        "- \u2705 Check Google Drive backup status\n",
        "- \u2705 Verify model performance against baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# \ud83d\udcca RESULTS ANALYSIS & MONITORING\n",
        "print(\"\ud83d\udcca SAMo Multi-Dataset Results Analysis\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import subprocess\n",
        "\n",
        "# Check training logs\n",
        "print(\"\ud83d\udd0d Checking Training Logs...\")\n",
        "log_file = Path('logs/train_comprehensive_multidataset.log')\n",
        "if log_file.exists():\n",
        "    print(f\"\u2705 Training log found: {log_file}\")\n",
        "    \n",
        "    # Read last few lines for recent progress\n",
        "    with open(log_file, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "    \n",
        "    print(\"\\n\ud83d\udcc8 Recent Training Progress:\")\n",
        "    for line in lines[-10:]:  # Last 10 lines\n",
        "        if 'f1_macro' in line.lower() or 'eval_f1' in line.lower():\n",
        "            print(f\"   {line.strip()}\")\n",
        "else:\n",
        "    print(\"\u26a0\ufe0f  Training log not found\")\n",
        "\n",
        "# Check scientific logs\n",
        "print(\"\\n\ud83d\udd2c Checking Scientific Logs...\")\n",
        "logs_dir = Path('logs')\n",
        "experiment_dirs = [d for d in logs_dir.iterdir() if d.is_dir() and 'MultiDataset_BCE' in d.name]\n",
        "\n",
        "if experiment_dirs:\n",
        "    latest_experiment = max(experiment_dirs, key=lambda x: x.stat().st_mtime)\n",
        "    print(f\"\u2705 Latest experiment: {latest_experiment.name}\")\n",
        "    \n",
        "    # Check evaluation log\n",
        "    eval_log = latest_experiment / 'evaluation_log.json'\n",
        "    if eval_log.exists():\n",
        "        with open(eval_log, 'r') as f:\n",
        "            eval_data = json.load(f)\n",
        "        \n",
        "        print(f\"\\n\ud83d\udcca Evaluation Results:\")\n",
        "        for entry in eval_data[-3:]:  # Last 3 evaluations\n",
        "            metrics = entry.get('metrics', {})\n",
        "            epoch = entry.get('epoch', 'Unknown')\n",
        "            print(f\"   Epoch {epoch}:\")\n",
        "            for metric, value in metrics.items():\n",
        "                print(f\"      {metric}: {value:.4f}\")\n",
        "    \n",
        "    # Check experiment summary\n",
        "    summary_file = latest_experiment / 'experiment_summary.json'\n",
        "    if summary_file.exists():\n",
        "        with open(summary_file, 'r') as f:\n",
        "            summary = json.load(f)\n",
        "        \n",
        "        print(f\"\\n\ud83d\udccb Experiment Summary:\")\n",
        "        print(f\"   \ud83c\udd94 ID: {summary.get('experiment_id', 'Unknown')}\")\n",
        "        print(f\"   \ud83d\udcc5 Timestamp: {summary.get('timestamp', 'Unknown')}\")\n",
        "        print(f\"   \ud83d\udcc8 Training steps: {summary.get('total_training_steps', 'Unknown')}\")\n",
        "        print(f\"   \ud83d\udcca Evaluations: {summary.get('total_evaluations', 'Unknown')}\")\n",
        "        \n",
        "        final_metrics = summary.get('final_metrics')\n",
        "        if final_metrics:\n",
        "            print(f\"\\n\ud83c\udfaf Final Metrics:\")\n",
        "            for metric, value in final_metrics.items():\n",
        "                print(f\"   {metric}: {value:.4f}\")\n",
        "                \n",
        "            # Check against baseline\n",
        "            f1_macro = final_metrics.get('f1_macro', 0)\n",
        "            baseline = 0.5179  # 51.79% baseline\n",
        "            if f1_macro > baseline:\n",
        "                improvement = ((f1_macro - baseline) / baseline) * 100\n",
        "                print(f\"\\n\ud83c\udf89 SUCCESS! F1-macro improved by {improvement:.1f}% over baseline\")\n",
        "            else:\n",
        "                print(f\"\\n\u26a0\ufe0f  F1-macro ({f1_macro:.4f}) below baseline ({baseline:.4f})\")\n",
        "\n",
        "else:\n",
        "    print(\"\u26a0\ufe0f  No experiment logs found\")\n",
        "\n",
        "# Check model outputs\n",
        "print(\"\\n\ud83e\udd16 Checking Model Outputs...\")\n",
        "output_dir = Path('outputs/multidataset')\n",
        "if output_dir.exists():\n",
        "    print(f\"\u2705 Model output directory: {output_dir}\")\n",
        "    \n",
        "    # List model files\n",
        "    model_files = list(output_dir.glob('*'))\n",
        "    if model_files:\n",
        "        print(\"   \ud83d\udcc4 Model files:\")\n",
        "        for file in model_files:\n",
        "            size_mb = file.stat().st_size / (1024 * 1024) if file.is_file() else 0\n",
        "            print(f\"      - {file.name} ({size_mb:.1f} MB)\")\n",
        "    else:\n",
        "        print(\"   \u26a0\ufe0f  No model files found\")\n",
        "else:\n",
        "    print(\"\u26a0\ufe0f  Model output directory not found\")\n",
        "\n",
        "# Check Google Drive backup\n",
        "print(\"\\n\u2601\ufe0f  Checking Google Drive Backup...\")\n",
        "try:\n",
        "    result = subprocess.run(['rclone', 'ls', 'drive:00_Projects/\ud83c\udfaf TechLabs-2025/Final_Project/TRAINING/'], \n",
        "                          capture_output=True, text=True, timeout=30)\n",
        "    if result.returncode == 0:\n",
        "        print(\"\u2705 Google Drive accessible\")\n",
        "        \n",
        "        # Look for our experiment\n",
        "        backup_lines = result.stdout.strip().split('\\n')\n",
        "        multidataset_backups = [line for line in backup_lines if 'MultiDataset_BCE' in line]\n",
        "        \n",
        "        if multidataset_backups:\n",
        "            print(f\"   \ud83d\udce4 Found {len(multidataset_backups)} MultiDataset backups:\")\n",
        "            for backup in multidataset_backups[-3:]:  # Last 3\n",
        "                print(f\"      - {backup}\")\n",
        "        else:\n",
        "            print(\"   \u26a0\ufe0f  No MultiDataset backups found\")\n",
        "    else:\n",
        "        print(\"\u26a0\ufe0f  Google Drive not accessible\")\n",
        "except Exception as e:\n",
        "    print(f\"\u26a0\ufe0f  Google Drive check failed: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"\ud83d\udcca Analysis Complete!\")\n",
        "print(\"\" + \"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udd27 **TROUBLESHOOTING & MAINTENANCE**\n",
        "\n",
        "**Use this cell for:**\n",
        "- \u2705 System diagnostics and health checks\n",
        "- \u2705 Cleanup and maintenance tasks\n",
        "- \u2705 Manual backup and recovery\n",
        "- \u2705 Performance optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# \ud83d\udd27 TROUBLESHOOTING & MAINTENANCE\n",
        "print(\"\ud83d\udd27 SAMo Multi-Dataset Troubleshooting\")\n",
        "print(\"=\" * 45)\n",
        "\n",
        "import shutil\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "# System diagnostics\n",
        "print(\"\ud83d\udda5\ufe0f  System Diagnostics:\")\n",
        "\n",
        "# Check disk space\n",
        "try:\n",
        "    disk_usage = shutil.disk_usage('/')\n",
        "    free_gb = disk_usage.free / (1024**3)\n",
        "    used_percent = (disk_usage.used / disk_usage.total) * 100\n",
        "    \n",
        "    print(f\"   \ud83d\udcbe Disk space: {free_gb:.1f}GB free, {used_percent:.1f}% used\")\n",
        "    \n",
        "    if free_gb < 10:\n",
        "        print(\"   \u26a0\ufe0f  WARNING: Low disk space!\")\n",
        "    elif used_percent > 85:\n",
        "        print(\"   \u26a0\ufe0f  WARNING: High disk usage!\")\n",
        "    else:\n",
        "        print(\"   \u2705 Disk space OK\")\n",
        "except Exception as e:\n",
        "    print(f\"   \u274c Disk check failed: {e}\")\n",
        "\n",
        "# Check GPU memory\n",
        "print(\"\\n\ud83c\udfae GPU Memory Status:\")\n",
        "try:\n",
        "    result = subprocess.run(['nvidia-smi', '--query-gpu=memory.total,memory.used,memory.free', '--format=csv,noheader,nounits'], \n",
        "                          capture_output=True, text=True, timeout=10)\n",
        "    if result.returncode == 0:\n",
        "        gpu_memory = result.stdout.strip().split('\\n')\n",
        "        for i, memory in enumerate(gpu_memory):\n",
        "            total, used, free = memory.split(', ')\n",
        "            used_percent = (int(used) / int(total)) * 100\n",
        "            print(f\"   GPU {i}: {used}MB/{total}MB used ({used_percent:.1f}%)\")\n",
        "            \n",
        "            if used_percent > 90:\n",
        "                print(f\"      \u26a0\ufe0f  WARNING: High GPU memory usage!\")\n",
        "            elif used_percent > 70:\n",
        "                print(f\"      \u26a0\ufe0f  GPU memory getting full\")\n",
        "            else:\n",
        "                print(f\"      \u2705 GPU memory OK\")\n",
        "    else:\n",
        "        print(\"   \u26a0\ufe0f  nvidia-smi not available\")\n",
        "except Exception as e:\n",
        "    print(f\"   \u274c GPU check failed: {e}\")\n",
        "\n",
        "# Check running processes\n",
        "print(\"\\n\ud83d\udd04 Running Processes:\")\n",
        "try:\n",
        "    result = subprocess.run(['ps', 'aux'], capture_output=True, text=True, timeout=10)\n",
        "    if result.returncode == 0:\n",
        "        python_processes = [line for line in result.stdout.split('\\n') if 'python' in line and 'train' in line]\n",
        "        if python_processes:\n",
        "            print(\"   \ud83d\udd04 Active training processes:\")\n",
        "            for process in python_processes[:3]:  # Show first 3\n",
        "                print(f\"      {process.strip()}\")\n",
        "        else:\n",
        "            print(\"   \u2705 No active training processes\")\n",
        "    else:\n",
        "        print(\"   \u274c Process check failed\")\n",
        "except Exception as e:\n",
        "    print(f\"   \u274c Process check failed: {e}\")\n",
        "\n",
        "# Cleanup options\n",
        "print(\"\\n\ud83e\uddf9 Cleanup Options:\")\n",
        "print(\"   \ud83d\udcc1 Check large directories:\")\n",
        "\n",
        "directories_to_check = ['outputs', 'logs', 'models', 'data']\n",
        "for dir_name in directories_to_check:\n",
        "    dir_path = Path(dir_name)\n",
        "    if dir_path.exists():\n",
        "        total_size = sum(f.stat().st_size for f in dir_path.rglob('*') if f.is_file())\n",
        "        size_gb = total_size / (1024**3)\n",
        "        print(f\"      {dir_name}/: {size_gb:.2f}GB\")\n",
        "        \n",
        "        if size_gb > 5:\n",
        "            print(f\"         \u26a0\ufe0f  Large directory - consider cleanup\")\n",
        "    else:\n",
        "        print(f\"      {dir_name}/: Not found\")\n",
        "\n",
        "# Manual backup\n",
        "print(\"\\n\u2601\ufe0f  Manual Backup:\")\n",
        "print(\"   To manually backup to Google Drive:\")\n",
        "print(\"   ```bash\")\n",
        "print(\"   bash backup_to_gdrive.sh\")\n",
        "print(\"   ```\")\n",
        "\n",
        "# Performance tips\n",
        "print(\"\\n\u26a1 Performance Tips:\")\n",
        "print(\"   \u2022 Use smaller batch size if GPU memory is low\")\n",
        "print(\"   \u2022 Reduce max_length if training is slow\")\n",
        "print(\"   \u2022 Enable gradient checkpointing for memory efficiency\")\n",
        "print(\"   \u2022 Use mixed precision training (fp16)\")\n",
        "print(\"   \u2022 Monitor GPU utilization with nvidia-smi\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*45)\n",
        "print(\"\ud83d\udd27 Troubleshooting Complete!\")\n",
        "print(\"\" + \"=\"*45)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (deberta-v3)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}