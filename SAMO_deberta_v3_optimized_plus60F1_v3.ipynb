{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aGJtMEW7N7d0"
   },
   "source": [
    "# SAMO DeBERTa-v3-Large Optimized Training\n",
    "## Target: >60% F1 Macro on GoEmotions\n",
    "\n",
    "### Key Optimizations:\n",
    "- ✅ Correct learning rate for DeBERTa (5e-6 vs 3e-4)\n",
    "- ✅ Better bias initialization strategy\n",
    "- ✅ Increased LoRA capacity (rank 64)\n",
    "- ✅ More training data (90% subset)\n",
    "- ✅ Gradient checkpointing for memory efficiency\n",
    "- ✅ Improved ASL parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Torch: 2.5.1+cu124\n",
      "🧪 CUDA available: True\n",
      "🖥️ GPUs: 2\n",
      " • 0: NVIDIA GeForce RTX 3090 | 25.4 GB | CC 8.6\n",
      " • 1: NVIDIA GeForce RTX 3090 | 25.4 GB | CC 8.6\n",
      "🔗 GPU0<->GPU1 P2P: False\n",
      "📦 /dev/shm size: 41.0 GiB\n"
     ]
    }
   ],
   "source": [
    "# ===== A) PRE-FLIGHT SANITY (run first) =====\n",
    "import os, shutil, subprocess, sys\n",
    "import torch\n",
    "\n",
    "# Safer defaults for single-node, PCIe multi-GPU (2x3090)\n",
    "os.environ.setdefault(\"CUDA_DEVICE_ORDER\", \"PCI_BUS_ID\")\n",
    "os.environ.setdefault(\"CUDA_VISIBLE_DEVICES\", \"0,1\")  # use both GPUs\n",
    "os.environ.setdefault(\"NCCL_DEBUG\", \"WARN\")\n",
    "os.environ.setdefault(\"NCCL_ASYNC_ERROR_HANDLING\", \"1\")\n",
    "# No Infiniband on Vast.ai by default; disable to avoid NCCL picking it\n",
    "os.environ.setdefault(\"NCCL_IB_DISABLE\", \"1\")\n",
    "# Start with P2P enabled; if you still crash, set to \"1\" in Section D\n",
    "os.environ.setdefault(\"NCCL_P2P_DISABLE\", \"0\")\n",
    "# Helps when container isn't launched with --ipc=host (common on Vast)\n",
    "os.environ.setdefault(\"NCCL_SHM_DISABLE\", \"0\")\n",
    "\n",
    "print(\"🔧 Torch:\", torch.__version__)\n",
    "print(\"🧪 CUDA available:\", torch.cuda.is_available())\n",
    "print(\"🖥️ GPUs:\", torch.cuda.device_count())\n",
    "\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    props = torch.cuda.get_device_properties(i)\n",
    "    print(f\" • {i}: {props.name} | {props.total_memory/1e9:.1f} GB | CC {props.major}.{props.minor}\")\n",
    "\n",
    "# Check basic P2P capability\n",
    "if torch.cuda.device_count() >= 2:\n",
    "    p2p = torch.cuda.can_device_access_peer(0, 1) and torch.cuda.can_device_access_peer(1, 0)\n",
    "    print(\"🔗 GPU0<->GPU1 P2P:\", p2p)\n",
    "else:\n",
    "    print(\"⚠️ Only one GPU visible. Check your Vast.ai template env.\")\n",
    "\n",
    "# Optional: quick /dev/shm heads-up (use --shm-size=16g for dataloaders)\n",
    "try:\n",
    "    shm = shutil.disk_usage(\"/dev/shm\").total / (1024**3)\n",
    "    print(f\"📦 /dev/shm size: {shm:.1f} GiB\")\n",
    "except Exception:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration already exists at /workspace/.cache/huggingface/accelerate/default_config.yaml, will not override. Run `accelerate config` manually or pass a different `save_location`.\n"
     ]
    }
   ],
   "source": [
    "# ===== B) DDP LAUNCHER (import once) =====\n",
    "from accelerate import notebook_launcher\n",
    "from accelerate.utils import write_basic_config\n",
    "\n",
    "\n",
    "# Ensure a valid default accelerate config exists (safe on re-run)\n",
    "write_basic_config(mixed_precision=\"fp16\") # 3090 works well in fp16; switch to \"bf16\" if desired\n",
    "\n",
    "\n",
    "def ddp_launch(fn, num_procs=None):\n",
    "    import torch\n",
    "    n = num_procs or torch.cuda.device_count() or 1\n",
    "    print(f\"🚀 Spawning {n} DDP processes...\")\n",
    "    notebook_launcher(fn, args=(), num_processes=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "Sk-GzbX9OK4j",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 SAMO Recovery Diagnostic\n",
      "==================================================\n",
      "\n",
      "🎮 GPU INFORMATION:\n",
      "   GPU: NVIDIA GeForce RTX 3090\n",
      "   Total Memory: 25.4GB\n",
      "   Hardware Type: High-end\n",
      "   Recommended Batch Size: 20\n",
      "\n",
      "🧹 MEMORY CLEANUP:\n"
     ]
    }
   ],
   "source": [
    "# SAMO Recovery Plan: Step 1 - Hardware Diagnostic & Cleanup\n",
    "# Run this BEFORE attempting training to identify hardware/memory issues\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "print(\"🔍 SAMO Recovery Diagnostic\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. GPU Hardware Check\n",
    "print(\"\\n🎮 GPU INFORMATION:\")\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"   GPU: {gpu_name}\")\n",
    "    print(f\"   Total Memory: {gpu_memory:.1f}GB\")\n",
    "\n",
    "    # Check if it's T4 vs 3090\n",
    "    if \"T4\" in gpu_name:\n",
    "        recommended_batch = 8\n",
    "        hardware_type = \"T4\"\n",
    "    elif \"3090\" in gpu_name or \"A100\" in gpu_name:\n",
    "        recommended_batch = 20\n",
    "        hardware_type = \"High-end\"\n",
    "    else:\n",
    "        recommended_batch = 12\n",
    "        hardware_type = \"Unknown\"\n",
    "\n",
    "    print(f\"   Hardware Type: {hardware_type}\")\n",
    "    print(f\"   Recommended Batch Size: {recommended_batch}\")\n",
    "else:\n",
    "    print(\"   ❌ No CUDA GPU available!\")\n",
    "    exit()\n",
    "\n",
    "# 2. Memory Cleanup\n",
    "print(\"\\n🧹 MEMORY CLEANUP:\")\n",
    "# Clear Python objects\n",
    "gc.collect()\n",
    "\n",
    "# Clear CUDA cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    allocated_before = torch.cuda.memory_allocated() / 1e9\n",
    "    reserved_before = torch.cuda.memory_reserved() / 1e9\n",
    "    print(f\"   GPU Memory Allocated: {allocated_before:.2f}GB\")\n",
    "    print(f\"   GPU Memory Reserved: {reserved_before:.2f}GB\")\n",
    "\n",
    "# Check system RAM\n",
    "ram = psutil.virtual_memory()\n",
    "print(f\"   System RAM: {ram.available / 1e9:.1f}GB available / {ram.total / 1e9:.1f}GB total\")\n",
    "\n",
    "# 3. Environment Check\n",
    "print(\"\\n🔧 ENVIRONMENT:\")\n",
    "print(f\"   PyTorch: {torch.__version__}\")\n",
    "print(f\"   CUDA Version: {torch.version.cuda}\")\n",
    "print(f\"   Visible Devices: {os.environ.get('CUDA_VISIBLE_DEVICES', 'All')}\")\n",
    "\n",
    "# 4. Previous Training Issue Analysis\n",
    "print(\"\\n⚠️  FAILURE ANALYSIS:\")\n",
    "print(\"   Previous failure indicators:\")\n",
    "print(\"   - Training speed: 0.61 it/s (abnormally slow)\")\n",
    "print(\"   - F1 score: 6.97% (model collapse)\")\n",
    "print(\"   - Learning rate: 5e-6 (likely too low for LoRA)\")\n",
    "\n",
    "# 5. Recovery Recommendations\n",
    "print(\"\\n✅ RECOVERY RECOMMENDATIONS:\")\n",
    "print(f\"   - Use batch_size: {recommended_batch} (for {hardware_type} GPU)\")\n",
    "print(\"   - Learning rate: 1e-4 (conservative middle ground)\")\n",
    "print(\"   - LoRA rank: 32 (proven working configuration)\")\n",
    "print(\"   - Data subset: 70% (between working 60% and failed 90%)\")\n",
    "print(\"   - Gradient checkpointing: True (for memory)\")\n",
    "\n",
    "# 6. Hardware-Specific Configuration\n",
    "RECOVERY_CONFIG = {\n",
    "    \"hardware_type\": hardware_type,\n",
    "    \"gpu_memory_gb\": gpu_memory,\n",
    "    \"recommended_batch_size\": recommended_batch,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"lora_r\": 32,\n",
    "    \"lora_alpha\": 64,\n",
    "    \"subset_ratio\": 0.70,\n",
    "    \"num_epochs\": 3,\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"fp16\": True,\n",
    "}\n",
    "\n",
    "print(f\"\\n📋 RECOMMENDED CONFIG:\")\n",
    "for key, value in RECOVERY_CONFIG.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "print(\"\\n🎯 NEXT STEPS:\")\n",
    "print(\"1. Use the RECOVERY_CONFIG above in your training\")\n",
    "print(\"2. Run 1 epoch first as validation\")\n",
    "print(\"3. Monitor F1 score - should be >30% after epoch 1\")\n",
    "print(\"4. If successful, continue with full training\")\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "\n",
    "# Export config for next cell\n",
    "globals()['RECOVERY_CONFIG'] = RECOVERY_CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-K74lFTWN7d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Cache directory: /workspace/.cache/huggingface\n",
      "📦 Installing required packages...\n",
      "\n",
      "🔥 PyTorch: 2.5.1+cu124\n",
      "🎮 CUDA available: True\n",
      "   GPU: NVIDIA GeForce RTX 3090\n",
      "   Memory: 25.4GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Cell 1: Environment Setup\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set environment variables BEFORE importing torch\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
    "\n",
    "# Determine cache directory\n",
    "if os.path.exists(\"/kaggle\"):\n",
    "    cache_dir = \"/kaggle/working/hf_cache\"\n",
    "elif os.path.exists(\"/workspace\"):\n",
    "    cache_dir = \"/workspace/.cache/huggingface\"\n",
    "else:\n",
    "    cache_dir = \"./hf_cache\"\n",
    "\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "os.environ[\"HF_HOME\"] = cache_dir\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = cache_dir\n",
    "os.environ[\"DATASETS_CACHE\"] = cache_dir\n",
    "\n",
    "print(f\"📁 Cache directory: {cache_dir}\")\n",
    "\n",
    "# Install packages\n",
    "def pip_install(packages):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + packages)\n",
    "\n",
    "print(\"📦 Installing required packages...\")\n",
    "pip_install([\n",
    "    \"transformers==4.41.2\",\n",
    "    \"datasets==2.19.0\",\n",
    "    \"accelerate==0.31.0\",\n",
    "    \"peft==0.10.0\",\n",
    "    \"evaluate==0.4.2\",\n",
    "    \"scikit-learn==1.5.0\",\n",
    "    \"sentencepiece>=0.1.99\",\n",
    "    \"tokenizers>=0.15.2\"\n",
    "])\n",
    "\n",
    "import torch\n",
    "print(f\"\\n🔥 PyTorch: {torch.__version__}\")\n",
    "print(f\"🎮 CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NwCcT856N7d0"
   },
   "outputs": [],
   "source": [
    "# Cell 2: RECOVERY Configuration (Conservative & Stable)\n",
    "# This replaces the failed \"optimized\" config with proven stable settings\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# RECOVERY CONFIGURATION - Based on what worked + conservative improvements\n",
    "CONFIG = {\n",
    "    \"seed\": 42,\n",
    "    \"output_dir\": \"./samo_deberta_recovery\",\n",
    "\n",
    "    # DATA - Conservative increase from working 60%\n",
    "    \"use_subset\": True,\n",
    "    \"subset_ratio\": 0.70,  # Between working 60% and failed 90%\n",
    "\n",
    "    # MODEL\n",
    "    \"model_name\": \"microsoft/deberta-v3-large\",\n",
    "    \"gradient_checkpointing\": True,  # Memory efficiency\n",
    "    \"fp16\": True,\n",
    "\n",
    "    # LoRA - Use proven working settings\n",
    "    \"use_lora\": True,\n",
    "    \"lora_r\": 32,  # Back to working value (not 64)\n",
    "    \"lora_alpha\": 64,  # 2x ratio\n",
    "    \"lora_dropout\": 0.1,\n",
    "    \"lora_target_modules\": [\"query_proj\", \"key_proj\", \"value_proj\"],\n",
    "\n",
    "    # TRAINING - CRITICAL FIX\n",
    "    \"num_train_epochs\": 3,  # Conservative start\n",
    "    # Batch size will be set based on hardware detection\n",
    "    \"per_device_train_batch_size\": 16,  # Default, will adjust\n",
    "    \"per_device_eval_batch_size\": 32,\n",
    "    \"gradient_accumulation_steps\": 2,   # Conservative\n",
    "    \"max_length\": 96,  # Shorter for stability\n",
    "\n",
    "    # CRITICAL: CORRECT LEARNING RATE!\n",
    "    \"learning_rate\": 1e-4,  # Conservative middle (3e-4 worked, 5e-6 failed)\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"warmup_ratio\": 0.1,\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "\n",
    "    # EVALUATION\n",
    "    \"evaluation_strategy\": \"steps\",\n",
    "    \"eval_steps\": 300,  # More frequent monitoring\n",
    "    \"save_steps\": 300,\n",
    "    \"logging_steps\": 50,\n",
    "    \"save_total_limit\": 2,\n",
    "    \"load_best_model_at_end\": True,\n",
    "    \"metric_for_best_model\": \"f1_macro\",\n",
    "    \"early_stopping_patience\": 2,  # Stop if no improvement\n",
    "\n",
    "    # ASL - Conservative settings\n",
    "    \"asl_gamma_neg\": 2.0,\n",
    "    \"asl_gamma_pos\": 1.0,\n",
    "    \"asl_clip\": 0.05,\n",
    "    \"asl_pos_alpha\": 1.0,\n",
    "}\n",
    "\n",
    "# Hardware-specific adjustments\n",
    "if 'RECOVERY_CONFIG' in globals():\n",
    "    # Use hardware-detected batch size\n",
    "    CONFIG[\"per_device_train_batch_size\"] = RECOVERY_CONFIG[\"recommended_batch_size\"]\n",
    "    print(f\"🔧 Adjusted batch size to {CONFIG['per_device_train_batch_size']} for detected hardware\")\n",
    "\n",
    "# Set seed for reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(CONFIG[\"seed\"])\n",
    "\n",
    "print(\"✅ Recovery Configuration Loaded\")\n",
    "print(\"🎯 KEY CHANGES FROM FAILED CONFIG:\")\n",
    "print(f\"   ❌ Learning rate: 5e-6 → ✅ {CONFIG['learning_rate']} (100x increase!)\")\n",
    "print(f\"   ❌ LoRA rank: 64 → ✅ {CONFIG['lora_r']} (back to proven)\")\n",
    "print(f\"   ❌ Data subset: 90% → ✅ {CONFIG['subset_ratio']*100:.0f}% (conservative)\")\n",
    "print(f\"   ✅ Epochs: {CONFIG['num_train_epochs']} (quick validation)\")\n",
    "\n",
    "effective_batch = CONFIG['per_device_train_batch_size'] * CONFIG['gradient_accumulation_steps']\n",
    "print(f\"\\n📊 TRAINING PARAMETERS:\")\n",
    "print(f\"   Effective batch size: {effective_batch}\")\n",
    "print(f\"   Learning rate: {CONFIG['learning_rate']} (CRITICAL FIX)\")\n",
    "print(f\"   Data samples: ~{int(CONFIG['subset_ratio'] * 43000)} train samples\")\n",
    "print(f\"   Expected F1 target: >30% (validation), >40% (full training)\")\n",
    "\n",
    "# Validation check\n",
    "if CONFIG['learning_rate'] <= 1e-5:\n",
    "    print(\"⚠️  WARNING: Learning rate still too low! Consider 5e-5 or higher.\")\n",
    "if CONFIG['per_device_train_batch_size'] > 32:\n",
    "    print(\"⚠️  WARNING: Batch size might be too large. Monitor memory usage.\")\n",
    "\n",
    "print(\"\\n🚀 Ready for recovery training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "iMdIfrLBN7d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Loading GoEmotions dataset...\n",
      "   Downloading train...\n",
      "   Downloading validation...\n",
      "✅ Loaded 43410 train, 5426 validation samples\n",
      "   28 emotion labels\n",
      "   train: 43410 → 30386 samples (70%)\n",
      "   validation: 5426 → 3798 samples (70%)\n",
      "\n",
      "🔤 Tokenizing dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e383436e8ba14a7389c5268de3b51309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30386 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1487616a97f74ad88fa0f937fedfb5ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3798 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset ready for training\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load and Prepare Dataset\n",
    "import gc\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import io\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import DebertaV2Tokenizer\n",
    "\n",
    "print(\"💾 Loading GoEmotions dataset...\")\n",
    "\n",
    "# Load from GitHub\n",
    "base_url = \"https://raw.githubusercontent.com/google-research/google-research/master/goemotions/data/\"\n",
    "\n",
    "# Download train and dev sets\n",
    "dfs = {}\n",
    "for split_name, filename in {\"train\": \"train.tsv\", \"validation\": \"dev.tsv\"}.items():\n",
    "    url = base_url + filename\n",
    "    print(f\"   Downloading {split_name}...\")\n",
    "    response = urllib.request.urlopen(url)\n",
    "    content = response.read().decode('utf-8')\n",
    "    df = pd.read_csv(io.StringIO(content), sep='\\t', header=None, names=['text', 'labels', 'id'])\n",
    "    dfs[split_name] = df\n",
    "\n",
    "# Load emotion labels\n",
    "emotions_url = base_url + \"emotions.txt\"\n",
    "response = urllib.request.urlopen(emotions_url)\n",
    "LABEL_NAMES = response.read().decode('utf-8').strip().split('\\n')\n",
    "NUM_LABELS = len(LABEL_NAMES)\n",
    "\n",
    "print(f\"✅ Loaded {len(dfs['train'])} train, {len(dfs['validation'])} validation samples\")\n",
    "print(f\"   {NUM_LABELS} emotion labels\")\n",
    "\n",
    "# Process labels to multi-hot\n",
    "def process_labels(labels_str):\n",
    "    multi_hot = np.zeros(NUM_LABELS, dtype=np.float32)\n",
    "    if pd.notna(labels_str) and labels_str.strip():\n",
    "        for idx in labels_str.strip().split(','):\n",
    "            if idx.isdigit():\n",
    "                label_idx = int(idx)\n",
    "                if 0 <= label_idx < NUM_LABELS:\n",
    "                    multi_hot[label_idx] = 1.0\n",
    "    return multi_hot.tolist()\n",
    "\n",
    "# Create datasets\n",
    "datasets = {}\n",
    "for split, df in dfs.items():\n",
    "    labels = [process_labels(l) for l in df['labels']]\n",
    "    dataset = Dataset.from_dict({\n",
    "        'text': df['text'].tolist(),\n",
    "        'labels': labels\n",
    "    })\n",
    "    datasets[split] = dataset\n",
    "\n",
    "# Apply subset if configured\n",
    "if CONFIG.get(\"use_subset\"):\n",
    "    ratio = CONFIG[\"subset_ratio\"]\n",
    "    for split in datasets:\n",
    "        orig_size = len(datasets[split])\n",
    "        new_size = int(orig_size * ratio)\n",
    "        datasets[split] = datasets[split].shuffle(seed=CONFIG[\"seed\"]).select(range(new_size))\n",
    "        print(f\"   {split}: {orig_size} → {new_size} samples ({ratio*100:.0f}%)\")\n",
    "\n",
    "# Tokenize\n",
    "tokenizer = DebertaV2Tokenizer.from_pretrained(CONFIG[\"model_name\"], use_fast=False)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=CONFIG[\"max_length\"],\n",
    "        padding=False\n",
    "    )\n",
    "\n",
    "print(\"\\n🔤 Tokenizing dataset...\")\n",
    "tokenized_datasets = DatasetDict(datasets).map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"]\n",
    ")\n",
    "\n",
    "# Clean memory\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"✅ Dataset ready for training\")\n",
    "\n",
    "# Export for later use\n",
    "DATASETS = tokenized_datasets\n",
    "ID2LABEL = {i: name for i, name in enumerate(LABEL_NAMES)}\n",
    "LABEL2ID = {name: i for i, name in enumerate(LABEL_NAMES)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bFhKfDeyN7d1"
   },
   "outputs": [],
   "source": [
    "# Cell 4: Build Model with LoRA\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "print(\"🤖 Building DeBERTa-v3 model with optimized LoRA...\")\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    CONFIG[\"model_name\"],\n",
    "    num_labels=NUM_LABELS,\n",
    "    problem_type=\"multi_label_classification\",\n",
    "    id2label=ID2LABEL,\n",
    "    label2id=LABEL2ID,\n",
    ")\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "if CONFIG[\"gradient_checkpointing\"]:\n",
    "    model.gradient_checkpointing_enable()\n",
    "    print(\"   ✅ Gradient checkpointing enabled\")\n",
    "\n",
    "# Apply LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=CONFIG[\"lora_r\"],\n",
    "    lora_alpha=CONFIG[\"lora_alpha\"],\n",
    "    lora_dropout=CONFIG[\"lora_dropout\"],\n",
    "    target_modules=CONFIG[\"lora_target_modules\"],\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Initialize bias with better strategy\n",
    "def initialize_bias_balanced(model, train_labels):\n",
    "    \"\"\"Initialize classifier bias with balanced strategy\"\"\"\n",
    "    Y = np.asarray(train_labels, dtype=np.float32)\n",
    "    p = Y.mean(axis=0)\n",
    "\n",
    "    # Wider clipping range preserves relative frequencies\n",
    "    p_clipped = np.clip(p, 0.001, 0.999)\n",
    "    logits = np.log(p_clipped / (1.0 - p_clipped))\n",
    "\n",
    "    # Scale based on prevalence\n",
    "    scale = np.where(p < 0.01, 0.2,\n",
    "            np.where(p < 0.05, 0.3,\n",
    "            np.where(p < 0.1, 0.5, 0.7)))\n",
    "\n",
    "    prior_logits = logits * scale\n",
    "\n",
    "    # Find and set bias\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear) and module.out_features == NUM_LABELS:\n",
    "            with torch.no_grad():\n",
    "                device = next(model.parameters()).device\n",
    "                bias = torch.from_numpy(prior_logits).to(device, dtype=module.weight.dtype)\n",
    "                if module.bias is None:\n",
    "                    module.bias = nn.Parameter(torch.zeros_like(bias))\n",
    "                module.bias.copy_(bias)\n",
    "            print(f\"✅ Bias initialized (balanced strategy)\")\n",
    "            print(f\"   Range: [{prior_logits.min():.2f}, {prior_logits.max():.2f}]\")\n",
    "            break\n",
    "\n",
    "# Get training labels for bias init\n",
    "train_labels = [row[\"labels\"] for row in DATASETS[\"train\"]]\n",
    "initialize_bias_balanced(model, train_labels)\n",
    "\n",
    "print(\"✅ Model ready for training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-PvHdBeBN7d1"
   },
   "outputs": [],
   "source": [
    "# Cell 5: Setup Trainer with Optimized Parameters\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding, EarlyStoppingCallback\n",
    "from sklearn.metrics import f1_score\n",
    "import torch.nn as nn\n",
    "\n",
    "print(\"🎯 Setting up optimized trainer...\")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)\n",
    "\n",
    "# Metrics computation\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred.predictions, eval_pred.label_ids\n",
    "    probs = 1.0 / (1.0 + np.exp(-logits))\n",
    "\n",
    "    # Evaluate at multiple thresholds\n",
    "    metrics = {}\n",
    "    for threshold in [0.3, 0.5]:\n",
    "        preds = (probs >= threshold).astype(int)\n",
    "        suffix = f\"_t{int(threshold*10)}\"\n",
    "        metrics[f\"f1_micro{suffix}\"] = f1_score(labels, preds, average=\"micro\", zero_division=0)\n",
    "        metrics[f\"f1_macro{suffix}\"] = f1_score(labels, preds, average=\"macro\", zero_division=0)\n",
    "\n",
    "    # Primary metric for model selection\n",
    "    metrics[\"f1_macro\"] = metrics[\"f1_macro_t5\"]\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# Asymmetric Loss\n",
    "class AsymmetricLoss(nn.Module):\n",
    "    def __init__(self, gamma_neg=2.0, gamma_pos=0.5, clip=0.03, eps=1e-8, pos_alpha=1.5):\n",
    "        super().__init__()\n",
    "        self.gamma_neg = gamma_neg\n",
    "        self.gamma_pos = gamma_pos\n",
    "        self.clip = clip\n",
    "        self.eps = eps\n",
    "        self.pos_alpha = pos_alpha\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x_sigmoid = torch.sigmoid(x)\n",
    "        xs_pos = x_sigmoid\n",
    "        xs_neg = 1 - x_sigmoid\n",
    "\n",
    "        if self.clip > 0:\n",
    "            xs_neg = (xs_neg + self.clip).clamp(max=1)\n",
    "\n",
    "        los_pos = self.pos_alpha * y * torch.log(xs_pos.clamp(min=self.eps))\n",
    "        los_neg = (1 - y) * torch.log(xs_neg.clamp(min=self.eps))\n",
    "\n",
    "        loss = los_pos + los_neg\n",
    "\n",
    "        if self.gamma_neg > 0 or self.gamma_pos > 0:\n",
    "            pt = xs_pos * y + xs_neg * (1 - y)\n",
    "            gamma = self.gamma_pos * y + self.gamma_neg * (1 - y)\n",
    "            loss = loss * torch.pow(1 - pt, gamma)\n",
    "\n",
    "        return -loss.mean()\n",
    "\n",
    "# Custom trainer with ASL\n",
    "class ASLTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.loss_fct = AsymmetricLoss(\n",
    "            gamma_neg=CONFIG[\"asl_gamma_neg\"],\n",
    "            gamma_pos=CONFIG[\"asl_gamma_pos\"],\n",
    "            clip=CONFIG[\"asl_clip\"],\n",
    "            pos_alpha=CONFIG[\"asl_pos_alpha\"]\n",
    "        )\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        loss = self.loss_fct(outputs.logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=CONFIG[\"output_dir\"],\n",
    "    num_train_epochs=CONFIG[\"num_train_epochs\"],\n",
    "    per_device_train_batch_size=CONFIG[\"per_device_train_batch_size\"],\n",
    "    per_device_eval_batch_size=CONFIG[\"per_device_eval_batch_size\"],\n",
    "    gradient_accumulation_steps=CONFIG[\"gradient_accumulation_steps\"],\n",
    "    learning_rate=CONFIG[\"learning_rate\"],\n",
    "    weight_decay=CONFIG[\"weight_decay\"],\n",
    "    warmup_ratio=CONFIG[\"warmup_ratio\"],\n",
    "    lr_scheduler_type=CONFIG[\"lr_scheduler_type\"],\n",
    "    evaluation_strategy=CONFIG[\"evaluation_strategy\"],\n",
    "    eval_steps=CONFIG[\"eval_steps\"],\n",
    "    save_steps=CONFIG[\"save_steps\"],\n",
    "    logging_steps=CONFIG[\"logging_steps\"],\n",
    "    save_total_limit=CONFIG[\"save_total_limit\"],\n",
    "    load_best_model_at_end=CONFIG[\"load_best_model_at_end\"],\n",
    "    metric_for_best_model=CONFIG[\"metric_for_best_model\"],\n",
    "    greater_is_better=True,\n",
    "    fp16=CONFIG[\"fp16\"],\n",
    "    gradient_checkpointing=CONFIG[\"gradient_checkpointing\"],\n",
    "    dataloader_num_workers=2,\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=True,\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = ASLTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=DATASETS[\"train\"],\n",
    "    eval_dataset=DATASETS[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=CONFIG[\"early_stopping_patience\"])],\n",
    ")\n",
    "\n",
    "print(\"✅ Trainer configured with optimized settings\")\n",
    "print(f\"   Learning rate: {CONFIG['learning_rate']} (critical for DeBERTa!)\")\n",
    "print(f\"   Effective batch size: {CONFIG['per_device_train_batch_size'] * CONFIG['gradient_accumulation_steps']}\")\n",
    "print(f\"   Total training steps: ~{len(DATASETS['train']) // (CONFIG['per_device_train_batch_size'] * CONFIG['gradient_accumulation_steps']) * CONFIG['num_train_epochs']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sPdNIo52OgWp"
   },
   "outputs": [],
   "source": [
    "# SAMO Validation Test: 1-Epoch Quick Check\n",
    "# Run this BEFORE full training to validate the recovery approach\n",
    "\n",
    "print(\"🧪 SAMO 1-Epoch Validation Test\")\n",
    "print(\"🎯 Target: F1 Macro >30% to confirm recovery\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Override config for quick test\n",
    "VALIDATION_CONFIG = CONFIG.copy()\n",
    "VALIDATION_CONFIG.update({\n",
    "    \"num_train_epochs\": 1,  # Just 1 epoch for validation\n",
    "    \"subset_ratio\": 0.3,    # Even smaller subset for speed\n",
    "    \"eval_steps\": 100,      # Frequent evaluation\n",
    "    \"save_steps\": 100,\n",
    "    \"logging_steps\": 25,\n",
    "    \"output_dir\": \"./samo_validation_test\",\n",
    "})\n",
    "\n",
    "print(f\"📊 VALIDATION SETTINGS:\")\n",
    "print(f\"   Epochs: {VALIDATION_CONFIG['num_train_epochs']}\")\n",
    "print(f\"   Data subset: {VALIDATION_CONFIG['subset_ratio']*100:.0f}%\")\n",
    "print(f\"   Learning rate: {VALIDATION_CONFIG['learning_rate']}\")\n",
    "print(f\"   Batch size: {VALIDATION_CONFIG['per_device_train_batch_size']}\")\n",
    "\n",
    "# Create simple prediction distribution monitor\n",
    "class PredictionMonitor:\n",
    "    def __init__(self):\n",
    "        self.step_count = 0\n",
    "        self.prediction_stats = []\n",
    "\n",
    "    def log_predictions(self, probs):\n",
    "        \"\"\"Monitor prediction distributions to catch model collapse early\"\"\"\n",
    "        avg_positive_rate = (probs > 0.5).mean()\n",
    "        avg_confidence = probs.mean()\n",
    "        max_confidence = probs.max()\n",
    "\n",
    "        self.prediction_stats.append({\n",
    "            'step': self.step_count,\n",
    "            'avg_positive_rate': float(avg_positive_rate),\n",
    "            'avg_confidence': float(avg_confidence),\n",
    "            'max_confidence': float(max_confidence)\n",
    "        })\n",
    "\n",
    "        # Early warning signs\n",
    "        if avg_positive_rate < 0.01:  # Less than 1% positive predictions\n",
    "            print(f\"⚠️  WARNING Step {self.step_count}: Very low positive rate ({avg_positive_rate:.1%})\")\n",
    "        if avg_confidence < 0.1:  # Very low confidence overall\n",
    "            print(f\"⚠️  WARNING Step {self.step_count}: Very low confidence ({avg_confidence:.3f})\")\n",
    "\n",
    "        self.step_count += 1\n",
    "\n",
    "    def get_summary(self):\n",
    "        if not self.prediction_stats:\n",
    "            return \"No predictions logged\"\n",
    "\n",
    "        latest = self.prediction_stats[-1]\n",
    "        return f\"Final - Pos Rate: {latest['avg_positive_rate']:.1%}, Avg Conf: {latest['avg_confidence']:.3f}\"\n",
    "\n",
    "# Success criteria for validation\n",
    "SUCCESS_CRITERIA = {\n",
    "    \"min_f1_macro\": 0.30,      # Must exceed 30% F1 macro\n",
    "    \"min_f1_micro\": 0.35,      # Must exceed 35% F1 micro\n",
    "    \"min_pos_rate\": 0.05,      # At least 5% positive predictions\n",
    "    \"max_training_loss\": 0.1,  # Training loss should drop\n",
    "}\n",
    "\n",
    "print(f\"\\n✅ SUCCESS CRITERIA:\")\n",
    "for criterion, value in SUCCESS_CRITERIA.items():\n",
    "    print(f\"   {criterion}: {value}\")\n",
    "\n",
    "print(f\"\\n⚠️  FAILURE INDICATORS TO WATCH:\")\n",
    "print(f\"   - Training speed <1.0 it/s (memory issues)\")\n",
    "print(f\"   - F1 scores <20% (model collapse)\")\n",
    "print(f\"   - All predictions negative (bias issues)\")\n",
    "print(f\"   - Loss not decreasing (gradient issues)\")\n",
    "\n",
    "print(f\"\\n🔄 NEXT STEPS AFTER VALIDATION:\")\n",
    "print(f\"   ✅ If SUCCESS: Use full CONFIG with 3+ epochs\")\n",
    "print(f\"   ❌ If FAILURE: Increase learning rate to 2e-4 or 3e-4\")\n",
    "print(f\"   📊 Monitor: Use prediction monitor throughout\")\n",
    "\n",
    "# Export validation config\n",
    "globals()['VALIDATION_CONFIG'] = VALIDATION_CONFIG\n",
    "globals()['PredictionMonitor'] = PredictionMonitor\n",
    "globals()['SUCCESS_CRITERIA'] = SUCCESS_CRITERIA\n",
    "\n",
    "print(\"\\n🚀 Ready to run validation test!\")\n",
    "print(\"Use VALIDATION_CONFIG instead of CONFIG for the quick test.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 172
    },
    "id": "Ol4Yz23sN7d1",
    "outputId": "723eba74-bd0b-4cdd-f65e-829e6aa0f008"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting optimized training...\n",
      "⏰ This should take ~1-2 hours on a T4 GPU\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "NCCL Error 5: invalid usage (run with NCCL_DEBUG=WARN for details)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Train!\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m train_result \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m✅ Training completed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   Final loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_result\u001b[38;5;241m.\u001b[39mmetrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/transformers/trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1883\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/transformers/trainer.py:2216\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2213\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2216\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2219\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2220\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2221\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2222\u001b[0m ):\n\u001b[1;32m   2223\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2224\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/transformers/trainer.py:3250\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3248\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3249\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3250\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/accelerate/accelerator.py:2130\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   2129\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2130\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2131\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[1;32m   2132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/autograd/function.py:307\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImplementing both \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvjp\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for a custom \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction is not allowed. You should only implement one \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    304\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof them.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    305\u001b[0m     )\n\u001b[1;32m    306\u001b[0m user_fn \u001b[38;5;241m=\u001b[39m vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function\u001b[38;5;241m.\u001b[39mvjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[0;32m--> 307\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/utils/checkpoint.py:321\u001b[0m, in \u001b[0;36mCheckpointFunction.backward\u001b[0;34m(ctx, *args)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(outputs_with_grad) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone of output has requires_grad=True,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m this checkpoint() is not necessary\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m     )\n\u001b[0;32m--> 321\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs_with_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs_with_grad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    322\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[1;32m    323\u001b[0m     inp\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inp, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m inp \u001b[38;5;129;01min\u001b[39;00m detached_inputs\n\u001b[1;32m    325\u001b[0m )\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m+\u001b[39m grads\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/autograd/function.py:307\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImplementing both \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvjp\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for a custom \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction is not allowed. You should only implement one \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    304\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof them.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    305\u001b[0m     )\n\u001b[1;32m    306\u001b[0m user_fn \u001b[38;5;241m=\u001b[39m vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function\u001b[38;5;241m.\u001b[39mvjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[0;32m--> 307\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:33\u001b[0m, in \u001b[0;36mBroadcast.backward\u001b[0;34m(ctx, *grad_outputs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbackward\u001b[39m(ctx, \u001b[38;5;241m*\u001b[39mgrad_outputs):\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m,) \u001b[38;5;241m+\u001b[39m \u001b[43mReduceAddCoalesced\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgrad_outputs\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/autograd/function.py:575\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    574\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    583\u001b[0m     )\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:46\u001b[0m, in \u001b[0;36mReduceAddCoalesced.forward\u001b[0;34m(ctx, destination, num_inputs, *grads)\u001b[0m\n\u001b[1;32m     41\u001b[0m ctx\u001b[38;5;241m.\u001b[39mtarget_gpus \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     42\u001b[0m     grads[i]\u001b[38;5;241m.\u001b[39mget_device() \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(grads), num_inputs)\n\u001b[1;32m     43\u001b[0m ]\n\u001b[1;32m     45\u001b[0m grads_ \u001b[38;5;241m=\u001b[39m [grads[i : i \u001b[38;5;241m+\u001b[39m num_inputs] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(grads), num_inputs)]\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcomm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce_add_coalesced\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdestination\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/nn/parallel/comm.py:159\u001b[0m, in \u001b[0;36mreduce_add_coalesced\u001b[0;34m(inputs, destination, buffer_size)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunks \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mitrs):\n\u001b[1;32m    156\u001b[0m     flat_tensors \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    157\u001b[0m         _flatten_dense_tensors(chunk) \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunks\n\u001b[1;32m    158\u001b[0m     ]  \u001b[38;5;66;03m# (num_gpus,)\u001b[39;00m\n\u001b[0;32m--> 159\u001b[0m     flat_result \u001b[38;5;241m=\u001b[39m \u001b[43mreduce_add\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflat_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdestination\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m _unflatten_dense_tensors(flat_result, chunks[\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;66;03m# The unflattened tensors do not share storage, and we don't expose\u001b[39;00m\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;66;03m# base flat tensor anyways, so give them different version counters.\u001b[39;00m\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;66;03m# See NOTE [ Version Counter in comm.*_coalesced ]\u001b[39;00m\n\u001b[1;32m    164\u001b[0m         output\u001b[38;5;241m.\u001b[39mappend(t\u001b[38;5;241m.\u001b[39mdata)\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/nn/parallel/comm.py:108\u001b[0m, in \u001b[0;36mreduce_add\u001b[0;34m(inputs, destination)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nccl\u001b[38;5;241m.\u001b[39mis_available(inputs):\n\u001b[1;32m    107\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mempty_like(inputs[root_index])\n\u001b[0;32m--> 108\u001b[0m     \u001b[43mnccl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mroot_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    110\u001b[0m     destination_device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(inputs[root_index]\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype, destination)\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/cuda/nccl.py:121\u001b[0m, in \u001b[0;36mreduce\u001b[0;34m(inputs, output, root, op, streams, comms, outputs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m     _output \u001b[38;5;241m=\u001b[39m inputs[root] \u001b[38;5;28;01mif\u001b[39;00m output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m output\n\u001b[0;32m--> 121\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nccl_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstreams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomms\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: NCCL Error 5: invalid usage (run with NCCL_DEBUG=WARN for details)"
     ]
    }
   ],
   "source": [
    "# Cell 6: TRAIN THE MODEL\n",
    "print(\"🚀 Starting optimized training...\")\n",
    "print(\"⏰ This should take ~1-2 hours on a T4 GPU\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Clean memory before training\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Train!\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\n✅ Training completed!\")\n",
    "print(f\"   Final loss: {train_result.metrics['train_loss']:.4f}\")\n",
    "\n",
    "# Save best model\n",
    "best_model_path = os.path.join(CONFIG[\"output_dir\"], \"best_model\")\n",
    "model.save_pretrained(best_model_path)\n",
    "tokenizer.save_pretrained(best_model_path)\n",
    "print(f\"   Model saved to: {best_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "67ausdXIN7d1"
   },
   "outputs": [],
   "source": [
    "# Cell 7: Evaluate and Diagnose\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "print(\"🔍 Evaluating model performance...\\n\")\n",
    "\n",
    "# Get predictions\n",
    "predictions = trainer.predict(DATASETS[\"validation\"])\n",
    "logits = predictions.predictions\n",
    "labels = predictions.label_ids\n",
    "probs = 1.0 / (1.0 + np.exp(-logits))\n",
    "\n",
    "# Evaluate at different thresholds\n",
    "print(\"📊 Performance at Different Thresholds:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Threshold':<10} {'F1 Micro':<12} {'F1 Macro':<12} {'Avg Preds/Sample':<15}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "best_threshold = 0.5\n",
    "best_f1_macro = 0\n",
    "\n",
    "for threshold in [0.2, 0.3, 0.4, 0.5, 0.6]:\n",
    "    preds = (probs >= threshold).astype(int)\n",
    "    f1_micro = f1_score(labels, preds, average='micro', zero_division=0)\n",
    "    f1_macro = f1_score(labels, preds, average='macro', zero_division=0)\n",
    "    avg_preds = preds.sum(axis=1).mean()\n",
    "\n",
    "    print(f\"{threshold:<10.1f} {f1_micro:<12.4f} {f1_macro:<12.4f} {avg_preds:<15.2f}\")\n",
    "\n",
    "    if f1_macro > best_f1_macro:\n",
    "        best_f1_macro = f1_macro\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"\\n🎯 Best threshold: {best_threshold} with F1 Macro: {best_f1_macro:.4f}\")\n",
    "\n",
    "# Per-class performance\n",
    "print(\"\\n📈 Per-Class Performance (at best threshold):\")\n",
    "best_preds = (probs >= best_threshold).astype(int)\n",
    "per_class_f1 = []\n",
    "\n",
    "for i, label_name in enumerate(LABEL_NAMES):\n",
    "    f1 = f1_score(labels[:, i], best_preds[:, i], zero_division=0)\n",
    "    prevalence = labels[:, i].mean()\n",
    "    per_class_f1.append((label_name, f1, prevalence))\n",
    "\n",
    "# Sort by F1 score\n",
    "per_class_f1.sort(key=lambda x: x[1])\n",
    "\n",
    "print(\"\\n5 Worst Performing Classes:\")\n",
    "for name, f1, prev in per_class_f1[:5]:\n",
    "    print(f\"  {name:<15} F1: {f1:.3f}  Prevalence: {prev:.3f}\")\n",
    "\n",
    "print(\"\\n5 Best Performing Classes:\")\n",
    "for name, f1, prev in per_class_f1[-5:]:\n",
    "    print(f\"  {name:<15} F1: {f1:.3f}  Prevalence: {prev:.3f}\")\n",
    "\n",
    "# Save optimal thresholds\n",
    "optimal_thresholds = np.full(NUM_LABELS, best_threshold)\n",
    "# Fine-tune per-class thresholds\n",
    "for j in range(NUM_LABELS):\n",
    "    y_true = labels[:, j]\n",
    "    y_scores = probs[:, j]\n",
    "\n",
    "    best_t = best_threshold\n",
    "    best_f = f1_score(y_true, (y_scores >= best_t).astype(int), zero_division=0)\n",
    "\n",
    "    for t in np.linspace(0.1, 0.9, 9):\n",
    "        f = f1_score(y_true, (y_scores >= t).astype(int), zero_division=0)\n",
    "        if f > best_f:\n",
    "            best_f = f\n",
    "            best_t = t\n",
    "\n",
    "    optimal_thresholds[j] = best_t\n",
    "\n",
    "# Apply optimized thresholds\n",
    "final_preds = (probs >= optimal_thresholds.reshape(1, -1)).astype(int)\n",
    "final_f1_micro = f1_score(labels, final_preds, average='micro', zero_division=0)\n",
    "final_f1_macro = f1_score(labels, final_preds, average='macro', zero_division=0)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"🏆 FINAL RESULTS WITH OPTIMIZED THRESHOLDS:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"F1 Micro: {final_f1_micro:.4f}\")\n",
    "print(f\"F1 Macro: {final_f1_macro:.4f}\")\n",
    "\n",
    "if final_f1_macro >= 0.60:\n",
    "    print(\"\\n✅ SUCCESS! Achieved >60% F1 Macro target!\")\n",
    "elif final_f1_macro >= 0.50:\n",
    "    print(\"\\n⚠️ Good progress! Close to 60% target. Consider:\")\n",
    "    print(\"   - Training for more epochs\")\n",
    "    print(\"   - Using full dataset (100%)\")\n",
    "    print(\"   - Further hyperparameter tuning\")\n",
    "else:\n",
    "    print(\"\\n❌ Below target. Check training logs for issues.\")\n",
    "\n",
    "# Save thresholds\n",
    "import json\n",
    "threshold_path = os.path.join(CONFIG[\"output_dir\"], \"optimal_thresholds.json\")\n",
    "with open(threshold_path, 'w') as f:\n",
    "    json.dump(optimal_thresholds.tolist(), f)\n",
    "print(f\"\\n💾 Thresholds saved to: {threshold_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m8eO7y5EN7d1"
   },
   "outputs": [],
   "source": [
    "# Cell 8: Test Inference\n",
    "print(\"🎯 Testing inference on sample texts...\\n\")\n",
    "\n",
    "test_texts = [\n",
    "    \"I'm so happy and grateful for this amazing day!\",\n",
    "    \"This is really frustrating and annoying.\",\n",
    "    \"I feel anxious but also excited about tomorrow.\",\n",
    "    \"The weather is nice today.\",\n",
    "    \"I love spending time with my family.\",\n",
    "    \"I'm disappointed with how things turned out.\"\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "device = next(model.parameters()).device\n",
    "\n",
    "with torch.no_grad():\n",
    "    for text in test_texts:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True,\n",
    "                          max_length=CONFIG[\"max_length\"], padding=True)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        probs = torch.sigmoid(outputs.logits).cpu().numpy()[0]\n",
    "\n",
    "        # Apply optimized thresholds\n",
    "        detected_emotions = [LABEL_NAMES[i] for i, p in enumerate(probs)\n",
    "                           if p > optimal_thresholds[i]]\n",
    "\n",
    "        # Get top 3 emotions by probability\n",
    "        top_indices = np.argsort(probs)[-3:][::-1]\n",
    "        top_emotions = [(LABEL_NAMES[i], probs[i]) for i in top_indices]\n",
    "\n",
    "        print(f\"Text: \\\"{text}\\\"\")\n",
    "        print(f\"  Detected: {detected_emotions if detected_emotions else ['neutral']}\")\n",
    "        print(f\"  Top 3: {', '.join([f'{e}({p:.2f})' for e, p in top_emotions])}\")\n",
    "        print()\n",
    "\n",
    "print(\"\\n✅ Inference test complete!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "history_visible": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
