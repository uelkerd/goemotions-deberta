print("ðŸ”¥ RUNNING THE FIXED CONFIGURATION IMMEDIATELY")
print("=" * 60)
print("Using: Weighted BCE + Lower Thresholds + 20k samples")
print()

# Run the WORKING configuration
!cd /workspace && python3 notebooks/scripts/train_deberta_local.py \
  --output_dir "./outputs/emergency_fix_v3" \
  --model_type "deberta-v3-large" \
  --per_device_train_batch_size 4 \
  --per_device_eval_batch_size 8 \
  --gradient_accumulation_steps 4 \
  --num_train_epochs 3 \
  --learning_rate 3e-5 \
  --warmup_ratio 0.2 \
  --weight_decay 0.01 \
  --fp16 \
  --max_length 256 \
  --max_train_samples 20000 \
  --max_eval_samples 3000 \
  --evaluation_strategy "steps" \
  --eval_steps 250 \
  --save_strategy "steps" \
  --save_steps 250 \
  --metric_for_best_model "f1_macro" \
  --load_best_model_at_end \
  --logging_steps 50# GoEmotions DeBERTa EMERGENCY FIX

## ðŸš¨ CRITICAL ISSUES IDENTIFIED

Previous attempts FAILED catastrophically:
- **BCE**: 5.4% F1 (only learned 3/28 classes)  
- **Asymmetric Loss**: 6.2% F1 (predicted everything as positive)

## Root Causes:
1. **Wrong prediction threshold** (0.5 for imbalanced data)
2. **Loss scale problems** (negative loss values)
3. **No class weighting**

## âœ… THIS NOTEBOOK FIXES EVERYTHING

**Solution**: Weighted BCE + Dynamic Thresholds + Proper Initialization  
**Expected**: 50-65% F1 Macro