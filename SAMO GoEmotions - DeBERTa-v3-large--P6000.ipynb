SAMO GoEmotions - DeBERTa-v3-large (P6000 FINAL STABLE & OPTIMIZED)Â¶
===================================================================================
This notebook is tuned for a successful and efficient run on a 24GB P6000 GPU.

import os
import sys
import subprocess
import warnings
import random
import json
import torch
import numpy as np
import pandas as pd
import gc

# ===================================================================================
# 1. ENVIRONMENT SETUP WITH GPU MEMORY MANAGEMENT
# ===================================================================================
print("ðŸš€ [1/8] Setting up the environment...")

# CRITICAL: Clear GPU cache and set memory fraction
if torch.cuda.is_available():
    torch.cuda.empty_cache()
    torch.cuda.set_per_process_memory_fraction(0.95)  # Use 95% of GPU memory max
    
warnings.filterwarnings('ignore')
os.environ['TOKENIZERS_PARALLELISM'] = 'false'
os.environ['HF_HOME'] = '/kaggle/working/hf'
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'  # Helps with fragmentation
os.makedirs(os.environ.get('HF_HOME'), exist_ok=True)

# Dependency Installation
def install_packages(packages):
    try:
        subprocess.check_call([
            sys.executable, '-m', 'pip', 'install', '-q', '--no-input', '--no-cache-dir'
        ] + packages, stderr=subprocess.DEVNULL)
        print("   âœ… Dependencies installed successfully.")
    except Exception as e:
        print(f"   âŒ Error installing packages: {e}")
        sys.exit(1)

install_packages([
    'transformers==4.41.2', 'datasets==2.19.0', 'accelerate==0.31.0',
    'peft==0.10.0', 'evaluate==0.4.2', 'scikit-learn==1.5.0', 'torch>=2.1'
])

import transformers, datasets, accelerate, peft
print("\n--- Library Versions ---")
print(f"   - Transformers: {transformers.__version__}, Datasets: {datasets.__version__}")
print(f"   - Accelerate: {accelerate.__version__}, PEFT: {peft.__version__}")
print(f"   - PyTorch: {torch.__version__}")
print("------------------------")
print(f"   - CUDA Available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"   - GPU Device:     {torch.cuda.get_device_name(0)}")
    print(f"   - GPU Memory:     {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
print("-" * 25)

# --- SAFE CONFIG DEFAULTS + REPRO SEED ---
import os, random
import numpy as np
import torch

# If CONFIG isn't defined yet, start empty
if "CONFIG" not in globals() or CONFIG is None:
    CONFIG = {}

DEFAULTS = {
    "seed": 42,
    "output_dir": "./out/fast_p6000",

    # ==== DATA ====
    "dataset_config": "raw",          # or "simplified"
    "use_subset": True,
    "subset_ratio": 0.60,             # ~60% of data for faster turnaround

    # ==== PRECISION / MEMORY ====
    "bf16": False,                    # Pascal has no BF16/TF32
    "fp16": True,
    "gradient_checkpointing": False,  # SPEED: disable recompute (uses more VRAM)

    # ==== MODEL ====
    "model_name_or_path": "microsoft/deberta-v3-large",
    "use_lora": True,
    "lora_r": 16,
    "lora_alpha": 32,
    "lora_dropout": 0.05,
    # keep targets tight; we'll auto-detect anyway
    "lora_target_modules": ["query_proj","key_proj","value_proj"],

    # ==== TRAINING CORE ====
    # Duplicates provided so either naming style works
    "num_train_epochs": 5,            # quick signal; early stopping will halt sooner if bad
    "train_epochs": 5,

    "per_device_train_batch_size": 16,
    "train_batch_size": 16,

    "per_device_eval_batch_size": 64, # SPEED: fast eval
    "eval_batch_size": 64,

    "gradient_accumulation_steps": 2, # effective batch 32 per device

    "max_length": 96,                 # SPEED: shorter seq â†’ fewer FLOPs/token

    "learning_rate": 2e-5,
    "weight_decay": 0.01,
    "warmup_ratio": 0.03,
    "lr_scheduler_type": "cosine",

    # ==== DATALOADER ====
    "dataloader_num_workers": 4,      # SPEED: parallel batch assembly
    # NOTE: also set dataloader_pin_memory=True in the Trainer args (Cell 5)

    # ==== LOGGING/CHECKPOINTS ====
    "logging_steps": 200,
    "evaluation_strategy": "epoch",   # you can switch to "steps" later for early signal
    "eval_steps": 2000,               # (ignored when strategy='epoch')
    "save_strategy": "epoch",
    "save_steps": 2000,               # (ignored when strategy='epoch')
    "save_total_limit": 1,
    "load_best_model_at_end": True,
    "metric_for_best_model": "f1_macro",
    "greater_is_better": True,

    # ==== LOSS (ASL) ====
    "asl_gamma_neg": 3.0,
    "asl_gamma_pos": 1.0,
    "asl_clip": 0.05,
    "asl_pos_alpha": 2.0,

    # ==== MISC ====
    "fp16_full_eval": True,
    "report_to": "none",
}

# Shallow-merge defaults -> user config (user values win)
CONFIG = {**(CONFIG or {}), **DEFAULTS}  # DEFAULTS now override old CONFIG


def set_seed_all(seed: int):
    os.environ["PYTHONHASHSEED"] = str(seed)
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    # Deterministic kernels for reproducibility (slower but stable)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

set_seed_all(CONFIG["seed"])

# Sanity print
print("âœ… Config ok. Seed:", CONFIG["seed"])
print("   Output dir:", CONFIG["output_dir"])
print("   fp16:", CONFIG["fp16"], "| grad ckpt:", CONFIG["gradient_checkpointing"])



 # ===================================================================================
# 3. DATA LOADING WITH SMART SAMPLING FOR SPEED  (ROBUST: raw vs simplified)
# ===================================================================================
print("\nðŸ’¾ [3/8] Loading and preparing GoEmotions dataset...")

import gc, numpy as np, torch, transformers
from datasets import load_dataset, DatasetDict, disable_progress_bar

disable_progress_bar()

# Clear any existing cache
gc.collect()
if torch.cuda.is_available():
    torch.cuda.empty_cache()

# ------------- Load dataset -------------
# Note: "raw" = per-emotion 0/1 columns; "simplified" = single "labels" (list[int])
CFG_NAME = CONFIG.get("dataset_config", "raw")   # set to "simplified" if you want the list-of-ids version
ds_raw = load_dataset("go_emotions", CFG_NAME)

print("   - Creating 90/10 train/validation split...")
train_validation_split = ds_raw["train"].train_test_split(test_size=0.1, seed=CONFIG["seed"])
ds = DatasetDict({
    "train": train_validation_split["train"],
    "validation": train_validation_split["test"],
})

# ------------- Optional subsampling -------------
if CONFIG.get("use_subset", False):
    subset_ratio = float(CONFIG.get("subset_ratio", 0.3))
    original_train_size = len(ds["train"])
    original_val_size   = len(ds["validation"])

    train_subset_size = max(1, int(original_train_size * subset_ratio))
    val_subset_size   = max(1, int(original_val_size * subset_ratio))

    ds["train"] = ds["train"].shuffle(seed=CONFIG["seed"]).select(range(train_subset_size))
    ds["validation"] = ds["validation"].shuffle(seed=CONFIG["seed"]).select(range(val_subset_size))

    print(f"   - Using {subset_ratio*100:.0f}% subset for faster training:")
    print(f"     â€¢ Train: {original_train_size:,} â†’ {len(ds['train']):,} samples")
    print(f"     â€¢ Valid: {original_val_size:,} â†’ {len(ds['validation']):,} samples")
else:
    print(f"   - Full dataset: {len(ds['train'])} train, {len(ds['validation'])} validation samples.")

# ------------- Detect label schema & build label list -------------
features = ds_raw["train"].features
EXPECTED = [
    'admiration','amusement','anger','annoyance','approval','caring','confusion','curiosity','desire',
    'disappointment','disapproval','disgust','embarrassment','excitement','fear','gratitude','grief','joy',
    'love','nervousness','optimism','pride','realization','relief','remorse','sadness','surprise','neutral'
]

if "labels" in features:
    # simplified schema
    LABEL_NAMES = list(features["labels"].feature.names)
    SCHEMA = "simplified"
else:
    # raw schema: per-emotion columns
    LABEL_NAMES = [name for name in EXPECTED if name in features]
    SCHEMA = "raw"

ID2LABEL = {i: n for i, n in enumerate(LABEL_NAMES)}
LABEL2ID = {n: i for i, n in enumerate(LABEL_NAMES)}
NUM_LABELS = len(LABEL_NAMES)
print(f"   - Detected schema: {SCHEMA}. Using {NUM_LABELS} emotion labels.")

# ------------- Tokenizer -------------
model_name = (CONFIG.get("model_name") or
              CONFIG.get("model_name_or_path") or
              "microsoft/deberta-v3-large")
tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)

# ------------- Preprocess -------------
def preprocess_simplified(examples):
    enc = tokenizer(
        examples["text"],
        truncation=True,
        max_length=int(CONFIG.get("max_length", 128)),
        padding=False
    )
    # list[list[int]] -> multi-hot
    batch_size = len(examples["text"])
    multi_hot = np.zeros((batch_size, NUM_LABELS), dtype=np.float32)
    for i, label_ids in enumerate(examples["labels"]):
        for lid in label_ids:
            if 0 <= lid < NUM_LABELS:
                multi_hot[i, lid] = 1.0
    enc["labels"] = multi_hot.tolist()
    return enc

def preprocess_raw(examples):
    enc = tokenizer(
        examples["text"],
        truncation=True,
        max_length=int(CONFIG.get("max_length", 128)),
        padding=False
    )
    # stack each per-emotion 0/1 column into [B, NUM_LABELS]
    cols = [np.asarray(examples[name], dtype=np.float32) for name in LABEL_NAMES]
    multi_hot = np.stack(cols, axis=1)  # shape [B, NUM_LABELS]
    enc["labels"] = multi_hot.tolist()
    return enc

preprocess_fn = preprocess_simplified if SCHEMA == "simplified" else preprocess_raw

print("   - Tokenizing and encoding dataset...")
all_cols = ds["train"].column_names
keep_cols = (["text", "labels"] if SCHEMA == "simplified" else ["text"] + LABEL_NAMES)
columns_to_remove = [c for c in all_cols if c not in keep_cols]

encoded_ds = ds.map(preprocess_fn, batched=True, remove_columns=columns_to_remove)

# Set format for PyTorch
encoded_ds.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])
print("   âœ… Dataset successfully processed.")

# Expose for downstream cells
DATASETS = encoded_ds
LABEL_META = {"names": LABEL_NAMES, "id2label": ID2LABEL, "label2id": LABEL2ID, "num_labels": NUM_LABELS}

# ===================================================================================
# 4. MODEL SETUP WITH MEMORY OPTIMIZATION  (robust name + LoRA autodetect)
# ===================================================================================
print("\nðŸ¤– [4/8] Building DeBERTa-v3 model with memory-optimized LoRA...")

import os, gc, torch, torch.nn as nn, transformers
from transformers import AutoModelForSequenceClassification

# Clear cache before loading model
gc.collect()
if torch.cuda.is_available():
    torch.cuda.empty_cache()

# Resolve model name robustly
model_name = (CONFIG.get("model_name")
              or CONFIG.get("model_name_or_path")
              or "microsoft/deberta-v3-large")
CONFIG["model_name"] = model_name  # normalize for later cells

# Load model
model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=LABEL_META["num_labels"] if "LABEL_META" in globals() else NUM_LABELS,
    problem_type="multi_label_classification",
    id2label=LABEL_META["id2label"] if "LABEL_META" in globals() else ID2LABEL,
    label2id=LABEL_META["label2id"] if "LABEL_META" in globals() else LABEL2ID,
)
# Needed for grad checkpointing to work without warnings
if hasattr(model.config, "use_cache"):
    model.config.use_cache = False

# Enable gradient checkpointing BEFORE moving to GPU/Trainer
if CONFIG.get("gradient_checkpointing", True):
    model.gradient_checkpointing_enable()
    print("   - Gradient checkpointing enabled for memory efficiency.")

# --- LoRA application (with target-module autodetection) ---
if CONFIG.get("use_lora", True):
    from peft import LoraConfig, get_peft_model, TaskType

    # Requested targets (common for DeBERTa-v3). We'll verify what exists.
    requested = CONFIG.get("lora_target_modules") or ["query_proj","key_proj","value_proj","dense"]

    # Find which requested substrings actually match Linear module names
    linear_names = [n for n, m in model.named_modules() if isinstance(m, nn.Linear)]
    active = [t for t in requested if any(t in n for n in linear_names)]

    # Fallback to the safest minimal set if detection yields none
    if not active:
        fallback = ["query_proj","key_proj","value_proj"]
        active = [t for t in fallback if any(t in n for n in linear_names)]

    if not active:
        raise RuntimeError(
            "LoRA target module detection failed. "
            "Set CONFIG['lora_target_modules'] to a list of substrings that occur in model Linear module names."
        )

    lora_config = LoraConfig(
        r=int(CONFIG.get("lora_r", 16)),
        lora_alpha=int(CONFIG.get("lora_alpha", 32)),
        lora_dropout=float(CONFIG.get("lora_dropout", 0.05)),
        target_modules=sorted(set(active)),
        bias="none",
        task_type=TaskType.SEQ_CLS,
    )
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()
    os.makedirs(CONFIG["output_dir"], exist_ok=True)
    model.config.save_pretrained(CONFIG["output_dir"])
    print(f"   - LoRA applied to modules containing: {sorted(set(active))}")

print("   âœ… Model built successfully.")



# === PRIOR BIAS INIT ===
import numpy as np, torch, torch.nn as nn

train_ds = DATASETS["train"] if "DATASETS" in globals() else encoded_ds["train"]
Y = np.stack(train_ds["labels"])  # [N, L]
p = Y.mean(axis=0).clip(1e-6, 1-1e-6)           # class prior
prior_logits = np.log(p / (1 - p)).astype(np.float32)

# find the final Linear head robustly
head = None
for n, m in model.named_modules():
    if isinstance(m, nn.Linear) and getattr(m, "out_features", None) == Y.shape[1]:
        head = m
# fallback: most HF heads expose model.classifier
if head is None and hasattr(model, "classifier"):
    m = model.classifier
    if isinstance(m, nn.Linear) and m.out_features == Y.shape[1]:
        head = m

assert head is not None, "Could not locate classification head."
with torch.no_grad():
    b = torch.tensor(prior_logits, device=next(model.parameters()).device, dtype=head.bias.dtype)
    head.bias.copy_(b)
print("âœ… Classifier bias set from label priors.")


# ===================================================================================
# 5. TRAINER WITH MEMORY-EFFICIENT SETTINGS (robust config + safer metrics)
# ===================================================================================
print("\nðŸ“ˆ [5/8] Configuring memory-efficient Trainer...")

import os, numpy as np, torch, torch.nn as nn
from transformers import Trainer, TrainingArguments, DataCollatorWithPadding, EarlyStoppingCallback

# Use dynamic padding to save memory
data_collator = DataCollatorWithPadding(
    tokenizer=tokenizer,
    pad_to_multiple_of=8,
    return_tensors="pt"
)

# --- Robust metrics: handle both tuple and EvalPrediction ---
THR_NPY = os.path.join(CONFIG["output_dir"], "val_thresholds.npy")

def compute_metrics(eval_pred):
    if isinstance(eval_pred, (tuple, list)):
        logits, labels = eval_pred
    else:
        logits, labels = eval_pred.predictions, eval_pred.label_ids
    probs = 1.0 / (1.0 + np.exp(-logits))
    thr = None
    if os.path.exists(THR_NPY):
        try:
            thr = np.load(THR_NPY)
        except Exception:
            thr = None
    if thr is not None and getattr(thr, "ndim", 0) == 1 and thr.shape[0] == probs.shape[1]:
        preds = (probs >= thr.reshape(1, -1)).astype(int)
    else:
        preds = (probs >= 0.30).astype(int)  # more realistic global cutoff
    from sklearn.metrics import f1_score
    return {
        "f1_micro": f1_score(labels, preds, average="micro", zero_division=0),
        "f1_macro": f1_score(labels, preds, average="macro", zero_division=0),
    }


# --- Asymmetric Loss (with safe defaults) ---
class AsymmetricLoss(nn.Module):
    def __init__(self, gamma_neg=3.0, gamma_pos=1.0, clip=0.05, eps=1e-8, pos_alpha=2.0):
        super().__init__()
        self.gamma_neg = gamma_neg
        self.gamma_pos = gamma_pos
        self.clip = clip
        self.eps = eps
        self.pos_alpha = pos_alpha  # upweight positives

    def forward(self, x, y):
        x_sigmoid = torch.sigmoid(x)
        xs_pos = x_sigmoid
        xs_neg = 1 - x_sigmoid
        if self.clip and self.clip > 0:
            xs_neg = (xs_neg + self.clip).clamp(max=1)
        # weighted log-likelihood
        los_pos = self.pos_alpha * y * torch.log(xs_pos.clamp(min=self.eps))
        los_neg = (1 - y) * torch.log(xs_neg.clamp(min=self.eps))
        loss = los_pos + los_neg
        if self.gamma_neg > 0 or self.gamma_pos > 0:
            pt = xs_pos * y + xs_neg * (1 - y)
            gamma = self.gamma_pos * y + self.gamma_neg * (1 - y)
            loss = loss * torch.pow(1 - pt, gamma)
        return -loss.mean()


class AsymmetricLossTrainer(Trainer):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.loss_fct = AsymmetricLoss(
            gamma_neg=float(CONFIG.get("asl_gamma_neg", 3.0)),
            gamma_pos=float(CONFIG.get("asl_gamma_pos", 1.0)),
            clip=float(CONFIG.get("asl_clip", 0.05)),
            pos_alpha=float(CONFIG.get("asl_pos_alpha", 2.0)),
        )


    def compute_loss(self, model, inputs, return_outputs=False):
        labels = inputs.pop("labels")
        outputs = model(**inputs)
        loss = self.loss_fct(outputs.logits, labels)
        return (loss, outputs) if return_outputs else loss

# --- Resolve config keys with fallbacks ---
def _cfg(key, *alts, default=None, cast=None):
    for k in (key,) + alts:
        if k in CONFIG:
            v = CONFIG[k]
            return cast(v) if cast else v
    return default

num_train_epochs = _cfg("num_train_epochs", "train_epochs", default=3, cast=float)
train_bs         = _cfg("train_batch_size", "per_device_train_batch_size", default=8, cast=int)
eval_bs          = _cfg("eval_batch_size", "per_device_eval_batch_size", default=8, cast=int)
grad_accum       = _cfg("gradient_accumulation_steps", default=4, cast=int)
lr               = _cfg("learning_rate", default=2e-5, cast=float)
weight_decay     = _cfg("weight_decay", default=0.01, cast=float)
warmup_ratio     = _cfg("warmup_ratio", default=0.06, cast=float)
scheduler_type   = _cfg("lr_scheduler_type", default="cosine")
fp16             = bool(_cfg("fp16", default=True))
grad_ckpt        = bool(_cfg("gradient_checkpointing", default=True))
optim_name       = _cfg("optim", default="adamw_torch")
num_workers      = _cfg("dataloader_num_workers", default=2, cast=int)

# --- TrainingArguments ---
training_args = TrainingArguments(
    output_dir=CONFIG["output_dir"],
    overwrite_output_dir=True,
    num_train_epochs=num_train_epochs,
    per_device_train_batch_size=train_bs,
    per_device_eval_batch_size=eval_bs,
    gradient_accumulation_steps=grad_accum,
    learning_rate=lr,
    weight_decay=weight_decay,
    warmup_ratio=warmup_ratio,
    lr_scheduler_type=scheduler_type,

    # Memory optimization
    fp16=fp16,
    optim=optim_name,
    gradient_checkpointing=grad_ckpt,
    max_grad_norm=1.0,

    # Evaluation/checkpointing
    logging_steps=100,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    save_total_limit=2,
    load_best_model_at_end=True,
    metric_for_best_model="f1_macro",
    greater_is_better=True,

    # Dataloading
    dataloader_num_workers=num_workers,
    dataloader_pin_memory=True,
    dataloader_drop_last=True,

    # Misc
    report_to="none",
    remove_unused_columns=True,
)

# Prefer the encoded dataset alias if present
train_ds = DATASETS["train"] if "DATASETS" in globals() else encoded_ds["train"]
val_ds   = DATASETS["validation"] if "DATASETS" in globals() else encoded_ds["validation"]

# --- Create trainer ---
trainer = AsymmetricLossTrainer(
    model=model,
    args=training_args,
    train_dataset=train_ds,
    eval_dataset=val_ds,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],
)

print("   âœ… Trainer configured with memory optimizations.")
print(f"   - Effective batch size: {train_bs * grad_accum}")


print("Trainer epochs ->", trainer.args.num_train_epochs)  # should be 5.0

# ===================================================================================
# 6. TRAINING (robust to config key names + safer step estimate)
# ===================================================================================
print("\nðŸ‹ï¸ [6/8] Starting FAST model training...")

import os, math, gc, torch

# Resolve config keys regardless of naming style
def _cfg(key, *alts, default=None, cast=None):
    for k in (key,) + alts:
        if k in CONFIG:
            v = CONFIG[k]
            return cast(v) if cast else v
    return default

num_epochs = _cfg("train_epochs", "num_train_epochs", default=3, cast=float)
train_bs   = _cfg("train_batch_size", "per_device_train_batch_size", default=8, cast=int)
grad_accum = _cfg("gradient_accumulation_steps", default=4, cast=int)

# Prefer the alias from Cell 3 if present
train_ds = DATASETS["train"] if "DATASETS" in globals() else encoded_ds["train"]

steps_per_epoch = max(1, math.ceil(len(train_ds) / max(1, train_bs) / max(1, grad_accum)))
total_steps_est = int(steps_per_epoch * num_epochs)

print("   - Estimated training time: 5â€“7 hours")
print(f"   - Steps/epoch (est.): {steps_per_epoch:,}")
print(f"   - Total steps (est.): {total_steps_est:,}")
print(f"   - Effective batch size: {train_bs * grad_accum}")

# Final memory cleanup before training
gc.collect()
if torch.cuda.is_available():
    torch.cuda.empty_cache()

try:
    train_result = trainer.train()
    print("\n   ðŸŽ‰ Training finished successfully!")
    print("   - Final Metrics:", train_result.metrics)

    # Save best model
    best_model_path = os.path.join(CONFIG["output_dir"], "best_model")
    os.makedirs(best_model_path, exist_ok=True)
    model.save_pretrained(best_model_path)
    tokenizer.save_pretrained(best_model_path)
    print(f"   - Best model saved to {best_model_path}")

except torch.cuda.OutOfMemoryError as e:
    print("\n   âŒ OOM error! Try:")
    print("   â€¢ Reduce train_batch_size or increase gradient_accumulation_steps")
    print("   â€¢ Lower max_length (e.g., 128 â†’ 96)")
    print("   â€¢ Keep fp16=True and gradient_checkpointing=True")
    print("   â€¢ Reduce LoRA r (e.g., 16 â†’ 8)")
    raise

# RUN ANYTIME after DATASETS/encoded_ds exist
import numpy as np
val_ds = DATASETS["validation"] if "DATASETS" in globals() else encoded_ds["validation"]
train_ds = DATASETS["train"] if "DATASETS" in globals() else encoded_ds["train"]

Yt = np.stack(train_ds["labels"])
Yv = np.stack(val_ds["labels"])
print("Positives per sample -> train:", Yt.mean(axis=1).mean().round(3), " | val:", Yv.mean(axis=1).mean().round(3))
print("Per-class prevalence (val) min/median/max:",
      float(Yv.mean(axis=0).min()), float(np.median(Yv.mean(axis=0))), float(Yv.mean(axis=0).max()))

# === 6.1 VAL DIAGNOSTICS ===
import numpy as np
from sklearn.metrics import f1_score, classification_report

val_ds = DATASETS["validation"] if "DATASETS" in globals() else encoded_ds["validation"]
pred = trainer.predict(val_ds)
logits = pred.predictions
labels = pred.label_ids.astype(int)
probs = 1.0 / (1.0 + np.exp(-logits))

true_pps = labels.sum() / labels.shape[0]   # avg positives per sample (truth)
pred_pps_05 = (probs >= 0.5).sum() / labels.shape[0]  # avg positives/sample @0.5

print(f"Avg true positives/sample: {true_pps:.3f}")
print(f"Avg preds/sample @0.5:     {pred_pps_05:.3f}")

# Per-class F1 @ 0.5 to spot dead classes
preds05 = (probs >= 0.5).astype(int)
per_class_f1 = []
for j, name in enumerate(LABEL_META["names"] if "LABEL_META" in globals() else LABEL_NAMES):
    f = f1_score(labels[:, j], preds05[:, j], zero_division=0)
    per_class_f1.append((name, float(f), float(labels[:, j].mean())))
per_class_f1_sorted = sorted(per_class_f1, key=lambda x: x[1])
print("\nWorst 10 classes @0.5 threshold (name, f1, prevalence):")
for name, f, prev in per_class_f1_sorted[:10]:
    print(f"{name:14s}  f1={f:0.3f}  prev={prev:0.3f}")

# ===================================================================================
# 7. THRESHOLD OPTIMIZATION (self-contained + fast + robust)
# ===================================================================================
print("\nðŸ” [7/8] Optimizing per-label decision thresholds...")

import os, gc, json, numpy as np
from sklearn.metrics import f1_score

# Clear memory before validation
gc.collect()
if torch.cuda.is_available():
    torch.cuda.empty_cache()

# Pick validation dataset alias
val_ds = DATASETS["validation"] if "DATASETS" in globals() else encoded_ds["validation"]

print("   - Predicting on the validation set...")
pred = trainer.predict(val_ds)
logits = pred.predictions
labels = pred.label_ids.astype(int)
probs  = 1.0 / (1.0 + np.exp(-logits))

# Determine label meta robustly
if "LABEL_META" in globals():
    label_names = LABEL_META["names"]
    num_labels  = LABEL_META["num_labels"]
else:
    label_names = LABEL_NAMES
    num_labels  = len(label_names)

# Base metrics @ 0.5 threshold
base_preds = (probs >= 0.5).astype(int)
base_f1_micro = f1_score(labels, base_preds, average="micro", zero_division=0)
base_f1_macro = f1_score(labels, base_preds, average="macro", zero_division=0)

# Per-label grid search (coarse but fast)
print("   - Searching best F1 threshold per label...")
grid = np.linspace(0.05, 0.60, 12)  # narrower & faster; widen if needed
optimal_thresholds = np.full(num_labels, 0.5, dtype=np.float32)

for j in range(num_labels):
    y = labels[:, j]
    if y.sum() == 0:
        continue  # keep 0.5 if no positives in val for that class
    pj = probs[:, j]
    best_f, best_t = -1.0, 0.5
    for t in grid:
        f = f1_score(y, (pj >= t).astype(int), zero_division=0)
        if f > best_f:
            best_f, best_t = f, t
    optimal_thresholds[j] = best_t

# Tuned metrics
tuned_preds = (probs >= optimal_thresholds.reshape(1, -1)).astype(int)
tuned_f1_micro = f1_score(labels, tuned_preds, average="micro", zero_division=0)
tuned_f1_macro = f1_score(labels, tuned_preds, average="macro", zero_division=0)

print("\n--- Validation (0.5 vs tuned thresholds) ---")
print(f"   F1 Micro @0.5 : {base_f1_micro:.4f}")
print(f"   F1 Macro @0.5 : {base_f1_macro:.4f}")
print(f"   F1 Micro tuned: {tuned_f1_micro:.4f}")
print(f"   F1 Macro tuned: {tuned_f1_macro:.4f}")
if "neutral" in label_names:
    idx = [i for i, n in enumerate(label_names) if n != "neutral"]
    f1_macro_no_neutral = f1_score(labels[:, idx], tuned_preds[:, idx], average="macro", zero_division=0)
    print(f"   F1 Macro tuned (no 'neutral'): {f1_macro_no_neutral:.4f}")
print("--------------------------------------------")

# Save thresholds to both handy locations
os.makedirs(CONFIG["output_dir"], exist_ok=True)
np.save(os.path.join(CONFIG["output_dir"], "val_thresholds.npy"), optimal_thresholds)
with open(os.path.join(CONFIG["output_dir"], "val_thresholds.json"), "w") as f:
    json.dump(optimal_thresholds.tolist(), f)

best_dir = os.path.join(CONFIG["output_dir"], "best_model")
os.makedirs(best_dir, exist_ok=True)
with open(os.path.join(best_dir, "optimal_thresholds.json"), "w") as f:
    json.dump(optimal_thresholds.tolist(), f)

print("   âœ… Thresholds saved: val_thresholds.npy/json and best_model/optimal_thresholds.json")


# ===================================================================================
# 8. INFERENCE DEMO
# ===================================================================================
print("\nðŸŽ¯ [8/8] Running inference demo...")

from peft import PeftModel

# Clear memory for inference
gc.collect()
if torch.cuda.is_available():
    torch.cuda.empty_cache()

final_model_path = os.path.join(CONFIG["output_dir"], "best_model")
with open(os.path.join(final_model_path, "optimal_thresholds.json")) as f:
    final_thresholds = np.array(json.load(f))

# Load model for inference
print("   - Loading model for inference...")
base_model = AutoModelForSequenceClassification.from_pretrained(
    CONFIG["model_name"],
    num_labels=NUM_LABELS,
    problem_type="multi_label_classification",
    id2label=ID2LABEL,
    label2id=LABEL2ID,
    # Don't force dtype
)

inference_model = PeftModel.from_pretrained(base_model, final_model_path)
inference_model.eval()
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
inference_model.to(device)

test_texts = [
    "I'm so happy and grateful for this amazing day!",
    "This is really frustrating and annoying.",
    "I feel a bit anxious but also excited about tomorrow.",
    "The weather is nice today.",
]

print("\n--- Inference Results ---")
with torch.no_grad():
    for text in test_texts:
        inputs = tokenizer(text, return_tensors="pt", truncation=True, 
                          max_length=CONFIG["max_length"], padding=True)
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        outputs = inference_model(**inputs)
        probs = torch.sigmoid(outputs.logits).cpu().numpy()[0]
        
        detected_emotions = [LABEL_NAMES[i] for i, p in enumerate(probs) 
                           if p > final_thresholds[i]]
        
        top_indices = np.argsort(probs)[-3:][::-1]
        top_scores = {LABEL_NAMES[i]: f"{probs[i]:.3f}" for i in top_indices}
        
        print(f"\nText: \"{text}\"")
        print(f"   -> Top: {top_scores}")
        print(f"   -> Detected: {detected_emotions if detected_emotions else ['none']}")

print("\nâœ… All steps completed successfully!")

