{
  "experiment_id": "20250903_173117",
  "timestamp": "2025-09-03T17:31:55.028285",
  "results": {
    "bce_baseline": {
      "success": false,
      "error": "Training failed (code 1)",
      "stdout": "\ud83d\ude80 GoEmotions DeBERTa Training (SCIENTIFIC VERSION)\n============================================================\n\ud83d\udcc1 Output directory: rigorous_experiments/exp_bce_baseline_20250903_173117\n\ud83e\udd16 Model: deberta-v3-large (from local cache)\n\ud83d\udcca Dataset: GoEmotions (from local cache)\n\ud83d\udd2c Scientific logging: ENABLED\n\ud83e\udd16 Loading deberta-v3-large...\n\ud83d\udcc1 Found local cache at models/deberta-v3-large\n\u2705 deberta-v3-large tokenizer loaded from local cache\n\u2705 deberta-v3-large model loaded from local cache\n\ud83d\udcca Loading GoEmotions dataset from local cache...\n\u2705 GoEmotions dataset loaded from local cache\n   Training examples: 43410\n   Validation examples: 5426\n   Total emotions: 28\n\ud83d\udd04 Creating datasets...\n\u2705 Created 43410 training examples\n\u2705 Created 5426 validation examples\n\ud83c\udfaf Using Asymmetric Loss for better class imbalance handling\n\ud83d\ude80 Starting training...\n",
      "stderr": "inner_training_loop(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2672, in _inner_training_loop\n    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 4060, in training_step\n    self.accelerator.backward(loss, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\", line 2730, in backward\n    self.scaler.scale(loss).backward(**kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 525, in backward\n    torch.autograd.backward(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 267, in backward\n    _engine_run_backward(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\", line 744, in _engine_run_backward\n    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\", line 301, in apply\n    return user_fn(self, *args)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py\", line 320, in backward\n    torch.autograd.backward(outputs_with_grad, args_with_grad)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 267, in backward\n    _engine_run_backward(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\", line 744, in _engine_run_backward\n    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.\n\n  0%|          | 0/5427 [00:00<?, ?it/s]\n"
    },
    "asymmetric_loss": {
      "success": false,
      "error": "Training failed (code 1)",
      "stdout": "\ud83d\ude80 GoEmotions DeBERTa Training (SCIENTIFIC VERSION)\n============================================================\n\ud83d\udcc1 Output directory: rigorous_experiments/exp_asymmetric_loss_20250903_173117\n\ud83e\udd16 Model: deberta-v3-large (from local cache)\n\ud83d\udcca Dataset: GoEmotions (from local cache)\n\ud83d\udd2c Scientific logging: ENABLED\n\ud83e\udd16 Loading deberta-v3-large...\n\ud83d\udcc1 Found local cache at models/deberta-v3-large\n\u2705 deberta-v3-large tokenizer loaded from local cache\n\u2705 deberta-v3-large model loaded from local cache\n\ud83d\udcca Loading GoEmotions dataset from local cache...\n\u2705 GoEmotions dataset loaded from local cache\n   Training examples: 43410\n   Validation examples: 5426\n   Total emotions: 28\n\ud83d\udd04 Creating datasets...\n\u2705 Created 43410 training examples\n\u2705 Created 5426 validation examples\n\ud83c\udfaf Using Asymmetric Loss for better class imbalance handling\n\ud83d\ude80 Starting training...\n",
      "stderr": "inner_training_loop(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2672, in _inner_training_loop\n    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 4060, in training_step\n    self.accelerator.backward(loss, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\", line 2730, in backward\n    self.scaler.scale(loss).backward(**kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 525, in backward\n    torch.autograd.backward(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 267, in backward\n    _engine_run_backward(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\", line 744, in _engine_run_backward\n    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\", line 301, in apply\n    return user_fn(self, *args)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py\", line 320, in backward\n    torch.autograd.backward(outputs_with_grad, args_with_grad)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 267, in backward\n    _engine_run_backward(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\", line 744, in _engine_run_backward\n    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.\n\n  0%|          | 0/5427 [00:00<?, ?it/s]\n"
    },
    "combined_loss_07": {
      "success": false,
      "error": "Training failed (code 1)",
      "stdout": "\ud83d\ude80 GoEmotions DeBERTa Training (SCIENTIFIC VERSION)\n============================================================\n\ud83d\udcc1 Output directory: rigorous_experiments/exp_combined_loss_07_20250903_173117\n\ud83e\udd16 Model: deberta-v3-large (from local cache)\n\ud83d\udcca Dataset: GoEmotions (from local cache)\n\ud83d\udd2c Scientific logging: ENABLED\n\ud83e\udd16 Loading deberta-v3-large...\n\ud83d\udcc1 Found local cache at models/deberta-v3-large\n\u2705 deberta-v3-large tokenizer loaded from local cache\n\u2705 deberta-v3-large model loaded from local cache\n\ud83d\udcca Loading GoEmotions dataset from local cache...\n\u2705 GoEmotions dataset loaded from local cache\n   Training examples: 43410\n   Validation examples: 5426\n   Total emotions: 28\n\ud83d\udd04 Creating datasets...\n\u2705 Created 43410 training examples\n\u2705 Created 5426 validation examples\n\ud83d\ude80 Using Combined Loss (ASL + Class Weighting + Focal Loss) for maximum performance\n\ud83d\udcca Loss combination ratio: 0.7 ASL + 0.30000000000000004 Focal\n\ud83d\udcca Class weights computed: tensor([ 0.3754,  0.6660,  0.9894,  0.6277,  0.5275,  1.4263,  1.1333,  0.7076,\n         2.4187,  1.2217,  0.7667,  1.9551,  5.1167,  1.8175,  2.6013,  0.5824,\n        20.1345,  1.0677,  0.7432,  9.4534,  0.9806, 13.9672,  1.3967, 10.1331,\n         2.8447,  1.1692,  1.4626,  0.1090])\n\ud83c\udfaf Loss combination: 0.7 ASL + 0.30000000000000004 Focal\n\ud83d\ude80 Starting training...\n",
      "stderr": "inner_training_loop(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2672, in _inner_training_loop\n    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 4060, in training_step\n    self.accelerator.backward(loss, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\", line 2730, in backward\n    self.scaler.scale(loss).backward(**kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 525, in backward\n    torch.autograd.backward(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 267, in backward\n    _engine_run_backward(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\", line 744, in _engine_run_backward\n    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\", line 301, in apply\n    return user_fn(self, *args)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py\", line 320, in backward\n    torch.autograd.backward(outputs_with_grad, args_with_grad)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 267, in backward\n    _engine_run_backward(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\", line 744, in _engine_run_backward\n    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.\n\n  0%|          | 0/5427 [00:00<?, ?it/s]\n"
    },
    "combined_loss_05": {
      "success": false,
      "error": "Training failed (code 1)",
      "stdout": "\ud83d\ude80 GoEmotions DeBERTa Training (SCIENTIFIC VERSION)\n============================================================\n\ud83d\udcc1 Output directory: rigorous_experiments/exp_combined_loss_05_20250903_173117\n\ud83e\udd16 Model: deberta-v3-large (from local cache)\n\ud83d\udcca Dataset: GoEmotions (from local cache)\n\ud83d\udd2c Scientific logging: ENABLED\n\ud83e\udd16 Loading deberta-v3-large...\n\ud83d\udcc1 Found local cache at models/deberta-v3-large\n\u2705 deberta-v3-large tokenizer loaded from local cache\n\u2705 deberta-v3-large model loaded from local cache\n\ud83d\udcca Loading GoEmotions dataset from local cache...\n\u2705 GoEmotions dataset loaded from local cache\n   Training examples: 43410\n   Validation examples: 5426\n   Total emotions: 28\n\ud83d\udd04 Creating datasets...\n\u2705 Created 43410 training examples\n\u2705 Created 5426 validation examples\n\ud83d\ude80 Using Combined Loss (ASL + Class Weighting + Focal Loss) for maximum performance\n\ud83d\udcca Loss combination ratio: 0.5 ASL + 0.5 Focal\n\ud83d\udcca Class weights computed: tensor([ 0.3754,  0.6660,  0.9894,  0.6277,  0.5275,  1.4263,  1.1333,  0.7076,\n         2.4187,  1.2217,  0.7667,  1.9551,  5.1167,  1.8175,  2.6013,  0.5824,\n        20.1345,  1.0677,  0.7432,  9.4534,  0.9806, 13.9672,  1.3967, 10.1331,\n         2.8447,  1.1692,  1.4626,  0.1090])\n\ud83c\udfaf Loss combination: 0.5 ASL + 0.5 Focal\n\ud83d\ude80 Starting training...\n",
      "stderr": "inner_training_loop(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2672, in _inner_training_loop\n    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 4060, in training_step\n    self.accelerator.backward(loss, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\", line 2730, in backward\n    self.scaler.scale(loss).backward(**kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 525, in backward\n    torch.autograd.backward(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 267, in backward\n    _engine_run_backward(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\", line 744, in _engine_run_backward\n    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\", line 301, in apply\n    return user_fn(self, *args)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py\", line 320, in backward\n    torch.autograd.backward(outputs_with_grad, args_with_grad)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 267, in backward\n    _engine_run_backward(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\", line 744, in _engine_run_backward\n    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.\n\n  0%|          | 0/5427 [00:00<?, ?it/s]\n"
    },
    "combined_loss_03": {
      "success": false,
      "error": "Training failed (code 1)",
      "stdout": "\ud83d\ude80 GoEmotions DeBERTa Training (SCIENTIFIC VERSION)\n============================================================\n\ud83d\udcc1 Output directory: rigorous_experiments/exp_combined_loss_03_20250903_173117\n\ud83e\udd16 Model: deberta-v3-large (from local cache)\n\ud83d\udcca Dataset: GoEmotions (from local cache)\n\ud83d\udd2c Scientific logging: ENABLED\n\ud83e\udd16 Loading deberta-v3-large...\n\ud83d\udcc1 Found local cache at models/deberta-v3-large\n\u2705 deberta-v3-large tokenizer loaded from local cache\n\u2705 deberta-v3-large model loaded from local cache\n\ud83d\udcca Loading GoEmotions dataset from local cache...\n\u2705 GoEmotions dataset loaded from local cache\n   Training examples: 43410\n   Validation examples: 5426\n   Total emotions: 28\n\ud83d\udd04 Creating datasets...\n\u2705 Created 43410 training examples\n\u2705 Created 5426 validation examples\n\ud83d\ude80 Using Combined Loss (ASL + Class Weighting + Focal Loss) for maximum performance\n\ud83d\udcca Loss combination ratio: 0.3 ASL + 0.7 Focal\n\ud83d\udcca Class weights computed: tensor([ 0.3754,  0.6660,  0.9894,  0.6277,  0.5275,  1.4263,  1.1333,  0.7076,\n         2.4187,  1.2217,  0.7667,  1.9551,  5.1167,  1.8175,  2.6013,  0.5824,\n        20.1345,  1.0677,  0.7432,  9.4534,  0.9806, 13.9672,  1.3967, 10.1331,\n         2.8447,  1.1692,  1.4626,  0.1090])\n\ud83c\udfaf Loss combination: 0.3 ASL + 0.7 Focal\n\ud83d\ude80 Starting training...\n",
      "stderr": "inner_training_loop(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2672, in _inner_training_loop\n    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 4060, in training_step\n    self.accelerator.backward(loss, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\", line 2730, in backward\n    self.scaler.scale(loss).backward(**kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 525, in backward\n    torch.autograd.backward(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 267, in backward\n    _engine_run_backward(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\", line 744, in _engine_run_backward\n    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\", line 301, in apply\n    return user_fn(self, *args)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py\", line 320, in backward\n    torch.autograd.backward(outputs_with_grad, args_with_grad)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 267, in backward\n    _engine_run_backward(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\", line 744, in _engine_run_backward\n    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.\n\n  0%|          | 0/5427 [00:00<?, ?it/s]\n"
    }
  }
}