{
  "experiment_id": "20250903_150423",
  "timestamp": "2025-09-03T15:05:04.607636",
  "results": {
    "bce_baseline": {
      "success": false,
      "error": "Training failed (code 1)",
      "stdout": "\ud83d\ude80 GoEmotions DeBERTa Training (SCIENTIFIC VERSION)\n============================================================\n\ud83d\udcc1 Output directory: rigorous_experiments/exp_bce_baseline_20250903_150423\n\ud83e\udd16 Model: deberta-v3-large (from local cache)\n\ud83d\udcca Dataset: GoEmotions (from local cache)\n\ud83d\udd2c Scientific logging: ENABLED\n\ud83e\udd16 Loading deberta-v3-large...\n\ud83d\udcc1 Found local cache at models/deberta-v3-large\n\u2705 deberta-v3-large tokenizer loaded from local cache\n\u2705 deberta-v3-large model loaded from local cache\n\ud83d\udcca Loading GoEmotions dataset from local cache...\n\u2705 GoEmotions dataset loaded from local cache\n   Training examples: 43410\n   Validation examples: 5426\n   Total emotions: 28\n\ud83d\udd04 Creating datasets...\n\u2705 Created 43410 training examples\n\u2705 Created 5426 validation examples\n\ud83c\udfaf Using Asymmetric Loss for better class imbalance handling\n\ud83d\ude80 Starting training...\n",
      "stderr": "ansformers/models/deberta_v2/modeling_deberta_v2.py\", line 659, in forward\n    output_states, attn_weights = layer_module(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_layers.py\", line 94, in __call__\n    return super().__call__(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 438, in forward\n    attention_output, att_matrix = self.attention(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 371, in forward\n    self_output, att_matrix = self.self(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 251, in forward\n    rel_att = self.disentangled_attention_bias(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 330, in disentangled_attention_bias\n    score += c2p_att / scale.to(dtype=c2p_att.dtype)\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU \n\n  0%|          | 0/1357 [00:00<?, ?it/s]\n"
    },
    "asymmetric_loss": {
      "success": false,
      "error": "Training failed (code 1)",
      "stdout": "\ud83d\ude80 GoEmotions DeBERTa Training (SCIENTIFIC VERSION)\n============================================================\n\ud83d\udcc1 Output directory: rigorous_experiments/exp_asymmetric_loss_20250903_150423\n\ud83e\udd16 Model: deberta-v3-large (from local cache)\n\ud83d\udcca Dataset: GoEmotions (from local cache)\n\ud83d\udd2c Scientific logging: ENABLED\n\ud83e\udd16 Loading deberta-v3-large...\n\ud83d\udcc1 Found local cache at models/deberta-v3-large\n\u2705 deberta-v3-large tokenizer loaded from local cache\n\u2705 deberta-v3-large model loaded from local cache\n\ud83d\udcca Loading GoEmotions dataset from local cache...\n\u2705 GoEmotions dataset loaded from local cache\n   Training examples: 43410\n   Validation examples: 5426\n   Total emotions: 28\n\ud83d\udd04 Creating datasets...\n\u2705 Created 43410 training examples\n\u2705 Created 5426 validation examples\n\ud83c\udfaf Using Asymmetric Loss for better class imbalance handling\n\ud83d\ude80 Starting training...\n",
      "stderr": "ansformers/models/deberta_v2/modeling_deberta_v2.py\", line 659, in forward\n    output_states, attn_weights = layer_module(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_layers.py\", line 94, in __call__\n    return super().__call__(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 438, in forward\n    attention_output, att_matrix = self.attention(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 371, in forward\n    self_output, att_matrix = self.self(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 251, in forward\n    rel_att = self.disentangled_attention_bias(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 330, in disentangled_attention_bias\n    score += c2p_att / scale.to(dtype=c2p_att.dtype)\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU \n\n  0%|          | 0/1357 [00:00<?, ?it/s]\n"
    },
    "combined_loss_07": {
      "success": false,
      "error": "Training failed (code 1)",
      "stdout": "\ud83d\ude80 GoEmotions DeBERTa Training (SCIENTIFIC VERSION)\n============================================================\n\ud83d\udcc1 Output directory: rigorous_experiments/exp_combined_loss_07_20250903_150423\n\ud83e\udd16 Model: deberta-v3-large (from local cache)\n\ud83d\udcca Dataset: GoEmotions (from local cache)\n\ud83d\udd2c Scientific logging: ENABLED\n\ud83e\udd16 Loading deberta-v3-large...\n\ud83d\udcc1 Found local cache at models/deberta-v3-large\n\u2705 deberta-v3-large tokenizer loaded from local cache\n\u2705 deberta-v3-large model loaded from local cache\n\ud83d\udcca Loading GoEmotions dataset from local cache...\n\u2705 GoEmotions dataset loaded from local cache\n   Training examples: 43410\n   Validation examples: 5426\n   Total emotions: 28\n\ud83d\udd04 Creating datasets...\n\u2705 Created 43410 training examples\n\u2705 Created 5426 validation examples\n\ud83d\ude80 Using Combined Loss (ASL + Class Weighting + Focal Loss) for maximum performance\n\ud83d\udcca Loss combination ratio: 0.7 ASL + 0.30000000000000004 Focal\n\ud83d\udcca Class weights computed: tensor([ 0.3754,  0.6660,  0.9894,  0.6277,  0.5275,  1.4263,  1.1333,  0.7076,\n         2.4187,  1.2217,  0.7667,  1.9551,  5.1167,  1.8175,  2.6013,  0.5824,\n        20.1345,  1.0677,  0.7432,  9.4534,  0.9806, 13.9672,  1.3967, 10.1331,\n         2.8447,  1.1692,  1.4626,  0.1090])\n\ud83c\udfaf Loss combination: 0.7 ASL + 0.30000000000000004 Focal\n\ud83d\ude80 Starting training...\n",
      "stderr": "ansformers/models/deberta_v2/modeling_deberta_v2.py\", line 659, in forward\n    output_states, attn_weights = layer_module(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_layers.py\", line 94, in __call__\n    return super().__call__(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 438, in forward\n    attention_output, att_matrix = self.attention(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 371, in forward\n    self_output, att_matrix = self.self(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 251, in forward\n    rel_att = self.disentangled_attention_bias(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 330, in disentangled_attention_bias\n    score += c2p_att / scale.to(dtype=c2p_att.dtype)\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU \n\n  0%|          | 0/1357 [00:00<?, ?it/s]\n"
    },
    "combined_loss_05": {
      "success": false,
      "error": "Training failed (code 1)",
      "stdout": "\ud83d\ude80 GoEmotions DeBERTa Training (SCIENTIFIC VERSION)\n============================================================\n\ud83d\udcc1 Output directory: rigorous_experiments/exp_combined_loss_05_20250903_150423\n\ud83e\udd16 Model: deberta-v3-large (from local cache)\n\ud83d\udcca Dataset: GoEmotions (from local cache)\n\ud83d\udd2c Scientific logging: ENABLED\n\ud83e\udd16 Loading deberta-v3-large...\n\ud83d\udcc1 Found local cache at models/deberta-v3-large\n\u2705 deberta-v3-large tokenizer loaded from local cache\n\u2705 deberta-v3-large model loaded from local cache\n\ud83d\udcca Loading GoEmotions dataset from local cache...\n\u2705 GoEmotions dataset loaded from local cache\n   Training examples: 43410\n   Validation examples: 5426\n   Total emotions: 28\n\ud83d\udd04 Creating datasets...\n\u2705 Created 43410 training examples\n\u2705 Created 5426 validation examples\n\ud83d\ude80 Using Combined Loss (ASL + Class Weighting + Focal Loss) for maximum performance\n\ud83d\udcca Loss combination ratio: 0.5 ASL + 0.5 Focal\n\ud83d\udcca Class weights computed: tensor([ 0.3754,  0.6660,  0.9894,  0.6277,  0.5275,  1.4263,  1.1333,  0.7076,\n         2.4187,  1.2217,  0.7667,  1.9551,  5.1167,  1.8175,  2.6013,  0.5824,\n        20.1345,  1.0677,  0.7432,  9.4534,  0.9806, 13.9672,  1.3967, 10.1331,\n         2.8447,  1.1692,  1.4626,  0.1090])\n\ud83c\udfaf Loss combination: 0.5 ASL + 0.5 Focal\n\ud83d\ude80 Starting training...\n",
      "stderr": "ansformers/models/deberta_v2/modeling_deberta_v2.py\", line 659, in forward\n    output_states, attn_weights = layer_module(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_layers.py\", line 94, in __call__\n    return super().__call__(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 438, in forward\n    attention_output, att_matrix = self.attention(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 371, in forward\n    self_output, att_matrix = self.self(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 251, in forward\n    rel_att = self.disentangled_attention_bias(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 330, in disentangled_attention_bias\n    score += c2p_att / scale.to(dtype=c2p_att.dtype)\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU \n\n  0%|          | 0/1357 [00:00<?, ?it/s]\n"
    },
    "combined_loss_03": {
      "success": false,
      "error": "Training failed (code 1)",
      "stdout": "\ud83d\ude80 GoEmotions DeBERTa Training (SCIENTIFIC VERSION)\n============================================================\n\ud83d\udcc1 Output directory: rigorous_experiments/exp_combined_loss_03_20250903_150423\n\ud83e\udd16 Model: deberta-v3-large (from local cache)\n\ud83d\udcca Dataset: GoEmotions (from local cache)\n\ud83d\udd2c Scientific logging: ENABLED\n\ud83e\udd16 Loading deberta-v3-large...\n\ud83d\udcc1 Found local cache at models/deberta-v3-large\n\u2705 deberta-v3-large tokenizer loaded from local cache\n\u2705 deberta-v3-large model loaded from local cache\n\ud83d\udcca Loading GoEmotions dataset from local cache...\n\u2705 GoEmotions dataset loaded from local cache\n   Training examples: 43410\n   Validation examples: 5426\n   Total emotions: 28\n\ud83d\udd04 Creating datasets...\n\u2705 Created 43410 training examples\n\u2705 Created 5426 validation examples\n\ud83d\ude80 Using Combined Loss (ASL + Class Weighting + Focal Loss) for maximum performance\n\ud83d\udcca Loss combination ratio: 0.3 ASL + 0.7 Focal\n\ud83d\udcca Class weights computed: tensor([ 0.3754,  0.6660,  0.9894,  0.6277,  0.5275,  1.4263,  1.1333,  0.7076,\n         2.4187,  1.2217,  0.7667,  1.9551,  5.1167,  1.8175,  2.6013,  0.5824,\n        20.1345,  1.0677,  0.7432,  9.4534,  0.9806, 13.9672,  1.3967, 10.1331,\n         2.8447,  1.1692,  1.4626,  0.1090])\n\ud83c\udfaf Loss combination: 0.3 ASL + 0.7 Focal\n\ud83d\ude80 Starting training...\n",
      "stderr": "ansformers/models/deberta_v2/modeling_deberta_v2.py\", line 659, in forward\n    output_states, attn_weights = layer_module(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_layers.py\", line 94, in __call__\n    return super().__call__(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 438, in forward\n    attention_output, att_matrix = self.attention(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 371, in forward\n    self_output, att_matrix = self.self(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 251, in forward\n    rel_att = self.disentangled_attention_bias(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 330, in disentangled_attention_bias\n    score += c2p_att / scale.to(dtype=c2p_att.dtype)\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU \n\n  0%|          | 0/1357 [00:00<?, ?it/s]\n"
    }
  }
}