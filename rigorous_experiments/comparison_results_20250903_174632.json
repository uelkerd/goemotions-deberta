{
  "experiment_id": "20250903_174632",
  "timestamp": "2025-09-03T17:46:39.425930",
  "results": {
    "bce_baseline": {
      "success": false,
      "error": "Training failed (code 1)",
      "stdout": "\ud83d\ude80 GoEmotions DeBERTa Training (SCIENTIFIC VERSION)\n============================================================\n\ud83d\udcc1 Output directory: rigorous_experiments/exp_bce_baseline_20250903_174632\n\ud83e\udd16 Model: deberta-v3-large (from local cache)\n\ud83d\udcca Dataset: GoEmotions (from local cache)\n\ud83d\udd2c Scientific logging: ENABLED\n\ud83e\udd16 Loading deberta-v3-large...\n\ud83d\udcc1 Found local cache at models/deberta-v3-large\n\u2705 deberta-v3-large tokenizer loaded from local cache\n\u2705 deberta-v3-large model loaded from local cache\n\ud83d\udcca Loading GoEmotions dataset from local cache...\n\u2705 GoEmotions dataset loaded from local cache\n   Training examples: 43410\n   Validation examples: 5426\n   Total emotions: 28\n\ud83d\udd04 Creating datasets...\n\u2705 Created 43410 training examples\n\u2705 Created 5426 validation examples\n\ud83d\ude80 Using gradient checkpointing for memory efficiency\n\ud83d\udcca Using standard BCE Loss\n\ud83d\ude80 Starting training...\n",
      "stderr": "inner_training_loop(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2672, in _inner_training_loop\n    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 4060, in training_step\n    self.accelerator.backward(loss, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\", line 2730, in backward\n    self.scaler.scale(loss).backward(**kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 525, in backward\n    torch.autograd.backward(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 267, in backward\n    _engine_run_backward(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\", line 744, in _engine_run_backward\n    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\", line 301, in apply\n    return user_fn(self, *args)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py\", line 320, in backward\n    torch.autograd.backward(outputs_with_grad, args_with_grad)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 267, in backward\n    _engine_run_backward(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\", line 744, in _engine_run_backward\n    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\nRuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.\n\n  0%|          | 0/5427 [00:00<?, ?it/s]\n"
    }
  }
}