{
  "experiment_id": "20250903_215459",
  "timestamp": "2025-09-03T21:56:26.496398",
  "results": {
    "bce_baseline": {
      "success": false,
      "error": "Training failed (code 1)",
      "stdout": "\ud83d\ude80 GoEmotions DeBERTa Training (SCIENTIFIC VERSION)\n============================================================\n\ud83d\udcc1 Output directory: rigorous_experiments/exp_bce_baseline_20250903_215459\n\ud83e\udd16 Model: deberta-v3-large (from local cache)\n\ud83d\udcca Dataset: GoEmotions (from local cache)\n\ud83d\udd2c Scientific logging: ENABLED\n\ud83e\udd16 Loading deberta-v3-large...\n\ud83d\udcc1 Found local cache at models/deberta-v3-large\n\u2705 deberta-v3-large tokenizer loaded from local cache\n\u2705 deberta-v3-large model loaded from local cache\n\ud83d\udcca Loading GoEmotions dataset from local cache...\n\u2705 GoEmotions dataset loaded from local cache\n   Training examples: 43410\n   Validation examples: 5426\n   Total emotions: 28\n\ud83d\udd04 Creating datasets...\n\u2705 Created 43410 training examples\n\u2705 Created 5426 validation examples\n\ud83d\udd27 Disabling gradient checkpointing to prevent RuntimeError during backward pass\n\ud83d\udcca Using standard BCE Loss\n\ud83d\ude80 Starting training...\n",
      "stderr": "gs)\n  File \"/venv/deberta-v3/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 438, in forward\n    attention_output, att_matrix = self.attention(\n  File \"/venv/deberta-v3/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/venv/deberta-v3/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/venv/deberta-v3/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 371, in forward\n    self_output, att_matrix = self.self(\n  File \"/venv/deberta-v3/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/venv/deberta-v3/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/venv/deberta-v3/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 251, in forward\n    rel_att = self.disentangled_attention_bias(\n  File \"/venv/deberta-v3/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 330, in disentangled_attention_bias\n    score += c2p_att / scale.to(dtype=c2p_att.dtype)\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 23.68 GiB of which 12.88 MiB is free. Process 1245685 has 12.58 GiB memory in use. Process 1346699 has 11.08 GiB memory in use. Of the allocated memory 10.52 GiB is allocated by PyTorch, and 174.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n  0%|          | 1/2714 [00:09<7:04:44,  9.39s/it]\n"
    },
    "asymmetric_loss": {
      "success": false,
      "error": "Training failed (code 1)",
      "stdout": "\ud83d\ude80 GoEmotions DeBERTa Training (SCIENTIFIC VERSION)\n============================================================\n\ud83d\udcc1 Output directory: rigorous_experiments/exp_asymmetric_loss_20250903_215459\n\ud83e\udd16 Model: deberta-v3-large (from local cache)\n\ud83d\udcca Dataset: GoEmotions (from local cache)\n\ud83d\udd2c Scientific logging: ENABLED\n\ud83e\udd16 Loading deberta-v3-large...\n\ud83d\udcc1 Found local cache at models/deberta-v3-large\n\u2705 deberta-v3-large tokenizer loaded from local cache\n\u2705 deberta-v3-large model loaded from local cache\n\ud83d\udcca Loading GoEmotions dataset from local cache...\n\u2705 GoEmotions dataset loaded from local cache\n   Training examples: 43410\n   Validation examples: 5426\n   Total emotions: 28\n\ud83d\udd04 Creating datasets...\n\u2705 Created 43410 training examples\n\u2705 Created 5426 validation examples\n\ud83d\udd27 Disabling gradient checkpointing to prevent RuntimeError during backward pass\n\ud83c\udfaf Using Asymmetric Loss for better class imbalance handling\n\ud83d\ude80 Starting training...\n",
      "stderr": "gs)\n  File \"/venv/deberta-v3/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 438, in forward\n    attention_output, att_matrix = self.attention(\n  File \"/venv/deberta-v3/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/venv/deberta-v3/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/venv/deberta-v3/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 371, in forward\n    self_output, att_matrix = self.self(\n  File \"/venv/deberta-v3/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/venv/deberta-v3/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/venv/deberta-v3/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 251, in forward\n    rel_att = self.disentangled_attention_bias(\n  File \"/venv/deberta-v3/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 330, in disentangled_attention_bias\n    score += c2p_att / scale.to(dtype=c2p_att.dtype)\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 23.68 GiB of which 12.88 MiB is free. Process 1245685 has 12.58 GiB memory in use. Process 1348228 has 11.08 GiB memory in use. Of the allocated memory 10.52 GiB is allocated by PyTorch, and 174.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n  0%|          | 1/2714 [00:09<6:59:12,  9.27s/it]\n"
    },
    "combined_loss_07": {
      "success": false,
      "error": "Training failed (code 1)",
      "stdout": "\ud83d\ude80 GoEmotions DeBERTa Training (SCIENTIFIC VERSION)\n============================================================\n\ud83d\udcc1 Output directory: rigorous_experiments/exp_combined_loss_07_20250903_215459\n\ud83e\udd16 Model: deberta-v3-large (from local cache)\n\ud83d\udcca Dataset: GoEmotions (from local cache)\n\ud83d\udd2c Scientific logging: ENABLED\n\ud83e\udd16 Loading deberta-v3-large...\n\ud83d\udcc1 Found local cache at models/deberta-v3-large\n\u2705 deberta-v3-large tokenizer loaded from local cache\n\u2705 deberta-v3-large model loaded from local cache\n\ud83d\udcca Loading GoEmotions dataset from local cache...\n\u2705 GoEmotions dataset loaded from local cache\n   Training examples: 43410\n   Validation examples: 5426\n   Total emotions: 28\n\ud83d\udd04 Creating datasets...\n\u2705 Created 43410 training examples\n\u2705 Created 5426 validation examples\n\ud83d\udd27 Disabling gradient checkpointing to prevent RuntimeError during backward pass\n\ud83d\ude80 Using Combined Loss (ASL + Class Weighting + Focal Loss) for maximum performance\n\ud83d\udcca Loss combination ratio: 0.7 ASL + 0.30000000000000004 Focal\n\ud83d\udcca Class weights computed: tensor([ 0.3754,  0.6660,  0.9894,  0.6277,  0.5275,  1.4263,  1.1333,  0.7076,\n         2.4187,  1.2217,  0.7667,  1.9551,  5.1167,  1.8175,  2.6013,  0.5824,\n        20.1345,  1.0677,  0.7432,  9.4534,  0.9806, 13.9672,  1.3967, 10.1331,\n         2.8447,  1.1692,  1.4626,  0.1090])\n\ud83c\udfaf Loss combination: 0.7 ASL + 0.30000000000000004 Focal\n\ud83d\ude80 Starting training...\n",
      "stderr": "gs)\n  File \"/venv/deberta-v3/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 438, in forward\n    attention_output, att_matrix = self.attention(\n  File \"/venv/deberta-v3/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/venv/deberta-v3/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/venv/deberta-v3/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 371, in forward\n    self_output, att_matrix = self.self(\n  File \"/venv/deberta-v3/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/venv/deberta-v3/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/venv/deberta-v3/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 251, in forward\n    rel_att = self.disentangled_attention_bias(\n  File \"/venv/deberta-v3/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 330, in disentangled_attention_bias\n    score += c2p_att / scale.to(dtype=c2p_att.dtype)\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 23.68 GiB of which 12.88 MiB is free. Process 1245685 has 12.58 GiB memory in use. Process 1349516 has 11.08 GiB memory in use. Of the allocated memory 10.50 GiB is allocated by PyTorch, and 190.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n  0%|          | 1/2714 [00:08<6:44:32,  8.95s/it]\n"
    },
    "combined_loss_05": {
      "success": false,
      "error": "Training failed (code 1)",
      "stdout": "\ud83d\ude80 GoEmotions DeBERTa Training (SCIENTIFIC VERSION)\n============================================================\n\ud83d\udcc1 Output directory: rigorous_experiments/exp_combined_loss_05_20250903_215459\n\ud83e\udd16 Model: deberta-v3-large (from local cache)\n\ud83d\udcca Dataset: GoEmotions (from local cache)\n\ud83d\udd2c Scientific logging: ENABLED\n\ud83e\udd16 Loading deberta-v3-large...\n\ud83d\udcc1 Found local cache at models/deberta-v3-large\n\u2705 deberta-v3-large tokenizer loaded from local cache\n\u2705 deberta-v3-large model loaded from local cache\n\ud83d\udcca Loading GoEmotions dataset from local cache...\n\u2705 GoEmotions dataset loaded from local cache\n   Training examples: 43410\n   Validation examples: 5426\n   Total emotions: 28\n\ud83d\udd04 Creating datasets...\n\u2705 Created 43410 training examples\n\u2705 Created 5426 validation examples\n\ud83d\udd27 Disabling gradient checkpointing to prevent RuntimeError during backward pass\n\ud83d\ude80 Using Combined Loss (ASL + Class Weighting + Focal Loss) for maximum performance\n\ud83d\udcca Loss combination ratio: 0.5 ASL + 0.5 Focal\n\ud83d\udcca Class weights computed: tensor([ 0.3754,  0.6660,  0.9894,  0.6277,  0.5275,  1.4263,  1.1333,  0.7076,\n         2.4187,  1.2217,  0.7667,  1.9551,  5.1167,  1.8175,  2.6013,  0.5824,\n        20.1345,  1.0677,  0.7432,  9.4534,  0.9806, 13.9672,  1.3967, 10.1331,\n         2.8447,  1.1692,  1.4626,  0.1090])\n\ud83c\udfaf Loss combination: 0.5 ASL + 0.5 Focal\n\ud83d\ude80 Starting training...\n",
      "stderr": "gs)\n  File \"/venv/deberta-v3/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 438, in forward\n    attention_output, att_matrix = self.attention(\n  File \"/venv/deberta-v3/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/venv/deberta-v3/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/venv/deberta-v3/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 371, in forward\n    self_output, att_matrix = self.self(\n  File \"/venv/deberta-v3/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/venv/deberta-v3/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/venv/deberta-v3/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 251, in forward\n    rel_att = self.disentangled_attention_bias(\n  File \"/venv/deberta-v3/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 330, in disentangled_attention_bias\n    score += c2p_att / scale.to(dtype=c2p_att.dtype)\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 23.68 GiB of which 16.88 MiB is free. Process 1245685 has 12.58 GiB memory in use. Process 1350931 has 11.08 GiB memory in use. Of the allocated memory 10.50 GiB is allocated by PyTorch, and 186.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n  0%|          | 1/2714 [00:09<6:57:11,  9.23s/it]\n"
    },
    "combined_loss_03": {
      "success": false,
      "error": "Training failed (code 1)",
      "stdout": "\ud83d\ude80 GoEmotions DeBERTa Training (SCIENTIFIC VERSION)\n============================================================\n\ud83d\udcc1 Output directory: rigorous_experiments/exp_combined_loss_03_20250903_215459\n\ud83e\udd16 Model: deberta-v3-large (from local cache)\n\ud83d\udcca Dataset: GoEmotions (from local cache)\n\ud83d\udd2c Scientific logging: ENABLED\n\ud83e\udd16 Loading deberta-v3-large...\n\ud83d\udcc1 Found local cache at models/deberta-v3-large\n\u2705 deberta-v3-large tokenizer loaded from local cache\n\u2705 deberta-v3-large model loaded from local cache\n\ud83d\udcca Loading GoEmotions dataset from local cache...\n\u2705 GoEmotions dataset loaded from local cache\n   Training examples: 43410\n   Validation examples: 5426\n   Total emotions: 28\n\ud83d\udd04 Creating datasets...\n\u2705 Created 43410 training examples\n\u2705 Created 5426 validation examples\n\ud83d\udd27 Disabling gradient checkpointing to prevent RuntimeError during backward pass\n\ud83d\ude80 Using Combined Loss (ASL + Class Weighting + Focal Loss) for maximum performance\n\ud83d\udcca Loss combination ratio: 0.3 ASL + 0.7 Focal\n\ud83d\udcca Class weights computed: tensor([ 0.3754,  0.6660,  0.9894,  0.6277,  0.5275,  1.4263,  1.1333,  0.7076,\n         2.4187,  1.2217,  0.7667,  1.9551,  5.1167,  1.8175,  2.6013,  0.5824,\n        20.1345,  1.0677,  0.7432,  9.4534,  0.9806, 13.9672,  1.3967, 10.1331,\n         2.8447,  1.1692,  1.4626,  0.1090])\n\ud83c\udfaf Loss combination: 0.3 ASL + 0.7 Focal\n\ud83d\ude80 Starting training...\n",
      "stderr": "rgs)\n  File \"/venv/deberta-v3/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 438, in forward\n    attention_output, att_matrix = self.attention(\n  File \"/venv/deberta-v3/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/venv/deberta-v3/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/venv/deberta-v3/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 371, in forward\n    self_output, att_matrix = self.self(\n  File \"/venv/deberta-v3/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/venv/deberta-v3/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/venv/deberta-v3/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 251, in forward\n    rel_att = self.disentangled_attention_bias(\n  File \"/venv/deberta-v3/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\", line 330, in disentangled_attention_bias\n    score += c2p_att / scale.to(dtype=c2p_att.dtype)\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 23.68 GiB of which 4.88 MiB is free. Process 1245685 has 12.58 GiB memory in use. Process 1352378 has 11.09 GiB memory in use. Of the allocated memory 10.52 GiB is allocated by PyTorch, and 182.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n\n\n  0%|          | 1/2714 [00:09<7:02:19,  9.34s/it]\n"
    }
  }
}