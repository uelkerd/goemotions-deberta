{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18602aa3-918c-4947-98e1-945d33e7ef92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Sep  2 22:58:10 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        On  |   00000000:C1:00.0 Off |                  N/A |\n",
      "| 30%   26C    P8             38W /  350W |       2MiB /  24576MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 3090        On  |   00000000:C2:00.0 Off |                  N/A |\n",
      "| 30%   26C    P8             40W /  350W |       2MiB /  24576MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: Cannot install accelerate==0.33.0, accelerate==0.34.0, accelerate==0.34.1, accelerate==0.34.2, accelerate==1.0.0, accelerate==1.0.1, accelerate==1.1.0, accelerate==1.1.1, accelerate==1.10.0, accelerate==1.10.1, accelerate==1.2.0, accelerate==1.2.1, accelerate==1.3.0, accelerate==1.4.0, accelerate==1.5.0, accelerate==1.5.1, accelerate==1.5.2, accelerate==1.6.0, accelerate==1.7.0, accelerate==1.8.0, accelerate==1.8.1, accelerate==1.9.0, transformers and transformers==4.35.0 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "!pip -q install --upgrade pip\n",
    "!pip -q install torch==2.3.1+cu118 torchvision==0.18.1+cu118 torchaudio==2.3.1+cu118 --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip -q install \"transformers==4.35.0\" \"accelerate==0.24.0\" \"datasets>=2.20\" \"evaluate\" \"scikit-learn\" \"peft>=0.11\" \"tensorboard\" \"pyarrow<18\" \"tiktoken\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee68e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.11.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2025.9.1)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2025.6.15)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "🔧 Transformers version: 4.56.0\n",
      "🎯 Target version: 4.35.0 (compatible with DeBERTa-v3-large)\n",
      "⚠️  Version mismatch - may need to restart kernel\n"
     ]
    }
   ],
   "source": [
    "# Check current versions and model compatibility\n",
    "import transformers\n",
    "import accelerate\n",
    "print(f\"🔧 Transformers version: {transformers.__version__}\")\n",
    "print(f\"🚀 Accelerate version: {accelerate.__version__}\")\n",
    "print(f\"🎯 Using RoBERTa-large - stable and compatible with current versions\")\n",
    "print(\"✅ RoBERTa-large works with any transformers version and no tiktoken issues\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04dbd738-9657-4f25-bff9-2b197a8d201e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🛡️  Script is PROTECTED (read-only) - cannot be modified!\n",
      "✅ Script has complete main function with training logic!\n",
      "📄 Script: train_samo.py\n",
      "📏 Size: 8948 bytes\n",
      "🔐 SHA256: 0b4eb0fb109a48e58dd8d4fec6a267eea317664fd31199780dd6bf4eb8950e3e\n",
      "🚀 Script is ready for training and protected from modification!\n"
     ]
    }
   ],
   "source": [
    "# Check training script status (PROTECTED - READ-ONLY)\n",
    "import os\n",
    "import stat\n",
    "import hashlib\n",
    "\n",
    "if os.path.exists(\"train_samo.py\"):\n",
    "    # Check file permissions\n",
    "    file_stat = os.stat(\"train_samo.py\")\n",
    "    is_readonly = not (file_stat.st_mode & stat.S_IWRITE)\n",
    "    \n",
    "    if is_readonly:\n",
    "        print(\"🛡️  Script is PROTECTED (read-only) - cannot be modified!\")\n",
    "    else:\n",
    "        print(\"⚠️  Script is writable - may be overwritten\")\n",
    "    \n",
    "    # Check if main function exists\n",
    "    with open(\"train_samo.py\", \"r\") as f:\n",
    "        content = f.read()\n",
    "        if \"def main():\" in content and \"trainer.train()\" in content:\n",
    "            print(\"✅ Script has complete main function with training logic!\")\n",
    "        elif \"def main():\" in content:\n",
    "            print(\"✅ Script has main function\")\n",
    "        else:\n",
    "            print(\"❌ Script missing main function!\")\n",
    "    \n",
    "    # Show script info\n",
    "    print(\"📄 Script: train_samo.py\")\n",
    "    print(\"📏 Size:\", os.path.getsize(\"train_samo.py\"), \"bytes\")\n",
    "    print(\"🔐 SHA256:\", hashlib.sha256(open(\"train_samo.py\",'rb').read()).hexdigest())\n",
    "    \n",
    "    if is_readonly:\n",
    "        print(\"🚀 Script is ready for training and protected from modification!\")\n",
    "else:\n",
    "    print(\"❌ Script not found!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c162df17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Training script is complete with main function\n",
      "🚀 Ready to start training!\n",
      "📄 Training script: train_samo.py\n",
      "📏 Size: 8948 bytes\n",
      "🔐 SHA256: 0b4eb0fb109a48e58dd8d4fec6a267eea317664fd31199780dd6bf4eb8950e3e\n",
      "✅ Script is ready for training!\n"
     ]
    }
   ],
   "source": [
    "# Verify the training script is complete and ready\n",
    "import os\n",
    "import hashlib\n",
    "\n",
    "# Check if main function exists\n",
    "with open(\"train_samo.py\", \"r\") as f:\n",
    "    content = f.read()\n",
    "    if \"def main():\" in content and \"if __name__ == \\\"__main__\\\":\" in content:\n",
    "        print(\"✅ Training script is complete with main function\")\n",
    "        print(\"🚀 Ready to start training!\")\n",
    "    else:\n",
    "        print(\"❌ Training script is missing main function!\")\n",
    "        print(\"The script will not work without the main() function.\")\n",
    "\n",
    "# Show script info\n",
    "if os.path.exists(\"train_samo.py\"):\n",
    "    print(\"📄 Training script: train_samo.py\")\n",
    "    print(\"📏 Size:\", os.path.getsize(\"train_samo.py\"), \"bytes\")\n",
    "    print(\"🔐 SHA256:\", hashlib.sha256(open(\"train_samo.py\",'rb').read()).hexdigest())\n",
    "    print(\"✅ Script is ready for training!\")\n",
    "else:\n",
    "    print(\"❌ Training script not found!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efe8e70c-f45c-4c2d-982a-e4ee94bbfac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration already exists at /workspace/.hf_home/accelerate/default_config.yaml, will not override. Run `accelerate config` manually or pass a different `save_location`.\n",
      "Accelerate config patched:\n",
      " {\n",
      "  \"compute_environment\": \"LOCAL_MACHINE\",\n",
      "  \"debug\": false,\n",
      "  \"distributed_type\": \"MULTI_GPU\",\n",
      "  \"downcast_bf16\": false,\n",
      "  \"enable_cpu_affinity\": false,\n",
      "  \"machine_rank\": 0,\n",
      "  \"main_training_function\": \"main\",\n",
      "  \"mixed_precision\": \"fp16\",\n",
      "  \"num_machines\": 1,\n",
      "  \"num_processes\": 2,\n",
      "  \"rdzv_backend\": \"static\",\n",
      "  \"same_network\": false,\n",
      "  \"tpu_use_cluster\": false,\n",
      "  \"tpu_use_sudo\": false,\n",
      "  \"use_cpu\": false\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "accelerate config default\n",
    "CONFIG=/workspace/.hf_home/accelerate/default_config.yaml\n",
    "python3 - <<'PY'\n",
    "import json\n",
    "from pathlib import Path\n",
    "p = Path(\"/workspace/.hf_home/accelerate/default_config.yaml\")\n",
    "config = json.loads(p.read_text())\n",
    "config['distributed_type'] = 'MULTI_GPU'\n",
    "config['mixed_precision'] = 'fp16'\n",
    "config['num_processes'] = 2\n",
    "p.write_text(json.dumps(config, indent=2))\n",
    "print(\"Accelerate config patched:\\n\", p.read_text())\n",
    "PY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a61bc9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ HF_TOKEN is set\n"
     ]
    }
   ],
   "source": [
    "# Set up Hugging Face authentication (optional but recommended)\n",
    "# You can get a token from: https://huggingface.co/settings/tokens\n",
    "import os\n",
    "\n",
    "# Option 1: Set environment variable (recommended for security)\n",
    "os.environ[\"HF_TOKEN\"] = \"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\"\n",
    "\n",
    "# Option 2: Use huggingface_hub login (interactive)\n",
    "# from huggingface_hub import login\n",
    "# login()  # This will prompt you to enter your token\n",
    "\n",
    "# Option 3: Check if token is already set\n",
    "if os.getenv(\"HF_TOKEN\"):\n",
    "    print(\"✅ HF_TOKEN is set\")\n",
    "else:\n",
    "    print(\"⚠️  HF_TOKEN not set - you may hit rate limits\")\n",
    "    print(\"To set it: os.environ['HF_TOKEN'] = 'your_token_here'\")\n",
    "    print(\"Or get a token from: https://huggingface.co/settings/tokens\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0860fd59-19f5-4545-abd8-df16e30f5cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "OUT_DIR = \"./samo_out\"\n",
    "MODEL_NAME = \"roberta-large\"  # Stable, powerful model without tiktoken issues\n",
    "!mkdir -p \"$OUT_DIR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12866914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Testing tokenizer loading...\n",
      "❌ Tokenizer loading failed: Converting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast converters: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']\n",
      "🔄 Try a different model or check dependencies\n"
     ]
    }
   ],
   "source": [
    "# Test tokenizer loading before full training\n",
    "print(\"🧪 Testing tokenizer loading...\")\n",
    "try:\n",
    "    from transformers import AutoTokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    print(f\"✅ Tokenizer loaded successfully for {MODEL_NAME}\")\n",
    "    print(f\"📏 Vocab size: {tokenizer.vocab_size}\")\n",
    "    \n",
    "    # Test tokenization\n",
    "    test_text = \"I love this movie! It's amazing.\"\n",
    "    tokens = tokenizer(test_text, return_tensors=\"pt\")\n",
    "    print(f\"🔤 Test tokenization: '{test_text}' -> {tokens['input_ids'].shape}\")\n",
    "    print(\"🚀 Ready for training!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Tokenizer loading failed: {e}\")\n",
    "    print(\"🔄 Try a different model or check dependencies\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90792e0a-3c67-4c8d-ade7-8e34fc9700df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 SAMO - GoEmotions Multi-Label Trainer\n",
      "📁 Output directory: ./samo_out\n",
      "🤖 Model: microsoft/deberta-v3-large\n",
      "📊 Loading GoEmotions dataset...\n",
      "🔑 Authenticating with Hugging Face...\n",
      "🚀 SAMO - GoEmotions Multi-Label Trainer\n",
      "📁 Output directory: ./samo_out\n",
      "🤖 Model: microsoft/deberta-v3-large\n",
      "📊 Loading GoEmotions dataset...\n",
      "🔑 Authenticating with Hugging Face...\n",
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
      "✅ Successfully authenticated with Hugging Face\n",
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
      "✅ Successfully authenticated with Hugging Face\n",
      "✅ Dataset loaded: 43410 train, 5427 test examples\n",
      "🔄 Preparing data...\n",
      "✅ Dataset loaded: 43410 train, 5427 test examples\n",
      "🔄 Preparing data...\n",
      "📈 Prepared 43410 training examples, 10853 validation examples\n",
      "📈 Prepared 43410 training examples, 10853 validation examples\n",
      "💾 Saved data to ./samo_out/train.jsonl and ./samo_out/val.jsonl\n",
      "🤖 Loading model and tokenizer...\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py\", line 1737, in convert_slow_tokenizer\n",
      "    ).converted()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py\", line 1631, in converted\n",
      "    tokenizer = self.tokenizer()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py\", line 1624, in tokenizer\n",
      "    vocab_scores, merges = self.extract_vocab_merges_from_model(self.vocab_file)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py\", line 1600, in extract_vocab_merges_from_model\n",
      "    bpe_ranks = load_tiktoken_bpe(tiktoken_url)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tiktoken/load.py\", line 158, in load_tiktoken_bpe\n",
      "    contents = read_file_cached(tiktoken_bpe_file, expected_hash)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tiktoken/load.py\", line 48, in read_file_cached\n",
      "    cache_key = hashlib.sha1(blobpath.encode()).hexdigest()\n",
      "AttributeError: 'NoneType' object has no attribute 'encode'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/train_samo.py\", line 249, in <module>\n",
      "    main()\n",
      "  File \"/home/user/train_samo.py\", line 186, in main\n",
      "    tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py\", line 1144, in from_pretrained\n",
      "    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\", line 2070, in from_pretrained\n",
      "    return cls._from_pretrained(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\", line 2316, in _from_pretrained\n",
      "    tokenizer = cls(*init_inputs, **init_kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/deberta_v2/tokenization_deberta_v2_fast.py\", line 103, in __init__\n",
      "    super().__init__(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_fast.py\", line 139, in __init__\n",
      "    fast_tokenizer = convert_slow_tokenizer(self, from_tiktoken=True)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py\", line 1739, in convert_slow_tokenizer\n",
      "    raise ValueError(\n",
      "ValueError: Converting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast converters: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']\n",
      "💾 Saved data to ./samo_out/train.jsonl and ./samo_out/val.jsonl\n",
      "🤖 Loading model and tokenizer...\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py\", line 1737, in convert_slow_tokenizer\n",
      "    ).converted()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py\", line 1631, in converted\n",
      "    tokenizer = self.tokenizer()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py\", line 1624, in tokenizer\n",
      "    vocab_scores, merges = self.extract_vocab_merges_from_model(self.vocab_file)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py\", line 1600, in extract_vocab_merges_from_model\n",
      "    bpe_ranks = load_tiktoken_bpe(tiktoken_url)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tiktoken/load.py\", line 158, in load_tiktoken_bpe\n",
      "    contents = read_file_cached(tiktoken_bpe_file, expected_hash)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/tiktoken/load.py\", line 48, in read_file_cached\n",
      "    cache_key = hashlib.sha1(blobpath.encode()).hexdigest()\n",
      "AttributeError: 'NoneType' object has no attribute 'encode'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/train_samo.py\", line 249, in <module>\n",
      "    main()\n",
      "  File \"/home/user/train_samo.py\", line 186, in main\n",
      "    tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py\", line 1144, in from_pretrained\n",
      "    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\", line 2070, in from_pretrained\n",
      "    return cls._from_pretrained(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\", line 2316, in _from_pretrained\n",
      "    tokenizer = cls(*init_inputs, **init_kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/deberta_v2/tokenization_deberta_v2_fast.py\", line 103, in __init__\n",
      "    super().__init__(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_fast.py\", line 139, in __init__\n",
      "    fast_tokenizer = convert_slow_tokenizer(self, from_tiktoken=True)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py\", line 1739, in convert_slow_tokenizer\n",
      "    raise ValueError(\n",
      "ValueError: Converting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast converters: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']\n",
      "W0902 22:58:45.080000 139871469117440 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 23445 closing signal SIGTERM\n",
      "E0902 22:58:45.112000 139871469117440 torch/distributed/elastic/multiprocessing/api.py:826] failed (exitcode: 1) local_rank: 0 (pid: 23444) of binary: /usr/bin/python3\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/accelerate\", line 7, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/accelerate_cli.py\", line 50, in main\n",
      "    args.func(args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py\", line 1226, in launch_command\n",
      "    multi_gpu_launcher(args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/commands/launch.py\", line 853, in multi_gpu_launcher\n",
      "    distrib_run.run(args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 870, in run\n",
      "    elastic_launch(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 132, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 263, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "train_samo.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2025-09-02_22:58:45\n",
      "  host      : 2fd62b2353f7\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 23444)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "!accelerate launch --num_processes=2 --mixed_precision=fp16 \\\n",
    "train_samo.py \\\n",
    "--output_dir \"$OUT_DIR\" \\\n",
    "--model_name \"$MODEL_NAME\" \\\n",
    "--per_device_train_batch_size 8 --per_device_eval_batch_size 16 \\\n",
    "--gradient_accumulation_steps 4 \\\n",
    "--num_train_epochs 3 \\\n",
    "--learning_rate 1e-5 --lr_scheduler_type cosine --warmup_ratio 0.1 \\\n",
    "--weight_decay 0.01 --fp16 true --tf32 true --gradient_checkpointing true \\\n",
    "--ddp_backend nccl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f5961cf-0c06-4960-aa96-6909e10e2439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Evaluation report not found at ./samo_out/eval_report.json\n",
      "This means training hasn't completed yet or failed.\n",
      "Please run Cell 4 (training) first and wait for it to complete.\n",
      "Output directory contents: ['train.jsonl', 'val.jsonl']\n"
     ]
    }
   ],
   "source": [
    "import json, os\n",
    "\n",
    "eval_report_path = os.path.join(OUT_DIR, \"eval_report.json\")\n",
    "if os.path.exists(eval_report_path):\n",
    "    with open(eval_report_path, \"r\") as f:\n",
    "        rep = json.load(f)\n",
    "    print(\"F1_micro:\", rep[\"f1_micro\"], \" F1_macro:\", rep[\"f1_macro\"])\n",
    "    # Show 5 worst & best classes by F1\n",
    "    pc = rep[\"per_class\"]\n",
    "    sorted_items = sorted(pc.items(), key=lambda kv: kv[1][\"f1\"])\n",
    "    print(\"\\nWorst 5:\")\n",
    "    for k,v in sorted_items[:5]:\n",
    "        print(k, v)\n",
    "    print(\"\\nBest 5:\")\n",
    "    for k,v in sorted_items[-5:]:\n",
    "        print(k, v)\n",
    "else:\n",
    "    print(f\"❌ Evaluation report not found at {eval_report_path}\")\n",
    "    print(\"This means training hasn't completed yet or failed.\")\n",
    "    print(\"Please run Cell 4 (training) first and wait for it to complete.\")\n",
    "    print(f\"Output directory contents: {os.listdir(OUT_DIR) if os.path.exists(OUT_DIR) else 'Directory does not exist'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (System)",
   "language": "python",
   "name": "system-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
