{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18602aa3-918c-4947-98e1-945d33e7ef92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Sep  3 10:32:54 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        On  |   00000000:C1:00.0 Off |                  N/A |\n",
      "| 30%   27C    P8             38W /  350W |       2MiB /  24576MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 3090        On  |   00000000:C2:00.0 Off |                  N/A |\n",
      "| 30%   26C    P8             40W /  350W |       2MiB /  24576MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: Cannot install accelerate==0.24.0, datasets==2.20.0, datasets==2.21.0, datasets==3.0.0, datasets==3.0.1, datasets==3.0.2, datasets==3.1.0, datasets==3.2.0, datasets==3.3.0, datasets==3.3.1, datasets==3.3.2, datasets==3.4.0, datasets==3.4.1, datasets==3.5.0, datasets==3.5.1, datasets==3.6.0, datasets==4.0.0, transformers and transformers==4.35.0 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "!pip -q install --upgrade pip\n",
    "!pip -q install torch==2.3.1+cu118 torchvision==0.18.1+cu118 torchaudio==2.3.1+cu118 --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip -q install \"transformers==4.35.0\" \"accelerate==0.24.0\" \"datasets>=2.20\" \"evaluate\" \"scikit-learn\" \"peft>=0.11\" \"tensorboard\" \"pyarrow<18\" \"tiktoken\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ee68e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Transformers version: 4.56.0\n",
      "🚀 Accelerate version: 1.10.1\n",
      "🎯 Using DeBERTa-v3-large - powerful model with slow tokenizer workaround\n",
      "✅ DeBERTa-v3-large with DebertaV2Tokenizer avoids tiktoken issues\n"
     ]
    }
   ],
   "source": [
    "# Check current versions and model compatibility\n",
    "import transformers\n",
    "import accelerate\n",
    "print(f\"🔧 Transformers version: {transformers.__version__}\")\n",
    "print(f\"🚀 Accelerate version: {accelerate.__version__}\")\n",
    "print(f\"🎯 Using DeBERTa-v3-large - powerful model with slow tokenizer workaround\")\n",
    "print(\"✅ DeBERTa-v3-large with DebertaV2Tokenizer avoids tiktoken issues\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04dbd738-9657-4f25-bff9-2b197a8d201e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🛡️  Script is PROTECTED (read-only) - cannot be modified!\n",
      "✅ Script has complete main function with training logic!\n",
      "📄 Script: train_samo.py\n",
      "📏 Size: 23623 bytes\n",
      "🔐 SHA256: 27f99c3e904d90f0b23ca891b8ad191b522987d8a58055dd88fa550a29a6c390\n",
      "🚀 Script is ready for training and protected from modification!\n"
     ]
    }
   ],
   "source": [
    "# Check training script status (PROTECTED - READ-ONLY)\n",
    "import os\n",
    "import stat\n",
    "import hashlib\n",
    "\n",
    "if os.path.exists(\"train_samo.py\"):\n",
    "    # Check file permissions\n",
    "    file_stat = os.stat(\"train_samo.py\")\n",
    "    is_readonly = not (file_stat.st_mode & stat.S_IWRITE)\n",
    "    \n",
    "    if is_readonly:\n",
    "        print(\"🛡️  Script is PROTECTED (read-only) - cannot be modified!\")\n",
    "    else:\n",
    "        print(\"⚠️  Script is writable - may be overwritten\")\n",
    "    \n",
    "    # Check if main function exists\n",
    "    with open(\"train_samo.py\", \"r\") as f:\n",
    "        content = f.read()\n",
    "        if \"def main():\" in content and \"trainer.train()\" in content:\n",
    "            print(\"✅ Script has complete main function with training logic!\")\n",
    "        elif \"def main():\" in content:\n",
    "            print(\"✅ Script has main function\")\n",
    "        else:\n",
    "            print(\"❌ Script missing main function!\")\n",
    "    \n",
    "    # Show script info\n",
    "    print(\"📄 Script: train_samo.py\")\n",
    "    print(\"📏 Size:\", os.path.getsize(\"train_samo.py\"), \"bytes\")\n",
    "    print(\"🔐 SHA256:\", hashlib.sha256(open(\"train_samo.py\",'rb').read()).hexdigest())\n",
    "    \n",
    "    if is_readonly:\n",
    "        print(\"🚀 Script is ready for training and protected from modification!\")\n",
    "else:\n",
    "    print(\"❌ Script not found!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c162df17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Training script is complete with main function\n",
      "🚀 Ready to start training!\n",
      "📄 Training script: train_samo.py\n",
      "📏 Size: 23623 bytes\n",
      "🔐 SHA256: 27f99c3e904d90f0b23ca891b8ad191b522987d8a58055dd88fa550a29a6c390\n",
      "✅ Script is ready for training!\n"
     ]
    }
   ],
   "source": [
    "# Verify the training script is complete and ready\n",
    "import os\n",
    "import hashlib\n",
    "\n",
    "# Check if main function exists\n",
    "with open(\"train_samo.py\", \"r\") as f:\n",
    "    content = f.read()\n",
    "    if \"def main():\" in content and \"if __name__ == \\\"__main__\\\":\" in content:\n",
    "        print(\"✅ Training script is complete with main function\")\n",
    "        print(\"🚀 Ready to start training!\")\n",
    "    else:\n",
    "        print(\"❌ Training script is missing main function!\")\n",
    "        print(\"The script will not work without the main() function.\")\n",
    "\n",
    "# Show script info\n",
    "if os.path.exists(\"train_samo.py\"):\n",
    "    print(\"📄 Training script: train_samo.py\")\n",
    "    print(\"📏 Size:\", os.path.getsize(\"train_samo.py\"), \"bytes\")\n",
    "    print(\"🔐 SHA256:\", hashlib.sha256(open(\"train_samo.py\",'rb').read()).hexdigest())\n",
    "    print(\"✅ Script is ready for training!\")\n",
    "else:\n",
    "    print(\"❌ Training script not found!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "efe8e70c-f45c-4c2d-982a-e4ee94bbfac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration already exists at /workspace/.hf_home/accelerate/default_config.yaml, will not override. Run `accelerate config` manually or pass a different `save_location`.\n",
      "Accelerate config patched:\n",
      " {\n",
      "  \"compute_environment\": \"LOCAL_MACHINE\",\n",
      "  \"debug\": false,\n",
      "  \"distributed_type\": \"MULTI_GPU\",\n",
      "  \"downcast_bf16\": false,\n",
      "  \"enable_cpu_affinity\": false,\n",
      "  \"machine_rank\": 0,\n",
      "  \"main_training_function\": \"main\",\n",
      "  \"mixed_precision\": \"fp16\",\n",
      "  \"num_machines\": 1,\n",
      "  \"num_processes\": 2,\n",
      "  \"rdzv_backend\": \"static\",\n",
      "  \"same_network\": false,\n",
      "  \"tpu_use_cluster\": false,\n",
      "  \"tpu_use_sudo\": false,\n",
      "  \"use_cpu\": false\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "accelerate config default\n",
    "CONFIG=/workspace/.hf_home/accelerate/default_config.yaml\n",
    "python3 - <<'PY'\n",
    "import json\n",
    "from pathlib import Path\n",
    "p = Path(\"/workspace/.hf_home/accelerate/default_config.yaml\")\n",
    "config = json.loads(p.read_text())\n",
    "config['distributed_type'] = 'MULTI_GPU'\n",
    "config['mixed_precision'] = 'fp16'\n",
    "config['num_processes'] = 2\n",
    "p.write_text(json.dumps(config, indent=2))\n",
    "print(\"Accelerate config patched:\\n\", p.read_text())\n",
    "PY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a61bc9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ HF_TOKEN is set\n"
     ]
    }
   ],
   "source": [
    "# Set up Hugging Face authentication (optional but recommended)\n",
    "# You can get a token from: https://huggingface.co/settings/tokens\n",
    "import os\n",
    "\n",
    "# Option 1: Set environment variable (recommended for security)\n",
    "os.environ[\"HF_TOKEN\"] = \"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\"\n",
    "\n",
    "# Option 2: Use huggingface_hub login (interactive)\n",
    "# from huggingface_hub import login\n",
    "# login()  # This will prompt you to enter your token\n",
    "\n",
    "# Option 3: Check if token is already set\n",
    "if os.getenv(\"HF_TOKEN\"):\n",
    "    print(\"✅ HF_TOKEN is set\")\n",
    "else:\n",
    "    print(\"⚠️  HF_TOKEN not set - you may hit rate limits\")\n",
    "    print(\"To set it: os.environ['HF_TOKEN'] = 'your_token_here'\")\n",
    "    print(\"Or get a token from: https://huggingface.co/settings/tokens\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0860fd59-19f5-4545-abd8-df16e30f5cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "OUT_DIR = \"./samo_out\"\n",
    "MODEL_NAME = \"microsoft/deberta-v3-large\"  # The powerful model we want - let's fix the tiktoken issue!\n",
    "!mkdir -p \"$OUT_DIR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12866914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Testing tokenizer loading with robust workaround...\n",
      "❌ DebertaV2Tokenizer failed: \n",
      "DebertaV2Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\n",
      "installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\n",
      "that match your environment. Please note that you may need to restart your runtime after installation.\n",
      "\n",
      "🔄 Let's try AutoTokenizer fallback...\n",
      "❌ All tokenizer methods failed: Converting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast converters: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']\n",
      "This indicates a deeper compatibility issue that needs investigation.\n"
     ]
    }
   ],
   "source": [
    "# Test tokenizer loading with robust workaround for DeBERTa-v3-large tiktoken issue\n",
    "print(\"🧪 Testing tokenizer loading with robust workaround...\")\n",
    "try:\n",
    "    from transformers import DebertaV2Tokenizer\n",
    "    import os\n",
    "    \n",
    "    # Set environment variable to force slow tokenizer (workaround for tiktoken issue)\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "    \n",
    "    # Use DebertaV2Tokenizer directly (most reliable for DeBERTa-v3)\n",
    "    tokenizer = DebertaV2Tokenizer.from_pretrained(MODEL_NAME)\n",
    "    print(f\"✅ Tokenizer loaded successfully for {MODEL_NAME} using DebertaV2Tokenizer!\")\n",
    "    print(f\"📏 Vocab size: {tokenizer.vocab_size}\")\n",
    "    print(f\"🔧 Tokenizer type: {type(tokenizer).__name__}\")\n",
    "    \n",
    "    # Test tokenization\n",
    "    test_text = \"I love this movie! It's amazing.\"\n",
    "    tokens = tokenizer(test_text, return_tensors=\"pt\")\n",
    "    print(f\"🔤 Test tokenization: '{test_text}' -> {tokens['input_ids'].shape}\")\n",
    "    print(\"🚀 Ready for training with DebertaV2Tokenizer workaround!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ DebertaV2Tokenizer failed: {e}\")\n",
    "    print(\"🔄 Let's try AutoTokenizer fallback...\")\n",
    "    try:\n",
    "        from transformers import AutoTokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\n",
    "        print(f\"✅ Fallback successful with AutoTokenizer (use_fast=False)\")\n",
    "        print(f\"📏 Vocab size: {tokenizer.vocab_size}\")\n",
    "    except Exception as e2:\n",
    "        print(f\"❌ All tokenizer methods failed: {e2}\")\n",
    "        print(\"This indicates a deeper compatibility issue that needs investigation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90792e0a-3c67-4c8d-ade7-8e34fc9700df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ PEFT available for LoRA fine-tuning\n",
      "🚀 SAMO - GoEmotions Multi-Label Trainer\n",
      "📁 Output directory: ./samo_out\n",
      "🤖 Model: microsoft/deberta-v3-large\n",
      "📊 Loading GoEmotions dataset using alternative method...\n",
      "🔑 Authenticating with Hugging Face...\n",
      "✅ PEFT available for LoRA fine-tuning\n",
      "🚀 SAMO - GoEmotions Multi-Label Trainer\n",
      "📁 Output directory: ./samo_out\n",
      "🤖 Model: microsoft/deberta-v3-large\n",
      "📊 Loading GoEmotions dataset using alternative method...\n",
      "🔑 Authenticating with Hugging Face...\n",
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
      "✅ Successfully authenticated with Hugging Face\n",
      "🤖 Using DeBERTa-v3-large: microsoft/deberta-v3-large\n",
      "📊 Loading real GoEmotions dataset directly from Hugging Face hub...\n",
      "📥 Downloading GoEmotions dataset files...\n",
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
      "✅ Successfully authenticated with Hugging Face\n",
      "🤖 Using DeBERTa-v3-large: microsoft/deberta-v3-large\n",
      "📊 Loading real GoEmotions dataset directly from Hugging Face hub...\n",
      "📥 Downloading GoEmotions dataset files...\n",
      "✅ Loaded real GoEmotions dataset with 48837 examples\n",
      "🔄 Preparing data from real GoEmotions dataset...\n",
      "✅ Loaded real GoEmotions dataset with 48837 examples\n",
      "🔄 Preparing data from real GoEmotions dataset...\n",
      "📈 Prepared 39070 training examples, 9767 validation examples\n",
      "📈 Prepared 39070 training examples, 9767 validation examples\n",
      "💾 Saved data to ./samo_out/train.jsonl and ./samo_out/val.jsonl\n",
      "🤖 Loading model and tokenizer from local cache...\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DebertaV2TokenizerFast'. \n",
      "The class this function is called from is 'DebertaTokenizer'.\n",
      "❌ DebertaTokenizer failed: expected str, bytes or os.PathLike object, not NoneType\n",
      "🔄 Falling back to microsoft/deberta-large (compatible model)...\n",
      "💾 Saved data to ./samo_out/train.jsonl and ./samo_out/val.jsonl\n",
      "🤖 Loading model and tokenizer from local cache...\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DebertaV2TokenizerFast'. \n",
      "The class this function is called from is 'DebertaTokenizer'.\n",
      "❌ DebertaTokenizer failed: expected str, bytes or os.PathLike object, not NoneType\n",
      "🔄 Falling back to microsoft/deberta-large (compatible model)...\n",
      "✅ Tokenizer loaded using microsoft/deberta-large fallback\n",
      "✅ Tokenizer loaded using microsoft/deberta-large fallback\n",
      "❌ DeBERTa-v3-large model failed: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\n",
      "See the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\n",
      "🔄 Loading microsoft/deberta-large model...\n",
      "❌ DeBERTa-v3-large model failed: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\n",
      "See the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\n",
      "🔄 Loading microsoft/deberta-large model...\n",
      "❌ DeBERTa-large model failed: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\n",
      "See the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\n",
      "🔄 Loading RoBERTa-large model...\n",
      "❌ DeBERTa-large model failed: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\n",
      "See the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\n",
      "🔄 Loading RoBERTa-large model...\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "✅ Model loaded from RoBERTa-large\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "✅ Model loaded from RoBERTa-large\n",
      "🎯 Applying LoRA fine-tuning...\n",
      "🎯 Applying LoRA fine-tuning...\n",
      "trainable params: 10,515,484 || all params: 365,903,928 || trainable%: 2.8738\n",
      "✅ LoRA applied successfully!\n",
      "🎯 Initializing classifier bias with balanced strategy...\n",
      "trainable params: 10,515,484 || all params: 365,903,928 || trainable%: 2.8738\n",
      "✅ LoRA applied successfully!\n",
      "🎯 Initializing classifier bias with balanced strategy...\n",
      "✅ Bias initialized for base_model.model.classifier.original_module.out_proj\n",
      "   Range: [-1.47, -0.51]\n",
      "   Mean prevalence: 0.042\n",
      "✅ Bias initialized for base_model.model.classifier.original_module.out_proj\n",
      "   Range: [-1.47, -0.51]\n",
      "   Mean prevalence: 0.042\n",
      "/home/user/train_samo.py:193: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `ASLTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/home/user/train_samo.py:193: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `ASLTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n",
      "🏋️ Starting training...\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 1, 'pad_token_id': 0}.\n",
      "🏋️ Starting training...\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 1, 'pad_token_id': 0}.\n",
      "  0%|                                                  | 0/1833 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "{'loss': 0.1258, 'grad_norm': 0.6608861684799194, 'learning_rate': 5.380434782608695e-06, 'epoch': 0.16}\n",
      "{'loss': 0.0874, 'grad_norm': 0.33366525173187256, 'learning_rate': 9.997958490501997e-06, 'epoch': 0.33}\n",
      "{'loss': 0.0611, 'grad_norm': 0.16567645967006683, 'learning_rate': 9.880475702837468e-06, 'epoch': 0.49}\n",
      "{'loss': 0.0537, 'grad_norm': 0.12277791649103165, 'learning_rate': 9.586386450641347e-06, 'epoch': 0.66}\n",
      "{'loss': 0.0508, 'grad_norm': 0.08572876453399658, 'learning_rate': 9.126332742244279e-06, 'epoch': 0.82}\n",
      "{'loss': 0.0496, 'grad_norm': 0.10114697366952896, 'learning_rate': 8.516962229203554e-06, 'epoch': 0.98}\n",
      " 33%|█████████████▎                          | 611/1833 [02:21<04:08,  4.91it/s]\n",
      "  0%|                                                   | 0/306 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|▌                                          | 4/306 [00:00<00:10, 27.63it/s]\u001b[A\n",
      "  2%|▉                                          | 7/306 [00:00<00:12, 23.45it/s]\u001b[A\n",
      "  3%|█▎                                        | 10/306 [00:00<00:13, 22.18it/s]\u001b[A\n",
      "  4%|█▊                                        | 13/306 [00:00<00:13, 21.54it/s]\u001b[A\n",
      "  5%|██▏                                       | 16/306 [00:00<00:13, 21.24it/s]\u001b[A\n",
      "  6%|██▌                                       | 19/306 [00:00<00:13, 21.06it/s]\u001b[A\n",
      "  7%|███                                       | 22/306 [00:01<00:13, 20.93it/s]\u001b[A\n",
      "  8%|███▍                                      | 25/306 [00:01<00:13, 20.73it/s]\u001b[A\n",
      "  9%|███▊                                      | 28/306 [00:01<00:14, 18.79it/s]\u001b[A\n",
      " 10%|████▎                                     | 31/306 [00:01<00:14, 19.29it/s]\u001b[A\n",
      " 11%|████▋                                     | 34/306 [00:01<00:13, 19.69it/s]\u001b[A\n",
      " 12%|█████                                     | 37/306 [00:01<00:13, 19.97it/s]\u001b[A\n",
      " 13%|█████▍                                    | 40/306 [00:01<00:13, 20.11it/s]\u001b[A\n",
      " 14%|█████▉                                    | 43/306 [00:02<00:13, 20.12it/s]\u001b[A\n",
      " 15%|██████▎                                   | 46/306 [00:02<00:12, 20.23it/s]\u001b[A\n",
      " 16%|██████▋                                   | 49/306 [00:02<00:12, 20.26it/s]\u001b[A\n",
      " 17%|███████▏                                  | 52/306 [00:02<00:12, 20.29it/s]\u001b[A\n",
      " 18%|███████▌                                  | 55/306 [00:02<00:12, 20.39it/s]\u001b[A\n",
      " 19%|███████▉                                  | 58/306 [00:02<00:12, 20.50it/s]\u001b[A\n",
      " 20%|████████▎                                 | 61/306 [00:02<00:11, 20.61it/s]\u001b[A\n",
      " 21%|████████▊                                 | 64/306 [00:03<00:11, 20.61it/s]\u001b[A\n",
      " 22%|█████████▏                                | 67/306 [00:03<00:11, 20.62it/s]\u001b[A\n",
      " 23%|█████████▌                                | 70/306 [00:03<00:11, 20.61it/s]\u001b[A\n",
      " 24%|██████████                                | 73/306 [00:03<00:11, 20.68it/s]\u001b[A\n",
      " 25%|██████████▍                               | 76/306 [00:03<00:11, 20.70it/s]\u001b[A\n",
      " 26%|██████████▊                               | 79/306 [00:03<00:10, 20.74it/s]\u001b[A\n",
      " 27%|███████████▎                              | 82/306 [00:03<00:10, 20.76it/s]\u001b[A\n",
      " 28%|███████████▋                              | 85/306 [00:04<00:10, 20.78it/s]\u001b[A\n",
      " 29%|████████████                              | 88/306 [00:04<00:10, 20.72it/s]\u001b[A\n",
      " 30%|████████████▍                             | 91/306 [00:04<00:10, 20.67it/s]\u001b[A\n",
      " 31%|████████████▉                             | 94/306 [00:04<00:10, 20.67it/s]\u001b[A\n",
      " 32%|█████████████▎                            | 97/306 [00:04<00:10, 20.68it/s]\u001b[A\n",
      " 33%|█████████████▍                           | 100/306 [00:04<00:09, 20.67it/s]\u001b[A\n",
      " 34%|█████████████▊                           | 103/306 [00:04<00:09, 20.66it/s]\u001b[A\n",
      " 35%|██████████████▏                          | 106/306 [00:05<00:09, 20.69it/s]\u001b[A\n",
      " 36%|██████████████▌                          | 109/306 [00:05<00:09, 20.70it/s]\u001b[A\n",
      " 37%|███████████████                          | 112/306 [00:05<00:09, 20.71it/s]\u001b[A\n",
      " 38%|███████████████▍                         | 115/306 [00:05<00:09, 20.66it/s]\u001b[A\n",
      " 39%|███████████████▊                         | 118/306 [00:05<00:09, 20.64it/s]\u001b[A\n",
      " 40%|████████████████▏                        | 121/306 [00:05<00:08, 20.65it/s]\u001b[A\n",
      " 41%|████████████████▌                        | 124/306 [00:06<00:08, 20.65it/s]\u001b[A\n",
      " 42%|█████████████████                        | 127/306 [00:06<00:08, 20.64it/s]\u001b[A\n",
      " 42%|█████████████████▍                       | 130/306 [00:06<00:08, 20.65it/s]\u001b[A\n",
      " 43%|█████████████████▊                       | 133/306 [00:06<00:08, 20.66it/s]\u001b[A\n",
      " 44%|██████████████████▏                      | 136/306 [00:06<00:08, 20.65it/s]\u001b[A\n",
      " 45%|██████████████████▌                      | 139/306 [00:06<00:08, 20.67it/s]\u001b[A\n",
      " 46%|███████████████████                      | 142/306 [00:06<00:07, 20.70it/s]\u001b[A\n",
      " 47%|███████████████████▍                     | 145/306 [00:07<00:07, 20.70it/s]\u001b[A\n",
      " 48%|███████████████████▊                     | 148/306 [00:07<00:07, 20.73it/s]\u001b[A\n",
      " 49%|████████████████████▏                    | 151/306 [00:07<00:07, 20.73it/s]\u001b[A\n",
      " 50%|████████████████████▋                    | 154/306 [00:07<00:07, 20.75it/s]\u001b[A\n",
      " 51%|█████████████████████                    | 157/306 [00:07<00:07, 20.76it/s]\u001b[A\n",
      " 52%|█████████████████████▍                   | 160/306 [00:07<00:07, 20.77it/s]\u001b[A\n",
      " 53%|█████████████████████▊                   | 163/306 [00:07<00:06, 20.76it/s]\u001b[A\n",
      " 54%|██████████████████████▏                  | 166/306 [00:08<00:06, 20.77it/s]\u001b[A\n",
      " 55%|██████████████████████▋                  | 169/306 [00:08<00:06, 20.72it/s]\u001b[A\n",
      " 56%|███████████████████████                  | 172/306 [00:08<00:06, 20.70it/s]\u001b[A\n",
      " 57%|███████████████████████▍                 | 175/306 [00:08<00:06, 20.66it/s]\u001b[A\n",
      " 58%|███████████████████████▊                 | 178/306 [00:08<00:06, 20.62it/s]\u001b[A\n",
      " 59%|████████████████████████▎                | 181/306 [00:08<00:06, 20.59it/s]\u001b[A\n",
      " 60%|████████████████████████▋                | 184/306 [00:08<00:05, 20.65it/s]\u001b[A\n",
      " 61%|█████████████████████████                | 187/306 [00:09<00:05, 20.65it/s]\u001b[A\n",
      " 62%|█████████████████████████▍               | 190/306 [00:09<00:05, 20.66it/s]\u001b[A\n",
      " 63%|█████████████████████████▊               | 193/306 [00:09<00:05, 20.32it/s]\u001b[A\n",
      " 64%|██████████████████████████▎              | 196/306 [00:09<00:05, 20.36it/s]\u001b[A\n",
      " 65%|██████████████████████████▋              | 199/306 [00:09<00:05, 20.42it/s]\u001b[A\n",
      " 66%|███████████████████████████              | 202/306 [00:09<00:05, 20.44it/s]\u001b[A\n",
      " 67%|███████████████████████████▍             | 205/306 [00:09<00:04, 20.46it/s]\u001b[A\n",
      " 68%|███████████████████████████▊             | 208/306 [00:10<00:04, 20.49it/s]\u001b[A\n",
      " 69%|████████████████████████████▎            | 211/306 [00:10<00:04, 20.49it/s]\u001b[A\n",
      " 70%|████████████████████████████▋            | 214/306 [00:10<00:04, 20.35it/s]\u001b[A\n",
      " 71%|█████████████████████████████            | 217/306 [00:10<00:04, 20.38it/s]\u001b[A\n",
      " 72%|█████████████████████████████▍           | 220/306 [00:10<00:04, 20.42it/s]\u001b[A\n",
      " 73%|█████████████████████████████▉           | 223/306 [00:10<00:04, 20.34it/s]\u001b[A\n",
      " 74%|██████████████████████████████▎          | 226/306 [00:10<00:03, 20.36it/s]\u001b[A\n",
      " 75%|██████████████████████████████▋          | 229/306 [00:11<00:03, 20.42it/s]\u001b[A\n",
      " 76%|███████████████████████████████          | 232/306 [00:11<00:03, 20.48it/s]\u001b[A\n",
      " 77%|███████████████████████████████▍         | 235/306 [00:11<00:03, 20.50it/s]\u001b[A\n",
      " 78%|███████████████████████████████▉         | 238/306 [00:11<00:03, 20.57it/s]\u001b[A\n",
      " 79%|████████████████████████████████▎        | 241/306 [00:11<00:03, 20.62it/s]\u001b[A\n",
      " 80%|████████████████████████████████▋        | 244/306 [00:11<00:03, 20.63it/s]\u001b[A\n",
      " 81%|█████████████████████████████████        | 247/306 [00:11<00:02, 20.64it/s]\u001b[A\n",
      " 82%|█████████████████████████████████▍       | 250/306 [00:12<00:02, 20.68it/s]\u001b[A\n",
      " 83%|█████████████████████████████████▉       | 253/306 [00:12<00:02, 20.70it/s]\u001b[A\n",
      " 84%|██████████████████████████████████▎      | 256/306 [00:12<00:02, 20.70it/s]\u001b[A\n",
      " 85%|██████████████████████████████████▋      | 259/306 [00:12<00:02, 20.70it/s]\u001b[A\n",
      " 86%|███████████████████████████████████      | 262/306 [00:12<00:02, 20.70it/s]\u001b[A\n",
      " 87%|███████████████████████████████████▌     | 265/306 [00:12<00:01, 20.64it/s]\u001b[A\n",
      " 88%|███████████████████████████████████▉     | 268/306 [00:13<00:01, 20.34it/s]\u001b[A\n",
      " 89%|████████████████████████████████████▎    | 271/306 [00:13<00:01, 20.32it/s]\u001b[A\n",
      " 90%|████████████████████████████████████▋    | 274/306 [00:13<00:01, 20.34it/s]\u001b[A\n",
      " 91%|█████████████████████████████████████    | 277/306 [00:13<00:01, 20.41it/s]\u001b[A\n",
      " 92%|█████████████████████████████████████▌   | 280/306 [00:13<00:01, 20.48it/s]\u001b[A\n",
      " 92%|█████████████████████████████████████▉   | 283/306 [00:13<00:01, 20.55it/s]\u001b[A\n",
      " 93%|██████████████████████████████████████▎  | 286/306 [00:13<00:00, 20.62it/s]\u001b[A\n",
      " 94%|██████████████████████████████████████▋  | 289/306 [00:14<00:00, 20.64it/s]\u001b[A\n",
      " 95%|███████████████████████████████████████  | 292/306 [00:14<00:00, 20.68it/s]\u001b[A\n",
      " 96%|███████████████████████████████████████▌ | 295/306 [00:14<00:00, 20.68it/s]\u001b[A\n",
      " 97%|███████████████████████████████████████▉ | 298/306 [00:14<00:00, 20.71it/s]\u001b[A\n",
      " 98%|████████████████████████████████████████▎| 301/306 [00:14<00:00, 20.69it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.04486789554357529, 'eval_f1_micro_t3': 0.20457604306864063, 'eval_f1_macro_t3': 0.05536034889584448, 'eval_avg_preds_t3': 4.91143646974506, 'eval_f1_micro_t5': 0.1409293165074039, 'eval_f1_macro_t5': 0.012616566099835436, 'eval_avg_preds_t5': 0.2292413228217467, 'eval_f1_micro_t7': 0.0, 'eval_f1_macro_t7': 0.0, 'eval_avg_preds_t7': 0.0, 'eval_f1_macro': 0.012616566099835436, 'eval_f1_micro': 0.1409293165074039, 'eval_runtime': 14.9849, 'eval_samples_per_second': 651.791, 'eval_steps_per_second': 20.421, 'epoch': 1.0}\n",
      " 33%|█████████████▎                          | 611/1833 [02:36<04:08,  4.91it/s]\n",
      "100%|█████████████████████████████████████████| 306/306 [00:14<00:00, 20.70it/s]\u001b[A\n",
      "                                                                                \u001b[A/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "{'loss': 0.0489, 'grad_norm': 0.08027118444442749, 'learning_rate': 7.780325789025916e-06, 'epoch': 1.15}\n",
      "{'loss': 0.0486, 'grad_norm': 0.09258081018924713, 'learning_rate': 6.9430795850171825e-06, 'epoch': 1.31}\n",
      "{'loss': 0.0482, 'grad_norm': 0.10405334085226059, 'learning_rate': 6.035520477777751e-06, 'epoch': 1.47}\n",
      "{'loss': 0.0481, 'grad_norm': 0.08742526918649673, 'learning_rate': 5.0904896932540595e-06, 'epoch': 1.64}\n",
      "{'loss': 0.0484, 'grad_norm': 0.07675345987081528, 'learning_rate': 4.1421844195667695e-06, 'epoch': 1.8}\n",
      "{'loss': 0.0472, 'grad_norm': 0.09861519187688828, 'learning_rate': 3.224920336555287e-06, 'epoch': 1.96}\n",
      " 67%|██████████████████████████             | 1222/1833 [05:03<02:00,  5.06it/s]\n",
      "  0%|                                                   | 0/306 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|▌                                          | 4/306 [00:00<00:10, 27.55it/s]\u001b[A\n",
      "  2%|▉                                          | 7/306 [00:00<00:12, 23.40it/s]\u001b[A\n",
      "  3%|█▎                                        | 10/306 [00:00<00:13, 22.16it/s]\u001b[A\n",
      "  4%|█▊                                        | 13/306 [00:00<00:13, 21.57it/s]\u001b[A\n",
      "  5%|██▏                                       | 16/306 [00:00<00:13, 21.01it/s]\u001b[A\n",
      "  6%|██▌                                       | 19/306 [00:00<00:13, 20.83it/s]\u001b[A\n",
      "  7%|███                                       | 22/306 [00:01<00:13, 20.75it/s]\u001b[A\n",
      "  8%|███▍                                      | 25/306 [00:01<00:13, 20.63it/s]\u001b[A\n",
      "  9%|███▊                                      | 28/306 [00:01<00:13, 20.35it/s]\u001b[A\n",
      " 10%|████▎                                     | 31/306 [00:01<00:13, 20.49it/s]\u001b[A\n",
      " 11%|████▋                                     | 34/306 [00:01<00:13, 20.57it/s]\u001b[A\n",
      " 12%|█████                                     | 37/306 [00:01<00:13, 20.59it/s]\u001b[A\n",
      " 13%|█████▍                                    | 40/306 [00:01<00:12, 20.63it/s]\u001b[A\n",
      " 14%|█████▉                                    | 43/306 [00:02<00:12, 20.69it/s]\u001b[A\n",
      " 15%|██████▎                                   | 46/306 [00:02<00:12, 20.73it/s]\u001b[A\n",
      " 16%|██████▋                                   | 49/306 [00:02<00:12, 20.76it/s]\u001b[A\n",
      " 17%|███████▏                                  | 52/306 [00:02<00:12, 20.74it/s]\u001b[A\n",
      " 18%|███████▌                                  | 55/306 [00:02<00:12, 20.74it/s]\u001b[A\n",
      " 19%|███████▉                                  | 58/306 [00:02<00:11, 20.74it/s]\u001b[A\n",
      " 20%|████████▎                                 | 61/306 [00:02<00:11, 20.75it/s]\u001b[A\n",
      " 21%|████████▊                                 | 64/306 [00:03<00:11, 20.74it/s]\u001b[A\n",
      " 22%|█████████▏                                | 67/306 [00:03<00:11, 20.76it/s]\u001b[A\n",
      " 23%|█████████▌                                | 70/306 [00:03<00:11, 20.78it/s]\u001b[A\n",
      " 24%|██████████                                | 73/306 [00:03<00:11, 20.77it/s]\u001b[A\n",
      " 25%|██████████▍                               | 76/306 [00:03<00:11, 20.77it/s]\u001b[A\n",
      " 26%|██████████▊                               | 79/306 [00:03<00:10, 20.70it/s]\u001b[A\n",
      " 27%|███████████▎                              | 82/306 [00:03<00:10, 20.72it/s]\u001b[A\n",
      " 28%|███████████▋                              | 85/306 [00:04<00:10, 20.75it/s]\u001b[A\n",
      " 29%|████████████                              | 88/306 [00:04<00:10, 20.77it/s]\u001b[A\n",
      " 30%|████████████▍                             | 91/306 [00:04<00:10, 20.76it/s]\u001b[A\n",
      " 31%|████████████▉                             | 94/306 [00:04<00:10, 20.71it/s]\u001b[A\n",
      " 32%|█████████████▎                            | 97/306 [00:04<00:10, 20.71it/s]\u001b[A\n",
      " 33%|█████████████▍                           | 100/306 [00:04<00:09, 20.74it/s]\u001b[A\n",
      " 34%|█████████████▊                           | 103/306 [00:04<00:09, 20.76it/s]\u001b[A\n",
      " 35%|██████████████▏                          | 106/306 [00:05<00:09, 20.77it/s]\u001b[A\n",
      " 36%|██████████████▌                          | 109/306 [00:05<00:09, 20.75it/s]\u001b[A\n",
      " 37%|███████████████                          | 112/306 [00:05<00:09, 20.75it/s]\u001b[A\n",
      " 38%|███████████████▍                         | 115/306 [00:05<00:09, 20.74it/s]\u001b[A\n",
      " 39%|███████████████▊                         | 118/306 [00:05<00:09, 20.75it/s]\u001b[A\n",
      " 40%|████████████████▏                        | 121/306 [00:05<00:08, 20.71it/s]\u001b[A\n",
      " 41%|████████████████▌                        | 124/306 [00:05<00:08, 20.61it/s]\u001b[A\n",
      " 42%|█████████████████                        | 127/306 [00:06<00:08, 20.64it/s]\u001b[A\n",
      " 42%|█████████████████▍                       | 130/306 [00:06<00:08, 20.55it/s]\u001b[A\n",
      " 43%|█████████████████▊                       | 133/306 [00:06<00:08, 20.59it/s]\u001b[A\n",
      " 44%|██████████████████▏                      | 136/306 [00:06<00:08, 20.61it/s]\u001b[A\n",
      " 45%|██████████████████▌                      | 139/306 [00:06<00:08, 20.65it/s]\u001b[A\n",
      " 46%|███████████████████                      | 142/306 [00:06<00:07, 20.62it/s]\u001b[A\n",
      " 47%|███████████████████▍                     | 145/306 [00:06<00:07, 20.25it/s]\u001b[A\n",
      " 48%|███████████████████▊                     | 148/306 [00:07<00:07, 20.18it/s]\u001b[A\n",
      " 49%|████████████████████▏                    | 151/306 [00:07<00:07, 20.14it/s]\u001b[A\n",
      " 50%|████████████████████▋                    | 154/306 [00:07<00:07, 20.18it/s]\u001b[A\n",
      " 51%|█████████████████████                    | 157/306 [00:07<00:07, 20.22it/s]\u001b[A\n",
      " 52%|█████████████████████▍                   | 160/306 [00:07<00:07, 20.23it/s]\u001b[A\n",
      " 53%|█████████████████████▊                   | 163/306 [00:07<00:07, 20.24it/s]\u001b[A\n",
      " 54%|██████████████████████▏                  | 166/306 [00:08<00:06, 20.22it/s]\u001b[A\n",
      " 55%|██████████████████████▋                  | 169/306 [00:08<00:06, 20.26it/s]\u001b[A\n",
      " 56%|███████████████████████                  | 172/306 [00:08<00:06, 20.26it/s]\u001b[A\n",
      " 57%|███████████████████████▍                 | 175/306 [00:08<00:06, 20.28it/s]\u001b[A\n",
      " 58%|███████████████████████▊                 | 178/306 [00:08<00:06, 20.12it/s]\u001b[A\n",
      " 59%|████████████████████████▎                | 181/306 [00:08<00:06, 20.08it/s]\u001b[A\n",
      " 60%|████████████████████████▋                | 184/306 [00:08<00:06, 20.06it/s]\u001b[A\n",
      " 61%|█████████████████████████                | 187/306 [00:09<00:05, 20.09it/s]\u001b[A\n",
      " 62%|█████████████████████████▍               | 190/306 [00:09<00:05, 20.05it/s]\u001b[A\n",
      " 63%|█████████████████████████▊               | 193/306 [00:09<00:05, 20.07it/s]\u001b[A\n",
      " 64%|██████████████████████████▎              | 196/306 [00:09<00:05, 20.15it/s]\u001b[A\n",
      " 65%|██████████████████████████▋              | 199/306 [00:09<00:05, 20.23it/s]\u001b[A\n",
      " 66%|███████████████████████████              | 202/306 [00:09<00:05, 20.29it/s]\u001b[A\n",
      " 67%|███████████████████████████▍             | 205/306 [00:09<00:04, 20.33it/s]\u001b[A\n",
      " 68%|███████████████████████████▊             | 208/306 [00:10<00:04, 20.22it/s]\u001b[A\n",
      " 69%|████████████████████████████▎            | 211/306 [00:10<00:04, 20.29it/s]\u001b[A\n",
      " 70%|████████████████████████████▋            | 214/306 [00:10<00:04, 20.36it/s]\u001b[A\n",
      " 71%|█████████████████████████████            | 217/306 [00:10<00:04, 20.42it/s]\u001b[A\n",
      " 72%|█████████████████████████████▍           | 220/306 [00:10<00:04, 20.43it/s]\u001b[A\n",
      " 73%|█████████████████████████████▉           | 223/306 [00:10<00:04, 20.47it/s]\u001b[A\n",
      " 74%|██████████████████████████████▎          | 226/306 [00:10<00:03, 20.47it/s]\u001b[A\n",
      " 75%|██████████████████████████████▋          | 229/306 [00:11<00:03, 20.26it/s]\u001b[A\n",
      " 76%|███████████████████████████████          | 232/306 [00:11<00:03, 20.29it/s]\u001b[A\n",
      " 77%|███████████████████████████████▍         | 235/306 [00:11<00:03, 20.27it/s]\u001b[A\n",
      " 78%|███████████████████████████████▉         | 238/306 [00:11<00:03, 20.33it/s]\u001b[A\n",
      " 79%|████████████████████████████████▎        | 241/306 [00:11<00:03, 20.33it/s]\u001b[A\n",
      " 80%|████████████████████████████████▋        | 244/306 [00:11<00:03, 20.38it/s]\u001b[A\n",
      " 81%|█████████████████████████████████        | 247/306 [00:12<00:02, 20.40it/s]\u001b[A\n",
      " 82%|█████████████████████████████████▍       | 250/306 [00:12<00:02, 20.28it/s]\u001b[A\n",
      " 83%|█████████████████████████████████▉       | 253/306 [00:12<00:02, 20.37it/s]\u001b[A\n",
      " 84%|██████████████████████████████████▎      | 256/306 [00:12<00:02, 20.43it/s]\u001b[A\n",
      " 85%|██████████████████████████████████▋      | 259/306 [00:12<00:02, 20.27it/s]\u001b[A\n",
      " 86%|███████████████████████████████████      | 262/306 [00:12<00:02, 20.37it/s]\u001b[A\n",
      " 87%|███████████████████████████████████▌     | 265/306 [00:12<00:02, 20.43it/s]\u001b[A\n",
      " 88%|███████████████████████████████████▉     | 268/306 [00:13<00:01, 20.48it/s]\u001b[A\n",
      " 89%|████████████████████████████████████▎    | 271/306 [00:13<00:01, 20.53it/s]\u001b[A\n",
      " 90%|████████████████████████████████████▋    | 274/306 [00:13<00:01, 20.57it/s]\u001b[A\n",
      " 91%|█████████████████████████████████████    | 277/306 [00:13<00:01, 20.57it/s]\u001b[A\n",
      " 92%|█████████████████████████████████████▌   | 280/306 [00:13<00:01, 20.58it/s]\u001b[A\n",
      " 92%|█████████████████████████████████████▉   | 283/306 [00:13<00:01, 20.56it/s]\u001b[A\n",
      " 93%|██████████████████████████████████████▎  | 286/306 [00:13<00:00, 20.47it/s]\u001b[A\n",
      " 94%|██████████████████████████████████████▋  | 289/306 [00:14<00:00, 20.36it/s]\u001b[A\n",
      " 95%|███████████████████████████████████████  | 292/306 [00:14<00:00, 20.34it/s]\u001b[A\n",
      " 96%|███████████████████████████████████████▌ | 295/306 [00:14<00:00, 20.27it/s]\u001b[A\n",
      " 97%|███████████████████████████████████████▉ | 298/306 [00:14<00:00, 20.24it/s]\u001b[A\n",
      " 98%|████████████████████████████████████████▎| 301/306 [00:14<00:00, 20.23it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.043963003903627396, 'eval_f1_micro_t3': 0.21912124543423525, 'eval_f1_macro_t3': 0.05962894333468729, 'eval_avg_preds_t3': 4.599979522883178, 'eval_f1_micro_t5': 0.16823495865412033, 'eval_f1_macro_t5': 0.014562148287096456, 'eval_avg_preds_t5': 0.26190232415275927, 'eval_f1_micro_t7': 0.0, 'eval_f1_macro_t7': 0.0, 'eval_avg_preds_t7': 0.0, 'eval_f1_macro': 0.014562148287096456, 'eval_f1_micro': 0.16823495865412033, 'eval_runtime': 15.0397, 'eval_samples_per_second': 649.414, 'eval_steps_per_second': 20.346, 'epoch': 2.0}\n",
      " 67%|██████████████████████████             | 1222/1833 [05:18<02:00,  5.06it/s]\n",
      "100%|█████████████████████████████████████████| 306/306 [00:14<00:00, 20.22it/s]\u001b[A\n",
      "                                                                                \u001b[A/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "{'loss': 0.0472, 'grad_norm': 0.09567872434854507, 'learning_rate': 2.371889857542826e-06, 'epoch': 2.13}\n",
      "{'loss': 0.0477, 'grad_norm': 0.0850144624710083, 'learning_rate': 1.6139610179851256e-06, 'epoch': 2.29}\n",
      "{'loss': 0.0469, 'grad_norm': 0.08987871557474136, 'learning_rate': 9.78560474804789e-07, 'epoch': 2.46}\n",
      "{'loss': 0.0473, 'grad_norm': 0.09659868478775024, 'learning_rate': 4.886810365567874e-07, 'epoch': 2.62}\n",
      "{'loss': 0.0477, 'grad_norm': 0.09070688486099243, 'learning_rate': 1.6204963825800945e-07, 'epoch': 2.78}\n",
      " 94%|████████████████████████████████████▌  | 1718/1833 [07:15<00:25,  4.47it/s]"
     ]
    }
   ],
   "source": [
    "!accelerate launch --num_processes=2 --mixed_precision=fp16 \\\n",
    "train_samo.py \\\n",
    "--output_dir \"$OUT_DIR\" \\\n",
    "--model_name \"$MODEL_NAME\" \\\n",
    "--per_device_train_batch_size 8 --per_device_eval_batch_size 16 \\\n",
    "--gradient_accumulation_steps 4 \\\n",
    "--num_train_epochs 3 \\\n",
    "--learning_rate 1e-5 --lr_scheduler_type cosine --warmup_ratio 0.1 \\\n",
    "--weight_decay 0.01 --fp16 true --tf32 true --gradient_checkpointing true \\\n",
    "--ddp_backend nccl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5961cf-0c06-4960-aa96-6909e10e2439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "\n",
    "eval_report_path = os.path.join(OUT_DIR, \"eval_report.json\")\n",
    "if os.path.exists(eval_report_path):\n",
    "    with open(eval_report_path, \"r\") as f:\n",
    "        rep = json.load(f)\n",
    "    print(\"F1_micro:\", rep[\"f1_micro\"], \" F1_macro:\", rep[\"f1_macro\"])\n",
    "    # Show 5 worst & best classes by F1\n",
    "    pc = rep[\"per_class\"]\n",
    "    sorted_items = sorted(pc.items(), key=lambda kv: kv[1][\"f1\"])\n",
    "    print(\"\\nWorst 5:\")\n",
    "    for k,v in sorted_items[:5]:\n",
    "        print(k, v)\n",
    "    print(\"\\nBest 5:\")\n",
    "    for k,v in sorted_items[-5:]:\n",
    "        print(k, v)\n",
    "else:\n",
    "    print(f\"❌ Evaluation report not found at {eval_report_path}\")\n",
    "    print(\"This means training hasn't completed yet or failed.\")\n",
    "    print(\"Please run Cell 4 (training) first and wait for it to complete.\")\n",
    "    print(f\"Output directory contents: {os.listdir(OUT_DIR) if os.path.exists(OUT_DIR) else 'Directory does not exist'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (System)",
   "language": "python",
   "name": "system-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
