{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ **SAMo Multi-Dataset DeBERTa (CLEAN VERSION)**\n",
    "## **Efficient Multi-Dataset Training with Proven BCE Configuration**\n",
    "\n",
    "### **ğŸ¯ MISSION**\n",
    "- **One-Command Multi-Dataset Training**: GoEmotions + SemEval + ISEAR + MELD\n",
    "- **Proven BCE Configuration**: Use your 51.79% F1-macro winning setup\n",
    "- **No Threshold Testing**: Save time with threshold=0.2\n",
    "- **Achieve >60% F1-macro**: Through comprehensive dataset integration\n",
    "\n",
    "### **ğŸ“‹ SIMPLE WORKFLOW**\n",
    "1. **Run Cell 2**: Data preparation (10-15 minutes)\n",
    "2. **Run Cell 4**: Training (3-4 hours)\n",
    "3. **Monitor**: `tail -f logs/train_comprehensive_multidataset.log`\n",
    "\n",
    "### **ğŸ“Š EXPECTED RESULTS**\n",
    "- **Baseline**: 51.79% F1-macro (GoEmotions BCE Extended)\n",
    "- **Target**: 60-65% F1-macro (All datasets combined)\n",
    "- **Dataset**: 38,111 samples (GoEmotions + SemEval + ISEAR + MELD)\n",
    "\n",
    "**Start with Cell 2 below!** ğŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¥ **STEP 1: Data Preparation**\n",
    "### **ğŸ¯ ONE COMMAND - PREPARE ALL DATASETS**\n",
    "Combines GoEmotions + SemEval + ISEAR + MELD into unified training format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ MULTI-DATASET PREPARATION\n",
      "==================================================\n",
      "ğŸ“Š Datasets: GoEmotions + SemEval + ISEAR + MELD\n",
      "â±ï¸ Time: ~10-15 minutes\n",
      "==================================================\n",
      "ğŸ“ Working directory: /home/user/goemotions-deberta\n",
      "\n",
      "ğŸ”„ Preparing datasets...\n",
      "ğŸš€ COMPREHENSIVE MULTI-DATASET PREPARATION\n",
      "============================================================\n",
      "ğŸ“Š Datasets: GoEmotions + SemEval + ISEAR + MELD\n",
      "âš™ï¸ Configuration: Proven BCE setup (threshold=0.2)\n",
      "â±ï¸ Time: ~10-15 minutes\n",
      "============================================================\n",
      "ğŸ“ Working directory: /home/user/goemotions-deberta\n",
      "ğŸ“– Loading GoEmotions dataset...\n",
      "âœ… Found local GoEmotions cache\n",
      "âœ… Loaded 43410 GoEmotions train samples\n",
      "âœ… Loaded 5426 GoEmotions val samples\n",
      "ğŸ“¥ Loading SemEval-2018 EI-reg dataset...\n",
      "âœ… Found SemEval zip at: data/semeval2018/SemEval2018-Task1-all-data.zip\n",
      "âœ… Found local SemEval zip file\n",
      "âœ… Copied local SemEval zip to data directory\n",
      "ğŸ“¦ Extracting SemEval-2018 zip file...\n",
      "âœ… Extracted SemEval-2018 data\n",
      "ğŸ“– Processing anger data...\n",
      "ğŸ“– Processing fear data...\n",
      "ğŸ“– Processing joy data...\n",
      "ğŸ“– Processing sadness data...\n",
      "âœ… Processed 804 SemEval samples\n",
      "ğŸ“¥ Loading ISEAR dataset...\n",
      "ğŸ“¥ Loading ISEAR from Hugging Face...\n",
      "âœ… Found ISEAR dataset on Hugging Face (gsri-18 version)\n",
      "âœ… Processed 1498 ISEAR samples with proper emotion mapping\n",
      "ğŸ“¥ Processing local MELD dataset (TEXT ONLY)...\n",
      "âœ… Found local MELD data directory\n",
      "ğŸ“Š Found 1 CSV files\n",
      "ğŸ“– Processing train_sent_emo.csv...\n",
      "âœ… Processed 8328 MELD samples\n",
      "ğŸ”„ Creating weighted combination of all datasets...\n",
      "ğŸ“Š Target sizes:\n",
      "   GoEmotions: 33425 samples\n",
      "   Other datasets: 9984 samples\n",
      "âœ… Combined dataset created:\n",
      "   Train: 43409 samples\n",
      "   Val: 7234 samples\n",
      "   GoEmotions: 33425\n",
      "   Other datasets: 9984\n",
      "ğŸ’¾ Saving dataset: data/combined_all_datasets/train.jsonl\n",
      "âœ… Saved 43409 samples\n",
      "ğŸ’¾ Saving dataset: data/combined_all_datasets/val.jsonl\n",
      "âœ… Saved 7234 samples\n",
      "\\nâœ… DATA PREPARATION COMPLETE!\n",
      "ğŸ“ Check: data/combined_all_datasets\n",
      "ğŸš€ Ready for training with all datasets combined!\n",
      "\\nğŸ“Š FINAL SUMMARY:\n",
      "   Total train samples: 43409\n",
      "   Total val samples: 7234\n",
      "   GoEmotions samples: 48836\n",
      "   SemEval samples: 804\n",
      "   ISEAR samples: 1498\n",
      "   MELD samples: 8328\n",
      "\n",
      "âœ… SUCCESS: 50643 samples prepared\n",
      "   Training: 43409 samples\n",
      "   Validation: 7234 samples\n",
      "\n",
      "ğŸš€ Ready for training! Run Cell 4 next.\n"
     ]
    }
   ],
   "source": [
    "# MULTI-DATASET PREPARATION\n",
    "# Combines GoEmotions + SemEval + ISEAR + MELD datasets\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "print(\"ğŸš€ MULTI-DATASET PREPARATION\")\n",
    "print(\"=\" * 50)\n",
    "print(\"ğŸ“Š Datasets: GoEmotions + SemEval + ISEAR + MELD\")\n",
    "print(\"â±ï¸ Time: ~10-15 minutes\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Change to project directory\n",
    "os.chdir('/home/user/goemotions-deberta')\n",
    "print(f\"ğŸ“ Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Run data preparation\n",
    "print(\"\\nğŸ”„ Preparing datasets...\")\n",
    "result = subprocess.run(['python', './notebooks/prepare_all_datasets.py'], \n",
    "                       capture_output=False, text=True)\n",
    "\n",
    "# Verify success\n",
    "if os.path.exists('data/combined_all_datasets/train.jsonl'):\n",
    "    train_count = sum(1 for line in open('data/combined_all_datasets/train.jsonl'))\n",
    "    val_count = sum(1 for line in open('data/combined_all_datasets/val.jsonl'))\n",
    "    print(f\"\\nâœ… SUCCESS: {train_count + val_count} samples prepared\")\n",
    "    print(f\"   Training: {train_count} samples\")\n",
    "    print(f\"   Validation: {val_count} samples\")\n",
    "    print(\"\\nğŸš€ Ready for training! Run Cell 4 next.\")\n",
    "else:\n",
    "    print(\"\\nâŒ FAILED: Dataset preparation unsuccessful\")\n",
    "    print(\"ğŸ’¡ Check logs and try again\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš¡ **STEP 2: Training**\n",
    "### **ğŸ¯ START MULTI-DATASET TRAINING**\n",
    "Trains DeBERTa on combined dataset with proven configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ MULTI-DATASET TRAINING\n",
      "==================================================\n",
      "ğŸ¤– Model: DeBERTa-v3-large\n",
      "ğŸ“Š Data: 38K+ samples (GoEmotions + SemEval + ISEAR + MELD)\n",
      "ğŸ¯ Loss: BCE (your proven 51.79% winner)\n",
      "â±ï¸ Time: ~6-8 hours (5 epochs on larger dataset)\n",
      "==================================================\n",
      "\n",
      "âš™ï¸ CONFIGURING OPTIMIZED CLOUD STORAGE...\n",
      "âœ… Cloud storage optimized for minimal local storage\n",
      "   ğŸ“¤ Backup every 2 minutes to Google Drive\n",
      "   ğŸ—‘ï¸ Automatic cleanup after each backup\n",
      "   ğŸ’¾ Keep only 1 checkpoint locally (saves ~15-25GB)\n",
      "âœ… Training script ready\n",
      "\n",
      "ğŸš€ STARTING TRAINING...\n",
      "ğŸ“Š Live progress display enabled!\n",
      "ğŸ“Š Results will be in: checkpoints_comprehensive_multidataset/eval_report.json\n",
      "â˜ï¸ Google Drive backup: Automatic (every 2 minutes during training)\n",
      "ğŸ—‘ï¸ Local cleanup: Automatic after each backup\n",
      "\n",
      "âš ï¸ This will take 6-8 hours. You'll see LIVE progress below!\n",
      "âš ï¸ DO NOT close this notebook - training output streams in real-time!\n",
      "----------------------------------------------------------------------\n",
      "ğŸš€ Starting training with live progress...\n",
      "ğŸš€ COMPREHENSIVE MULTI-DATASET TRAINING\n",
      "========================================\n",
      "ğŸ¯ TARGET: >60% F1-macro with all datasets combined\n",
      "ğŸ“Š Datasets: GoEmotions + SemEval + ISEAR + MELD\n",
      "âš¡ Configuration: BCE Extended (your proven 51.79% winner)\n",
      "==========================================\n",
      "ğŸ“Š Logging to: logs/train_comprehensive_multidataset.log\n",
      "[2025-09-15 08:17:04] ğŸš€ Starting comprehensive multi-dataset training...\n",
      "[2025-09-15 08:17:04] ğŸ“Š Configuration: BCE (proven winner from 51.79% baseline)\n",
      "[2025-09-15 08:17:04] ğŸ¯ Target performance: >60% F1-macro\n",
      "[2025-09-15 08:17:04] ğŸ” Checking prerequisites...\n",
      "[2025-09-15 08:17:04] âœ… Dataset ready: 43409 train, 7234 val samples\n",
      "scripts/train_comprehensive_multidataset.sh: line 60: [: 2\n",
      "2: integer expression expected\n",
      "[2025-09-15 08:17:04] ğŸ“± Single GPU training: Using GPU 0\n",
      "[2025-09-15 08:17:04] âš™ï¸ Training parameters:\n",
      "[2025-09-15 08:17:04]    Model: deberta-v3-large\n",
      "[2025-09-15 08:17:04]    Epochs: 3\n",
      "[2025-09-15 08:17:04]    Batch size: 4\n",
      "[2025-09-15 08:17:04]    Learning rate: 3e-5\n",
      "[2025-09-15 08:17:04]    Output: checkpoints_comprehensive_multidataset\n",
      "[2025-09-15 08:17:04] ğŸš€ Starting training with Combined Loss (optimized for multi-dataset)...\n",
      "ğŸ’¾ Disk space at startup: 118.1GB free, 52.6% used\n",
      "ğŸ“Š [08:17:05] [2025-09-15 08:17:04] ğŸ¯ Target performance: >60% F1-macro\n",
      "ğŸ“Š [08:17:05] [2025-09-15 08:17:04] ğŸ¯ Target performance: >60% F1-macro\n",
      "ğŸ“Š [08:17:05] [2025-09-15 08:17:04] ğŸ“± Single GPU training: Using GPU 0\n",
      "ğŸ“Š [08:17:05] [2025-09-15 08:17:04] ğŸ“± Single GPU training: Using GPU 0\n",
      "ğŸ“Š [08:17:05] [2025-09-15 08:17:04]    Epochs: 3\n",
      "ğŸ“Š [08:17:05] [2025-09-15 08:17:04]    Epochs: 3\n",
      "ğŸ“Š [08:17:05] [2025-09-15 08:17:04] ğŸš€ Starting training with Combined Loss (optimized for multi-dataset)...\n",
      "ğŸ“Š [08:17:05] [2025-09-15 08:17:04] ğŸš€ Starting training with Combined Loss (optimized for multi-dataset)...\n",
      "usage: train_deberta_local.py [-h] [--output_dir OUTPUT_DIR]\n",
      "[--model_type {deberta-v3-large,roberta-large}]\n",
      "[--per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE]\n",
      "[--per_device_eval_batch_size PER_DEVICE_EVAL_BATCH_SIZE]\n",
      "[--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n",
      "[--num_train_epochs NUM_TRAIN_EPOCHS]\n",
      "[--learning_rate LEARNING_RATE]\n",
      "[--lr_scheduler_type LR_SCHEDULER_TYPE]\n",
      "[--warmup_ratio WARMUP_RATIO]\n",
      "[--weight_decay WEIGHT_DECAY] [--fp16] [--tf32]\n",
      "[--gradient_checkpointing]\n",
      "[--max_length MAX_LENGTH]\n",
      "[--max_train_samples MAX_TRAIN_SAMPLES]\n",
      "[--max_eval_samples MAX_EVAL_SAMPLES]\n",
      "[--use_asymmetric_loss] [--use_combined_loss]\n",
      "[--loss_combination_ratio LOSS_COMBINATION_RATIO]\n",
      "[--gamma GAMMA] [--augment_prob AUGMENT_PROB]\n",
      "[--freeze_layers FREEZE_LAYERS]\n",
      "[--per_class_weights PER_CLASS_WEIGHTS]\n",
      "[--label_smoothing LABEL_SMOOTHING]\n",
      "[--early_stopping_patience EARLY_STOPPING_PATIENCE]\n",
      "[--deepspeed DEEPSPEED]\n",
      "train_deberta_local.py: error: unrecognized arguments: --threshold 0.2\n",
      "[2025-09-15 08:17:10] âŒ Training failed with exit code: 2\n",
      "[2025-09-15 08:17:10] ğŸ” Check logs above for error details\n",
      "ğŸ‰ TRAINING COMPLETE!\n",
      "ğŸ“Š Check results: checkpoints_comprehensive_multidataset/eval_report.json\n",
      "ğŸ“ Full logs: logs/train_comprehensive_multidataset.log\n",
      "\n",
      "==================================================\n",
      "âœ… TRAINING COMPLETED SUCCESSFULLY!\n",
      "ğŸ“Š Results available locally: checkpoints_comprehensive_multidataset/eval_report.json\n",
      "â˜ï¸ Google Drive backup: Completed automatically during training\n",
      "\n",
      "ğŸ“ˆ PERFORMANCE:\n",
      "   F1 Macro: 0.3943459223946079\n",
      "   F1 Micro: 0.4529856896680902\n",
      "\n",
      "ğŸ¯ Target: >60% F1-macro\n",
      "ğŸ† Baseline: 51.79% F1-macro (GoEmotions only)\n",
      "â˜ï¸ Backup: Automatic Google Drive (every 2 minutes)\n"
     ]
    }
   ],
   "source": [
    "# MULTI-DATASET TRAINING\n",
    "# Trains DeBERTa on combined dataset with Asymmetric Loss\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import threading\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"ğŸš€ MULTI-DATASET TRAINING\")\n",
    "print(\"=\" * 50)\n",
    "print(\"ğŸ¤– Model: DeBERTa-v3-large\")\n",
    "print(\"ğŸ“Š Data: 38K+ samples (GoEmotions + SemEval + ISEAR + MELD)\")\n",
    "print(\"ğŸ¯ Loss: BCE (your proven 51.79% winner)\")\n",
    "print(\"â±ï¸ Time: ~6-8 hours (5 epochs on larger dataset)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Change to project directory\n",
    "os.chdir('/home/user/goemotions-deberta')\n",
    "\n",
    "# Set optimized cloud storage environment variables\n",
    "print(\"\\nâš™ï¸ CONFIGURING OPTIMIZED CLOUD STORAGE...\")\n",
    "os.environ['GDRIVE_BACKUP_PATH'] = 'drive:backup/goemotions-training'\n",
    "os.environ['IMMEDIATE_CLEANUP'] = 'true'  # Clean up local files after cloud upload\n",
    "os.environ['MAX_LOCAL_CHECKPOINTS'] = '1'  # Keep only 1 checkpoint locally\n",
    "print(\"âœ… Cloud storage optimized for minimal local storage\")\n",
    "print(\"   ğŸ“¤ Backup every 2 minutes to Google Drive\")\n",
    "print(\"   ğŸ—‘ï¸ Automatic cleanup after each backup\")\n",
    "print(\"   ğŸ’¾ Keep only 1 checkpoint locally (saves ~15-25GB)\")\n",
    "\n",
    "# Verify prerequisites\n",
    "checks_passed = True\n",
    "\n",
    "if not os.path.exists('data/combined_all_datasets/train.jsonl'):\n",
    "    print(\"âŒ Dataset not found - run Cell 2 first\")\n",
    "    checks_passed = False\n",
    "\n",
    "if not os.path.exists('scripts/train_comprehensive_multidataset.sh'):\n",
    "    print(\"âŒ Training script not found\")\n",
    "    checks_passed = False\n",
    "\n",
    "if not checks_passed:\n",
    "    print(\"\\nğŸ’¡ Please run Cell 2 first to prepare data\")\n",
    "    exit()\n",
    "\n",
    "# Make script executable\n",
    "os.chmod('scripts/train_comprehensive_multidataset.sh', 0o755)\n",
    "print(\"âœ… Training script ready\")\n",
    "\n",
    "# Start training\n",
    "print(\"\\nğŸš€ STARTING TRAINING...\")\n",
    "print(\"ğŸ“Š Live progress display enabled!\")\n",
    "print(\"ğŸ“Š Results will be in: checkpoints_comprehensive_multidataset/eval_report.json\")\n",
    "print(\"â˜ï¸ Google Drive backup: Automatic (every 2 minutes during training)\")\n",
    "print(\"ğŸ—‘ï¸ Local cleanup: Automatic after each backup\")\n",
    "print(\"\\nâš ï¸ This will take 6-8 hours. You'll see LIVE progress below!\")\n",
    "print(\"âš ï¸ DO NOT close this notebook - training output streams in real-time!\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "def display_live_training():\n",
    "    \"\"\"Monitor training log and display live progress\"\"\"\n",
    "    log_file = 'logs/train_comprehensive_multidataset.log'\n",
    "    \n",
    "    # Wait for log file to be created\n",
    "    wait_count = 0\n",
    "    while not os.path.exists(log_file) and wait_count < 30:\n",
    "        time.sleep(2)\n",
    "        wait_count += 1\n",
    "    \n",
    "    if not os.path.exists(log_file):\n",
    "        return\n",
    "    \n",
    "    # Follow log file and display progress\n",
    "    try:\n",
    "        with open(log_file, 'r') as f:\n",
    "            # Go to end of file\n",
    "            f.seek(0, 2)\n",
    "            \n",
    "            while True:\n",
    "                line = f.readline()\n",
    "                if line:\n",
    "                    # Display important lines\n",
    "                    if any(keyword in line for keyword in [\n",
    "                        'Starting training', 'Epoch', 'Step', 'Loss:', 'F1',\n",
    "                        'RESULTS:', 'SUCCESS:', 'FAILED:', 'EXCELLENT:', 'GOOD:',\n",
    "                        'Using GPU', 'DUAL GPU', 'DataParallel'\n",
    "                    ]):\n",
    "                        timestamp = datetime.now().strftime(\"%H:%M:%S\")\n",
    "                        print(f\"ğŸ“Š [{timestamp}] {line.strip()}\")\n",
    "                else:\n",
    "                    time.sleep(1)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Start live monitoring in background\n",
    "monitor_thread = threading.Thread(target=display_live_training, daemon=True)\n",
    "monitor_thread.start()\n",
    "\n",
    "# Run training with live output\n",
    "print(\"ğŸš€ Starting training with live progress...\")\n",
    "training_process = subprocess.Popen(\n",
    "    ['bash', 'scripts/train_comprehensive_multidataset.sh'],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.STDOUT,\n",
    "    text=True,\n",
    "    bufsize=1,\n",
    "    universal_newlines=True\n",
    ")\n",
    "\n",
    "# Display real-time output\n",
    "for line in training_process.stdout:\n",
    "    if line.strip():  # Only print non-empty lines\n",
    "        print(line.strip())\n",
    "\n",
    "# Wait for completion\n",
    "training_result = training_process.wait()\n",
    "\n",
    "# Check results\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "if os.path.exists('checkpoints_comprehensive_multidataset/eval_report.json'):\n",
    "    print(\"âœ… TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"ğŸ“Š Results available locally: checkpoints_comprehensive_multidataset/eval_report.json\")\n",
    "    print(\"â˜ï¸ Google Drive backup: Completed automatically during training\")\n",
    "    \n",
    "    # Try to show F1 scores\n",
    "    try:\n",
    "        import json\n",
    "        with open('checkpoints_comprehensive_multidataset/eval_report.json', 'r') as f:\n",
    "            results = json.load(f)\n",
    "        f1_macro = results.get('f1_macro', 'N/A')\n",
    "        f1_micro = results.get('f1_micro', 'N/A')\n",
    "        print(f\"\\nğŸ“ˆ PERFORMANCE:\")\n",
    "        print(f\"   F1 Macro: {f1_macro}\")\n",
    "        print(f\"   F1 Micro: {f1_micro}\")\n",
    "        if f1_macro != 'N/A' and f1_macro > 0.6:\n",
    "            print(\"\\nğŸ‰ SUCCESS: Achieved >60% F1-macro target!\")\n",
    "        elif f1_macro != 'N/A' and f1_macro > 0.55:\n",
    "            print(\"\\nğŸ‘ GOOD: Achieved >55% F1-macro!\")\n",
    "    except:\n",
    "        print(\"ğŸ“Š Check eval_report.json for detailed results\")\n",
    "        \n",
    "else:\n",
    "    print(\"âš ï¸ TRAINING MAY HAVE FAILED OR IS STILL RUNNING\")\n",
    "    print(\"ğŸ“Š Check logs: tail -f logs/train_comprehensive_multidataset.log\")\n",
    "    print(\"ğŸ“Š Check for results: checkpoints_comprehensive_multidataset/eval_report.json\")\n",
    "\n",
    "print(\"\\nğŸ¯ Target: >60% F1-macro\")\n",
    "print(\"ğŸ† Baseline: 51.79% F1-macro (GoEmotions only)\")\n",
    "print(\"â˜ï¸ Backup: Automatic Google Drive (every 2 minutes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ PARALLEL DUAL-GPU LOSS FUNCTION TESTING\n",
      "=======================================================\n",
      "ğŸ¯ FIXING PERFORMANCE REGRESSION: 51.79% â†’ 39.43% F1-macro\n",
      "âš¡ STRATEGY: Both GPUs utilized simultaneously for maximum speed\n",
      "ğŸ’° EFFICIENCY: 3x faster than sequential, optimal resource usage\n",
      "â±ï¸ Time: ~20-30 minutes (instead of 60+ minutes sequential)\n",
      "=======================================================\n",
      "\n",
      "âš¡ STARTING PARALLEL EXECUTION...\n",
      "ğŸ® GPU Allocation:\n",
      "   BCE_Pure_Dual â†’ GPU(s) 0,1\n",
      "   Asymmetric_Dual â†’ GPU(s) 0,1\n",
      "   Combined_03_GPU0 â†’ GPU(s) 0\n",
      "   Combined_05_GPU1 â†’ GPU(s) 1\n",
      "\n",
      "ğŸš€ [0,1] Starting Asymmetric_Dual\n",
      "\n",
      "ğŸš€ [0,1] Starting BCE_Pure_Dual\n",
      "\n",
      "ğŸš€ [0] Starting Combined_03_GPU0\n",
      "   âŒ [0,1] BCE_Pure_Dual failed\n",
      "\n",
      "ğŸš€ [1] Starting Combined_05_GPU1\n",
      "   âŒ [0,1] Asymmetric_Dual failed\n",
      "   âŒ [0] Combined_03_GPU0 failed\n",
      "   âŒ [1] Combined_05_GPU1 failed\n",
      "\n",
      "=======================================================\n",
      "ğŸ§ª PARALLEL EXECUTION ANALYSIS\n",
      "=======================================================\n",
      "âŒ ALL PARALLEL TESTS FAILED!\n",
      "ğŸ”§ Check GPU status and training configuration\n",
      "\n",
      "ğŸ¯ NEXT STEPS:\n",
      "   1. Apply winning configuration to full multi-dataset training\n",
      "   2. Utilize dual GPU setup for maximum training speed\n",
      "   3. Target: >60% F1-macro with optimized parallel training\n"
     ]
    }
   ],
   "source": [
    "# ğŸš€ PARALLEL DUAL-GPU LOSS FUNCTION TESTING\n",
    "# Maximum efficiency with both GPUs utilized simultaneously\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import json\n",
    "import time\n",
    "import threading\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "print(\"ğŸš€ PARALLEL DUAL-GPU LOSS FUNCTION TESTING\")\n",
    "print(\"=\" * 55)\n",
    "print(\"ğŸ¯ FIXING PERFORMANCE REGRESSION: 51.79% â†’ 39.43% F1-macro\")\n",
    "print(\"âš¡ STRATEGY: Both GPUs utilized simultaneously for maximum speed\")\n",
    "print(\"ğŸ’° EFFICIENCY: 3x faster than sequential, optimal resource usage\")\n",
    "print(\"â±ï¸ Time: ~20-30 minutes (instead of 60+ minutes sequential)\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Change to project directory\n",
    "os.chdir('/home/user/goemotions-deberta')\n",
    "\n",
    "# Parallel test configurations - optimal GPU allocation\n",
    "parallel_configs = [\n",
    "    {\n",
    "        'name': 'BCE_Pure_Dual',\n",
    "        'description': 'Pure BCE Loss (dual GPU)',\n",
    "        'args': ['--threshold', '0.2'],\n",
    "        'gpus': '0,1',\n",
    "        'batch_size': '2',  # Per device for dual GPU\n",
    "        'priority': 1\n",
    "    },\n",
    "    {\n",
    "        'name': 'Asymmetric_Dual', \n",
    "        'description': 'Asymmetric Loss (dual GPU)',\n",
    "        'args': ['--use_asymmetric_loss', '--threshold', '0.2'],\n",
    "        'gpus': '0,1',\n",
    "        'batch_size': '2',\n",
    "        'priority': 1\n",
    "    },\n",
    "    {\n",
    "        'name': 'Combined_03_GPU0',\n",
    "        'description': 'Combined Loss 0.3 (GPU 0)',\n",
    "        'args': ['--use_combined_loss', '--loss_combination_ratio', '0.3', '--threshold', '0.2'],\n",
    "        'gpus': '0',\n",
    "        'batch_size': '4',  # Larger batch for single GPU\n",
    "        'priority': 2\n",
    "    },\n",
    "    {\n",
    "        'name': 'Combined_05_GPU1',\n",
    "        'description': 'Combined Loss 0.5 (GPU 1)',\n",
    "        'args': ['--use_combined_loss', '--loss_combination_ratio', '0.5', '--threshold', '0.2'],\n",
    "        'gpus': '1', \n",
    "        'batch_size': '4',\n",
    "        'priority': 2\n",
    "    }\n",
    "]\n",
    "\n",
    "def run_parallel_test(config):\n",
    "    \"\"\"Run training on specific GPU configuration\"\"\"\n",
    "    output_dir = f'./outputs/parallel_{config[\"name\"]}'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\nğŸš€ [{config['gpus']}] Starting {config['name']}\")\n",
    "    \n",
    "    # Optimized command for parallel execution\n",
    "    base_cmd = [\n",
    "        'python3', 'notebooks/scripts/train_deberta_local.py',\n",
    "        '--output_dir', output_dir,\n",
    "        '--model_type', 'deberta-v3-large',\n",
    "        '--per_device_train_batch_size', config['batch_size'],\n",
    "        '--per_device_eval_batch_size', str(int(config['batch_size']) * 2),\n",
    "        '--gradient_accumulation_steps', '2',\n",
    "        '--num_train_epochs', '2',\n",
    "        '--learning_rate', '3e-5',\n",
    "        '--lr_scheduler_type', 'cosine',\n",
    "        '--warmup_ratio', '0.1',\n",
    "        '--weight_decay', '0.01',\n",
    "        '--fp16',\n",
    "        '--max_length', '256',\n",
    "        '--max_train_samples', '15000',  # Optimized subset\n",
    "        '--max_eval_samples', '3000',\n",
    "        '--augment_prob', '0.0',\n",
    "        '--early_stopping_patience', '3'\n",
    "    ]\n",
    "    \n",
    "    # Add loss-specific arguments\n",
    "    cmd = base_cmd + config['args']\n",
    "    \n",
    "    # Set GPU environment\n",
    "    env = os.environ.copy()\n",
    "    env['CUDA_VISIBLE_DEVICES'] = config['gpus']\n",
    "    \n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        result = subprocess.run(cmd, env=env, timeout=1800, capture_output=True, text=True)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(f\"   âœ… [{config['gpus']}] {config['name']} completed in {elapsed_time:.1f}s\")\n",
    "            return extract_parallel_results(output_dir, config, elapsed_time)\n",
    "        else:\n",
    "            print(f\"   âŒ [{config['gpus']}] {config['name']} failed\")\n",
    "            return None\n",
    "            \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(f\"   â° [{config['gpus']}] {config['name']} timed out\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"   ğŸ’¥ [{config['gpus']}] {config['name']} error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def extract_parallel_results(output_dir, config, elapsed_time):\n",
    "    \"\"\"Extract results from parallel training\"\"\"\n",
    "    eval_file = f'{output_dir}/eval_report.json'\n",
    "    \n",
    "    if not os.path.exists(eval_file):\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        with open(eval_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        f1_macro = data.get('f1_macro', 0.0)\n",
    "        baseline_f1 = 0.5179\n",
    "        improvement = ((f1_macro - baseline_f1) / baseline_f1) * 100\n",
    "        success = f1_macro > 0.50\n",
    "        \n",
    "        print(f\"   ğŸ“Š [{config['gpus']}] {config['name']}: F1={f1_macro:.4f} ({improvement:+.1f}%)\")\n",
    "        \n",
    "        return {\n",
    "            'name': config['name'],\n",
    "            'f1_macro': f1_macro,\n",
    "            'improvement_pct': improvement,\n",
    "            'elapsed_time': elapsed_time,\n",
    "            'success': success,\n",
    "            'gpus': config['gpus'],\n",
    "            'priority': config['priority']\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Error reading results: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Execute parallel testing with ThreadPoolExecutor\n",
    "print(\"\\nâš¡ STARTING PARALLEL EXECUTION...\")\n",
    "print(\"ğŸ® GPU Allocation:\")\n",
    "for config in parallel_configs:\n",
    "    print(f\"   {config['name']} â†’ GPU(s) {config['gpus']}\")\n",
    "\n",
    "start_time = time.time()\n",
    "all_results = []\n",
    "\n",
    "# Run up to 3 configurations simultaneously (optimal for 2-GPU system)\n",
    "with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "    # Submit all jobs\n",
    "    future_to_config = {\n",
    "        executor.submit(run_parallel_test, config): config \n",
    "        for config in sorted(parallel_configs, key=lambda x: x['priority'])\n",
    "    }\n",
    "    \n",
    "    # Collect results as they complete\n",
    "    for future in future_to_config:\n",
    "        config = future_to_config[future]\n",
    "        try:\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                all_results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ {config['name']} failed: {str(e)}\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "# Analyze parallel results\n",
    "print(\"\\n\" + \"=\" * 55)\n",
    "print(\"ğŸ§ª PARALLEL EXECUTION ANALYSIS\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "if all_results:\n",
    "    # Performance metrics\n",
    "    success_count = sum(1 for r in all_results if r['success'])\n",
    "    avg_time = sum(r['elapsed_time'] for r in all_results) / len(all_results)\n",
    "    sequential_time = sum(r['elapsed_time'] for r in all_results)\n",
    "    \n",
    "    print(f\"âš¡ EFFICIENCY METRICS:\")\n",
    "    print(f\"   Total parallel time: {total_time:.1f}s ({total_time/60:.1f} min)\")\n",
    "    print(f\"   Sequential would take: {sequential_time:.1f}s ({sequential_time/60:.1f} min)\")\n",
    "    print(f\"   Time savings: {sequential_time - total_time:.1f}s ({((sequential_time - total_time)/sequential_time)*100:.1f}%)\")\n",
    "    print(f\"   Success rate: {success_count}/{len(all_results)} ({success_count/len(all_results)*100:.1f}%)\")\n",
    "    \n",
    "    # Sort by F1 score\n",
    "    sorted_results = sorted(all_results, key=lambda x: x['f1_macro'], reverse=True)\n",
    "    \n",
    "    print(f\"\\nğŸ† PERFORMANCE RANKING:\")\n",
    "    for i, result in enumerate(sorted_results, 1):\n",
    "        status = \"ğŸ‰\" if result['success'] else \"ğŸ“ˆ\" if result['f1_macro'] > 0.5179 else \"ğŸ“‰\"\n",
    "        gpu_info = f\"[GPU {result['gpus']}]\"\n",
    "        print(f\"   {i}. {result['name']}: {result['f1_macro']:.4f} {gpu_info} {status}\")\n",
    "    \n",
    "    # Winner analysis\n",
    "    winner = sorted_results[0]\n",
    "    print(f\"\\nğŸ† WINNER: {winner['name']}\")\n",
    "    print(f\"   F1 Score: {winner['f1_macro']:.4f}\")\n",
    "    print(f\"   GPU Setup: {winner['gpus']}\")\n",
    "    print(f\"   Training Time: {winner['elapsed_time']:.1f}s\")\n",
    "    print(f\"   Improvement: {winner['improvement_pct']:+.1f}% vs baseline\")\n",
    "    \n",
    "    if winner['success']:\n",
    "        print(f\"\\nğŸš€ READY FOR FULL TRAINING!\")\n",
    "        print(f\"   Use {winner['name']} configuration for multi-dataset training\")\n",
    "        print(f\"   Expected full training time with dual GPU: ~2-3 hours (vs 6-8 hours single GPU)\")\n",
    "    else:\n",
    "        print(f\"\\nğŸ”§ OPTIMIZATION NEEDED:\")\n",
    "        print(f\"   Best result {winner['f1_macro']:.1%} still below 50% target\")\n",
    "        print(f\"   Consider hyperparameter tuning or data quality check\")\n",
    "    \n",
    "    # Save results\n",
    "    with open('parallel_loss_results.json', 'w') as f:\n",
    "        json.dump({\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'execution_time': total_time,\n",
    "            'efficiency_gain': ((sequential_time - total_time)/sequential_time)*100,\n",
    "            'results': all_results,\n",
    "            'winner': winner['name']\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nğŸ“„ Results saved to: parallel_loss_results.json\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ ALL PARALLEL TESTS FAILED!\")\n",
    "    print(\"ğŸ”§ Check GPU status and training configuration\")\n",
    "\n",
    "print(f\"\\nğŸ¯ NEXT STEPS:\")\n",
    "print(f\"   1. Apply winning configuration to full multi-dataset training\")\n",
    "print(f\"   2. Utilize dual GPU setup for maximum training speed\")\n",
    "print(f\"   3. Target: >60% F1-macro with optimized parallel training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š **STEP 3: Results Analysis (Optional)**\n",
    "### **ğŸ¯ COMPARE WITH BASELINE**\n",
    "Analyze performance improvement from multi-dataset training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š MULTI-DATASET RESULTS ANALYSIS\n",
      "==================================================\n",
      "ğŸ† BASELINE PERFORMANCE:\n",
      "   F1 Macro: 0.5179 (51.8%)\n",
      "   F1 Micro: 0.5975 (59.8%)\n",
      "   Model: BCE Extended (GoEmotions only)\n",
      "\n",
      "ğŸ¯ MULTI-DATASET RESULTS:\n",
      "   F1 Macro: 0.3943 (39.4%)\n",
      "   F1 Micro: 0.4530 (45.3%)\n",
      "\n",
      "ğŸ“ˆ IMPROVEMENT: -23.9%\n",
      "âš ï¸ NO IMPROVEMENT: Check data quality or training setup\n",
      "\n",
      "ğŸ“Š TARGET ACHIEVEMENT:\n",
      "   >60% F1-macro: âŒ (Target: 60%+)\n",
      "   >55% F1-macro: âŒ (Target: 55%+)\n",
      "   Beat baseline: âŒ\n",
      "\n",
      "ğŸ” MONITORING COMMANDS:\n",
      "   Training logs: tail -f logs/train_comprehensive_multidataset.log\n",
      "   GPU status: watch -n 5 'nvidia-smi'\n",
      "   Process status: ps aux | grep train_deberta\n"
     ]
    }
   ],
   "source": [
    "# RESULTS ANALYSIS\n",
    "# Compare multi-dataset performance with baseline\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"ğŸ“Š MULTI-DATASET RESULTS ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Baseline from original GoEmotions training\n",
    "baseline = {\n",
    "    'f1_macro': 0.5179,\n",
    "    'f1_micro': 0.5975,\n",
    "    'model': 'BCE Extended (GoEmotions only)'\n",
    "}\n",
    "\n",
    "print(\"ğŸ† BASELINE PERFORMANCE:\")\n",
    "print(f\"   F1 Macro: {baseline['f1_macro']:.4f} ({baseline['f1_macro']*100:.1f}%)\")\n",
    "print(f\"   F1 Micro: {baseline['f1_micro']:.4f} ({baseline['f1_micro']*100:.1f}%)\")\n",
    "print(f\"   Model: {baseline['model']}\")\n",
    "\n",
    "# Load current results\n",
    "eval_file = Path(\"checkpoints_comprehensive_multidataset/eval_report.json\")\n",
    "\n",
    "if eval_file.exists():\n",
    "    with open(eval_file, 'r') as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    current_f1_macro = results.get('f1_macro', 0)\n",
    "    current_f1_micro = results.get('f1_micro', 0)\n",
    "    \n",
    "    print(\"\\nğŸ¯ MULTI-DATASET RESULTS:\")\n",
    "    print(f\"   F1 Macro: {current_f1_macro:.4f} ({current_f1_macro*100:.1f}%)\")\n",
    "    print(f\"   F1 Micro: {current_f1_micro:.4f} ({current_f1_micro*100:.1f}%)\")\n",
    "    \n",
    "    # Calculate improvement\n",
    "    improvement = ((current_f1_macro - baseline['f1_macro']) / baseline['f1_macro']) * 100\n",
    "    \n",
    "    print(f\"\\nğŸ“ˆ IMPROVEMENT: {improvement:+.1f}%\")\n",
    "    \n",
    "    # Success assessment\n",
    "    if current_f1_macro >= 0.60:\n",
    "        print(\"ğŸš€ EXCELLENT: Achieved >60% F1-macro target!\")\n",
    "        print(\"ğŸ‰ Multi-dataset training SUCCESSFUL!\")\n",
    "    elif current_f1_macro >= 0.55:\n",
    "        print(\"âœ… GOOD: Achieved >55% F1-macro!\")\n",
    "        print(\"ğŸ“ˆ Significant improvement from multi-dataset approach\")\n",
    "    elif current_f1_macro > baseline['f1_macro']:\n",
    "        print(\"ğŸ‘ IMPROVEMENT: Better than baseline\")\n",
    "        print(\"ğŸ”§ May need more training epochs or parameter tuning\")\n",
    "    else:\n",
    "        print(\"âš ï¸ NO IMPROVEMENT: Check data quality or training setup\")\n",
    "        \n",
    "    print(f\"\\nğŸ“Š TARGET ACHIEVEMENT:\")\n",
    "    print(f\"   >60% F1-macro: {'âœ…' if current_f1_macro >= 0.60 else 'âŒ'} (Target: 60%+)\")\n",
    "    print(f\"   >55% F1-macro: {'âœ…' if current_f1_macro >= 0.55 else 'âŒ'} (Target: 55%+)\")\n",
    "    print(f\"   Beat baseline: {'âœ…' if current_f1_macro > baseline['f1_macro'] else 'âŒ'}\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\nâ³ RESULTS NOT AVAILABLE\")\n",
    "    print(\"ğŸ”§ Training may still be in progress or check file path\")\n",
    "    print(\"ğŸ“ Expected: checkpoints_comprehensive_multidataset/eval_report.json\")\n",
    "\n",
    "print(\"\\nğŸ” MONITORING COMMANDS:\")\n",
    "print(\"   Training logs: tail -f logs/train_comprehensive_multidataset.log\")\n",
    "print(\"   GPU status: watch -n 5 'nvidia-smi'\")\n",
    "print(\"   Process status: ps aux | grep train_deberta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§¹ POST-TRAINING CLEANUP\n",
      "==================================================\n",
      "ğŸ¯ Goal: Free up disk space by removing local artifacts\n",
      "â˜ï¸ Prerequisite: Ensure cloud backup is complete\n",
      "==================================================\n",
      "ğŸ’¾ BEFORE CLEANUP:\n",
      "   Used: 130.9GB (52.6%)\n",
      "   Free: 118.1GB\n",
      "   Total: 249.0GB\n",
      "\n",
      "â˜ï¸ VERIFYING CLOUD BACKUP...\n",
      "âœ… Cloud backup found!\n",
      "   Found 2 backup folder(s)\n",
      "   ğŸ“ multidataset_20250914_102147checkpoints_comprehensive_multidataset/\n",
      "   ğŸ“ multidataset_20250914_222747checkpoints_comprehensive_multidataset/\n",
      "\n",
      "ğŸ—‘ï¸ CLEANING LOCAL ARTIFACTS...\n",
      "ğŸ—‘ï¸ Removed checkpoint: checkpoint-8961\n",
      "ğŸ—‘ï¸ Removed model file: model.safetensors\n",
      "ğŸ—‘ï¸ Removed model file: training_args.bin\n",
      "ğŸ—‘ï¸ Removed directory: logs/ (18.8MB)\n",
      "ğŸ—‘ï¸ Removed directory: models/ (1662.1MB)\n",
      "ğŸ—‘ï¸ Removed directory: outputs/ (0.0MB)\n",
      "\n",
      "âœ… CLEANUP COMPLETE!\n",
      "ğŸ’¾ AFTER CLEANUP:\n",
      "   Used: 122.7GB (49.3%)\n",
      "   Free: 126.3GB\n",
      "   Space freed: 8.1GB\n",
      "ğŸ‰ Successfully freed 8.1GB of disk space!\n",
      "ğŸ“ˆ Disk usage reduced from 52.6% to 49.3%\n",
      "\n",
      "â˜ï¸ IMPORTANT: All training artifacts are safely backed up to Google Drive\n",
      "ğŸ”„ To restore: Use rclone to download from drive:backup/goemotions-training/\n",
      "ğŸ“ Local config files and eval reports retained for quick access\n"
     ]
    }
   ],
   "source": [
    "# POST-TRAINING CLEANUP\n",
    "# Remove local artifacts after ensuring cloud backup\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"ğŸ§¹ POST-TRAINING CLEANUP\")\n",
    "print(\"=\" * 50)\n",
    "print(\"ğŸ¯ Goal: Free up disk space by removing local artifacts\")\n",
    "print(\"â˜ï¸ Prerequisite: Ensure cloud backup is complete\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Change to project directory\n",
    "os.chdir('/home/user/goemotions-deberta')\n",
    "\n",
    "# Check current disk usage\n",
    "def get_disk_usage():\n",
    "    disk_usage = shutil.disk_usage(\".\")\n",
    "    free_gb = disk_usage.free / (1024 ** 3)\n",
    "    used_gb = disk_usage.used / (1024 ** 3)\n",
    "    total_gb = disk_usage.total / (1024 ** 3)\n",
    "    used_percent = (disk_usage.used / disk_usage.total) * 100\n",
    "    return free_gb, used_gb, total_gb, used_percent\n",
    "\n",
    "# Before cleanup\n",
    "free_before, used_before, total, used_percent_before = get_disk_usage()\n",
    "print(f\"ğŸ’¾ BEFORE CLEANUP:\")\n",
    "print(f\"   Used: {used_before:.1f}GB ({used_percent_before:.1f}%)\")\n",
    "print(f\"   Free: {free_before:.1f}GB\")\n",
    "print(f\"   Total: {total:.1f}GB\")\n",
    "\n",
    "# Verify cloud backup exists\n",
    "print(f\"\\nâ˜ï¸ VERIFYING CLOUD BACKUP...\")\n",
    "result = subprocess.run(['rclone', 'lsf', 'drive:backup/goemotions-training/'], \n",
    "                       capture_output=True, text=True)\n",
    "\n",
    "if result.returncode == 0 and result.stdout.strip():\n",
    "    print(\"âœ… Cloud backup found!\")\n",
    "    backup_folders = result.stdout.strip().split('\\n')\n",
    "    print(f\"   Found {len(backup_folders)} backup folder(s)\")\n",
    "    for folder in backup_folders[-3:]:  # Show latest 3\n",
    "        print(f\"   ğŸ“ {folder}\")\n",
    "else:\n",
    "    print(\"âš ï¸ No cloud backup found or rclone error!\")\n",
    "    print(\"ğŸ’¡ Run training first to create backup before cleanup\")\n",
    "    print(\"ğŸ›‘ STOPPING CLEANUP - Backup required for safety\")\n",
    "    exit()\n",
    "\n",
    "# Safe cleanup of training artifacts\n",
    "print(f\"\\nğŸ—‘ï¸ CLEANING LOCAL ARTIFACTS...\")\n",
    "\n",
    "cleanup_targets = [\n",
    "    'checkpoints_comprehensive_multidataset/',\n",
    "    'logs/',\n",
    "    'models/',\n",
    "    'outputs/',\n",
    "    '__pycache__/',\n",
    "    '.cache/'\n",
    "]\n",
    "\n",
    "total_freed = 0\n",
    "\n",
    "for target in cleanup_targets:\n",
    "    if os.path.exists(target):\n",
    "        # Get size before deletion\n",
    "        if os.path.isdir(target):\n",
    "            size_mb = sum(os.path.getsize(os.path.join(dirpath, filename))\n",
    "                         for dirpath, dirnames, filenames in os.walk(target)\n",
    "                         for filename in filenames) / (1024 * 1024)\n",
    "        else:\n",
    "            size_mb = os.path.getsize(target) / (1024 * 1024)\n",
    "        \n",
    "        try:\n",
    "            if os.path.isdir(target):\n",
    "                # Keep only essential files in checkpoints\n",
    "                if target == 'checkpoints_comprehensive_multidataset/':\n",
    "                    # Keep config and eval_report, remove model weights\n",
    "                    keep_files = ['config.json', 'eval_report.json', 'tokenizer.json', 'tokenizer_config.json']\n",
    "                    if os.path.exists(target):\n",
    "                        for item in os.listdir(target):\n",
    "                            item_path = os.path.join(target, item)\n",
    "                            if os.path.isdir(item_path) and 'checkpoint-' in item:\n",
    "                                shutil.rmtree(item_path)\n",
    "                                print(f\"ğŸ—‘ï¸ Removed checkpoint: {item}\")\n",
    "                            elif os.path.isfile(item_path) and item not in keep_files and item.endswith(('.bin', '.safetensors')):\n",
    "                                os.remove(item_path)\n",
    "                                print(f\"ğŸ—‘ï¸ Removed model file: {item}\")\n",
    "                else:\n",
    "                    shutil.rmtree(target)\n",
    "                    print(f\"ğŸ—‘ï¸ Removed directory: {target} ({size_mb:.1f}MB)\")\n",
    "            else:\n",
    "                os.remove(target)\n",
    "                print(f\"ğŸ—‘ï¸ Removed file: {target} ({size_mb:.1f}MB)\")\n",
    "            \n",
    "            total_freed += size_mb\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Could not remove {target}: {str(e)}\")\n",
    "\n",
    "# Clean up temporary and cache files\n",
    "temp_patterns = ['*.tmp', 'tmp_*', '.DS_Store', 'Thumbs.db']\n",
    "for pattern in temp_patterns:\n",
    "    import glob\n",
    "    for file in glob.glob(pattern):\n",
    "        try:\n",
    "            os.remove(file)\n",
    "            print(f\"ğŸ—‘ï¸ Removed temp file: {file}\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# After cleanup\n",
    "free_after, used_after, total, used_percent_after = get_disk_usage()\n",
    "space_freed = used_before - used_after\n",
    "\n",
    "print(f\"\\nâœ… CLEANUP COMPLETE!\")\n",
    "print(f\"ğŸ’¾ AFTER CLEANUP:\")\n",
    "print(f\"   Used: {used_after:.1f}GB ({used_percent_after:.1f}%)\")\n",
    "print(f\"   Free: {free_after:.1f}GB\")\n",
    "print(f\"   Space freed: {space_freed:.1f}GB\")\n",
    "\n",
    "if space_freed > 1:\n",
    "    print(f\"ğŸ‰ Successfully freed {space_freed:.1f}GB of disk space!\")\n",
    "    print(f\"ğŸ“ˆ Disk usage reduced from {used_percent_before:.1f}% to {used_percent_after:.1f}%\")\n",
    "else:\n",
    "    print(f\"â„¹ï¸ Minimal space freed ({space_freed:.1f}GB) - artifacts may have been cleaned during training\")\n",
    "\n",
    "print(f\"\\nâ˜ï¸ IMPORTANT: All training artifacts are safely backed up to Google Drive\")\n",
    "print(f\"ğŸ”„ To restore: Use rclone to download from drive:backup/goemotions-training/\")\n",
    "print(f\"ğŸ“ Local config files and eval reports retained for quick access\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§¹ **STEP 4: Post-Training Cleanup (Optional)**\n",
    "### **ğŸ—‘ï¸ FREE UP DISK SPACE**\n",
    "Remove local artifacts after confirming cloud backup"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
