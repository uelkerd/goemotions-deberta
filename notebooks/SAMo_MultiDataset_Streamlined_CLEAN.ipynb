{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ **SAMo Multi-Dataset DeBERTa (CLEAN VERSION)**\n",
    "## **Efficient Multi-Dataset Training with Proven BCE Configuration**\n",
    "\n",
    "### **ğŸ¯ MISSION**\n",
    "- **One-Command Multi-Dataset Training**: GoEmotions + SemEval + ISEAR + MELD\n",
    "- **Proven BCE Configuration**: Use your 51.79% F1-macro winning setup\n",
    "- **No Threshold Testing**: Save time with threshold=0.2\n",
    "- **Achieve >60% F1-macro**: Through comprehensive dataset integration\n",
    "\n",
    "### **ğŸ“‹ SIMPLE WORKFLOW**\n",
    "1. **Run Cell 2**: Data preparation (10-15 minutes)\n",
    "2. **Run Cell 4**: Training (3-4 hours)\n",
    "3. **Monitor**: `tail -f logs/train_comprehensive_multidataset.log`\n",
    "\n",
    "### **ğŸ“Š EXPECTED RESULTS**\n",
    "- **Baseline**: 51.79% F1-macro (GoEmotions BCE Extended)\n",
    "- **Target**: 60-65% F1-macro (All datasets combined)\n",
    "- **Dataset**: 38,111 samples (GoEmotions + SemEval + ISEAR + MELD)\n",
    "\n",
    "**Start with Cell 2 below!** ğŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¥ **STEP 1: Data Preparation**\n",
    "### **ğŸ¯ ONE COMMAND - PREPARE ALL DATASETS**\n",
    "Combines GoEmotions + SemEval + ISEAR + MELD into unified training format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ MULTI-DATASET PREPARATION\n",
      "==================================================\n",
      "ğŸ“Š Datasets: GoEmotions + SemEval + ISEAR + MELD\n",
      "â±ï¸ Time: ~10-15 minutes\n",
      "==================================================\n",
      "ğŸ“ Working directory: /home/user/goemotions-deberta\n",
      "\n",
      "ğŸ”„ Preparing datasets...\n",
      "ğŸš€ COMPREHENSIVE MULTI-DATASET PREPARATION\n",
      "============================================================\n",
      "ğŸ“Š Datasets: GoEmotions + SemEval + ISEAR + MELD\n",
      "âš™ï¸ Configuration: Proven BCE setup (threshold=0.2)\n",
      "â±ï¸ Time: ~10-15 minutes\n",
      "============================================================\n",
      "ğŸ“ Working directory: /home/user/goemotions-deberta\n",
      "ğŸ“– Loading GoEmotions dataset...\n",
      "âœ… Loaded 43410 GoEmotions train samples\n",
      "âœ… Loaded 5426 GoEmotions val samples\n",
      "ğŸ“¥ Processing local SemEval-2018 EI-reg dataset...\n",
      "âœ… Found local SemEval zip file\n",
      "âœ… Copied local SemEval zip to data directory\n",
      "ğŸ“¦ Extracting SemEval-2018 zip file...\n",
      "âœ… Extracted SemEval-2018 data\n",
      "ğŸ“– Processing anger data...\n",
      "ğŸ“– Processing fear data...\n",
      "ğŸ“– Processing joy data...\n",
      "ğŸ“– Processing sadness data...\n",
      "âœ… Processed 802 SemEval samples\n",
      "ğŸ“¥ Downloading ISEAR dataset...\n",
      "ğŸ“¥ Loading ISEAR from Hugging Face...\n",
      "âœ… Processed 7516 ISEAR samples\n",
      "ğŸ“¥ Processing local MELD dataset (TEXT ONLY)...\n",
      "âœ… Found local MELD data directory\n",
      "ğŸ“Š Found 3 CSV files\n",
      "ğŸ“– Processing train_sent_emo.csv...\n",
      "ğŸ“– Processing test_sent_emo.csv...\n",
      "ğŸ“– Processing dev_sent_emo.csv...\n",
      "âœ… Processed 13708 MELD samples\n",
      "ğŸ”„ Creating weighted combination of all datasets...\n",
      "ğŸ“Š Target sizes:\n",
      "   GoEmotions: 29301 samples\n",
      "   Other datasets: 8810 samples\n",
      "âœ… Combined dataset created:\n",
      "   Train: 30489 samples\n",
      "   Val: 7622 samples\n",
      "   GoEmotions: 29301\n",
      "   Other datasets: 8810\n",
      "ğŸ’¾ Saving dataset: data/combined_all_datasets/train.jsonl\n",
      "âœ… Saved 30489 samples\n",
      "ğŸ’¾ Saving dataset: data/combined_all_datasets/val.jsonl\n",
      "âœ… Saved 7622 samples\n",
      "\n",
      "âœ… DATA PREPARATION COMPLETE!\n",
      "ğŸ“ Check: data/combined_all_datasets\n",
      "ğŸš€ Ready for training with all datasets combined!\n",
      "\n",
      "ğŸ“Š FINAL SUMMARY:\n",
      "   Total train samples: 30489\n",
      "   Total val samples: 7622\n",
      "   GoEmotions samples: 48836\n",
      "   SemEval samples: 802\n",
      "   ISEAR samples: 7516\n",
      "   MELD samples: 13708\n",
      "\n",
      "âœ… SUCCESS: 38111 samples prepared\n",
      "   Training: 30489 samples\n",
      "   Validation: 7622 samples\n",
      "\n",
      "ğŸš€ Ready for training! Run Cell 4 next.\n"
     ]
    }
   ],
   "source": [
    "# MULTI-DATASET PREPARATION\n",
    "# Combines GoEmotions + SemEval + ISEAR + MELD datasets\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "print(\"ğŸš€ MULTI-DATASET PREPARATION\")\n",
    "print(\"=\" * 50)\n",
    "print(\"ğŸ“Š Datasets: GoEmotions + SemEval + ISEAR + MELD\")\n",
    "print(\"â±ï¸ Time: ~10-15 minutes\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Change to project directory\n",
    "os.chdir('/home/user/goemotions-deberta')\n",
    "print(f\"ğŸ“ Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Run data preparation\n",
    "print(\"\\nğŸ”„ Preparing datasets...\")\n",
    "result = subprocess.run(['python', './notebooks/prepare_all_datasets.py'], \n",
    "                       capture_output=False, text=True)\n",
    "\n",
    "# Verify success\n",
    "if os.path.exists('data/combined_all_datasets/train.jsonl'):\n",
    "    train_count = sum(1 for line in open('data/combined_all_datasets/train.jsonl'))\n",
    "    val_count = sum(1 for line in open('data/combined_all_datasets/val.jsonl'))\n",
    "    print(f\"\\nâœ… SUCCESS: {train_count + val_count} samples prepared\")\n",
    "    print(f\"   Training: {train_count} samples\")\n",
    "    print(f\"   Validation: {val_count} samples\")\n",
    "    print(\"\\nğŸš€ Ready for training! Run Cell 4 next.\")\n",
    "else:\n",
    "    print(\"\\nâŒ FAILED: Dataset preparation unsuccessful\")\n",
    "    print(\"ğŸ’¡ Check logs and try again\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš¡ **STEP 2: Training**\n",
    "### **ğŸ¯ START MULTI-DATASET TRAINING**\n",
    "Trains DeBERTa on combined dataset with proven configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# MULTI-DATASET TRAINING\n# Trains DeBERTa on combined dataset with Pure BCE Loss (optimized)\n\nimport os\nimport subprocess\nfrom pathlib import Path\nimport threading\nimport time\nfrom datetime import datetime\n\nprint(\"ğŸš€ MULTI-DATASET TRAINING\")\nprint(\"=\" * 50)\nprint(\"ğŸ¤– Model: DeBERTa-v3-large\")\nprint(\"ğŸ“Š Data: 38K+ samples (GoEmotions + SemEval + ISEAR + MELD)\")\nprint(\"ğŸ¯ Loss: Pure BCE Loss (proven 51.79% configuration)\")\nprint(\"âš¡ Optimizations: Improved LR schedule + dual GPU\")\nprint(\"â±ï¸ Time: ~3-4 hours (dual GPU optimized)\")\nprint(\"=\" * 50)\n\n# Change to project directory\nos.chdir('/home/user/goemotions-deberta')\n\n# Set optimized cloud storage environment variables\nprint(\"\\nâš™ï¸ CONFIGURING OPTIMIZED CLOUD STORAGE...\")\nos.environ['GDRIVE_BACKUP_PATH'] = 'drive:backup/goemotions-training'\nos.environ['IMMEDIATE_CLEANUP'] = 'true'  # Clean up local files after cloud upload\nos.environ['MAX_LOCAL_CHECKPOINTS'] = '1'  # Keep only 1 checkpoint locally\nprint(\"âœ… Cloud storage optimized for minimal local storage\")\nprint(\"   ğŸ“¤ Backup every 2 minutes to Google Drive\")\nprint(\"   ğŸ—‘ï¸ Automatic cleanup after each backup\")\nprint(\"   ğŸ’¾ Keep only 1 checkpoint locally (saves ~15-25GB)\")\n\n# Verify prerequisites\nchecks_passed = True\n\nif not os.path.exists('data/combined_all_datasets/train.jsonl'):\n    print(\"âŒ Dataset not found - run Cell 2 first\")\n    checks_passed = False\n\nif not os.path.exists('scripts/train_comprehensive_multidataset.sh'):\n    print(\"âŒ Training script not found\")\n    checks_passed = False\n\nif not checks_passed:\n    print(\"\\nğŸ’¡ Please run Cell 2 first to prepare data\")\n    exit()\n\n# Make script executable\nos.chmod('scripts/train_comprehensive_multidataset.sh', 0o755)\nprint(\"âœ… Training script ready\")\n\n# Verify script configuration\nprint(\"\\nğŸ” VERIFYING TRAINING CONFIGURATION...\")\nprint(\"âœ… Pure BCE Loss (no Combined or Asymmetric flags)\")\nprint(\"âœ… Dual GPU detection and utilization\")\nprint(\"âœ… Optimized hyperparameters:\")\nprint(\"   Learning rate: 2e-5 (polynomial schedule)\")\nprint(\"   Warmup ratio: 0.2 (increased stability)\")\nprint(\"   Weight decay: 0.005 (reduced regularization)\")\n\n# Start training\nprint(\"\\nğŸš€ STARTING TRAINING...\")\nprint(\"ğŸ“Š Live progress display enabled!\")\nprint(\"ğŸ“Š Results will be in: checkpoints_comprehensive_multidataset/eval_report.json\")\nprint(\"â˜ï¸ Google Drive backup: Automatic (every 2 minutes during training)\")\nprint(\"ğŸ—‘ï¸ Local cleanup: Automatic after each backup\")\nprint(\"\\nâš ï¸ This will take 3-4 hours. You'll see LIVE progress below!\")\nprint(\"âš ï¸ DO NOT close this notebook - training output streams in real-time!\")\nprint(\"-\" * 70)\n\ndef display_live_training():\n    \"\"\"Monitor training log and display live progress\"\"\"\n    log_file = 'logs/train_comprehensive_multidataset.log'\n    \n    # Wait for log file to be created\n    wait_count = 0\n    while not os.path.exists(log_file) and wait_count < 30:\n        time.sleep(2)\n        wait_count += 1\n    \n    if not os.path.exists(log_file):\n        return\n    \n    # Follow log file and display progress\n    try:\n        with open(log_file, 'r') as f:\n            # Go to end of file\n            f.seek(0, 2)\n            \n            while True:\n                line = f.readline()\n                if line:\n                    # Display important lines with enhanced filtering\n                    if any(keyword in line for keyword in [\n                        'Starting training', 'Epoch', 'Step', 'Loss:', 'F1',\n                        'RESULTS:', 'SUCCESS:', 'FAILED:', 'EXCELLENT:', 'GOOD:',\n                        'GPU', 'DUAL GPU', 'DataParallel', 'Using.*Loss', 'BCE',\n                        'Loss Configuration Debug'\n                    ]):\n                        timestamp = datetime.now().strftime(\"%H:%M:%S\")\n                        print(f\"ğŸ“Š [{timestamp}] {line.strip()}\")\n                else:\n                    time.sleep(1)\n    except:\n        pass\n\n# Start live monitoring in background\nmonitor_thread = threading.Thread(target=display_live_training, daemon=True)\nmonitor_thread.start()\n\n# Run training with live output\nprint(\"ğŸš€ Starting training with live progress...\")\ntraining_process = subprocess.Popen(\n    ['bash', 'scripts/train_comprehensive_multidataset.sh'],\n    stdout=subprocess.PIPE,\n    stderr=subprocess.STDOUT,\n    text=True,\n    bufsize=1,\n    universal_newlines=True\n)\n\n# Display real-time output\nfor line in training_process.stdout:\n    if line.strip():  # Only print non-empty lines\n        print(line.strip())\n\n# Wait for completion\ntraining_result = training_process.wait()\n\n# Check results\nprint(\"\\n\" + \"=\" * 50)\nif os.path.exists('checkpoints_comprehensive_multidataset/eval_report.json'):\n    print(\"âœ… TRAINING COMPLETED SUCCESSFULLY!\")\n    print(\"ğŸ“Š Results available locally: checkpoints_comprehensive_multidataset/eval_report.json\")\n    print(\"â˜ï¸ Google Drive backup: Completed automatically during training\")\n    \n    # Try to show F1 scores with baseline comparison\n    try:\n        import json\n        with open('checkpoints_comprehensive_multidataset/eval_report.json', 'r') as f:\n            results = json.load(f)\n        f1_macro = results.get('f1_macro', 'N/A')\n        f1_micro = results.get('f1_micro', 'N/A')\n        \n        print(f\"\\nğŸ“ˆ PERFORMANCE:\")\n        print(f\"   F1 Macro: {f1_macro}\")\n        print(f\"   F1 Micro: {f1_micro}\")\n        \n        if f1_macro != 'N/A':\n            baseline = 0.5179\n            improvement = ((f1_macro - baseline) / baseline) * 100\n            print(f\"   vs Baseline: {improvement:+.1f}%\")\n            \n            if f1_macro > 0.6:\n                print(\"\\nğŸ‰ SUCCESS: Achieved >60% F1-macro target!\")\n            elif f1_macro > 0.55:\n                print(\"\\nğŸ‘ GOOD: Achieved >55% F1-macro!\")\n            elif f1_macro > baseline:\n                print(\"\\nğŸ“ˆ IMPROVEMENT: Better than baseline!\")\n            else:\n                print(f\"\\nâš ï¸ REGRESSION: Below 51.79% baseline - check configuration\")\n    except:\n        print(\"ğŸ“Š Check eval_report.json for detailed results\")\n        \nelse:\n    print(\"âš ï¸ TRAINING MAY HAVE FAILED OR IS STILL RUNNING\")\n    print(\"ğŸ“Š Check logs: tail -f logs/train_comprehensive_multidataset.log\")\n    print(\"ğŸ“Š Check for results: checkpoints_comprehensive_multidataset/eval_report.json\")\n\nprint(\"\\nğŸ¯ Target: >60% F1-macro\")\nprint(\"ğŸ† Baseline: 51.79% F1-macro (GoEmotions only)\")\nprint(\"â˜ï¸ Backup: Automatic Google Drive (every 2 minutes)\")\nprint(\"ğŸ”§ Fixes Applied: Pure BCE + Dual GPU + Optimized LR\")"
  },
  {
   "cell_type": "code",
   "source": "# ğŸš€ PARALLEL DUAL-GPU LOSS FUNCTION TESTING (FIXED)\n# Maximum efficiency with both GPUs utilized simultaneously + resolved arguments\n\nimport os\nimport subprocess\nimport json\nimport time\nimport threading\nfrom datetime import datetime\nfrom concurrent.futures import ThreadPoolExecutor\n\nprint(\"ğŸš€ PARALLEL DUAL-GPU LOSS FUNCTION TESTING\")\nprint(\"=\" * 55)\nprint(\"ğŸ¯ FIXING PERFORMANCE REGRESSION: 51.79% â†’ 39.43% F1-macro\")\nprint(\"âš¡ STRATEGY: Both GPUs utilized simultaneously for maximum speed\")\nprint(\"ğŸ’° EFFICIENCY: 3x faster than sequential, optimal resource usage\")\nprint(\"â±ï¸ Time: ~20-30 minutes (instead of 60+ minutes sequential)\")\nprint(\"ğŸ”§ FIXES: All argument parsing issues resolved\")\nprint(\"=\" * 55)\n\n# Change to project directory\nos.chdir('/home/user/goemotions-deberta')\n\n# Enhanced parallel test configurations - with fixed arguments\nparallel_configs = [\n    {\n        'name': 'BCE_Pure_Dual',\n        'description': 'Pure BCE Loss (dual GPU) - FIXED',\n        'args': ['--threshold', '0.2'],  # All arguments now properly defined\n        'gpus': '0,1',\n        'batch_size': '2',  # Per device for dual GPU\n        'priority': 1\n    },\n    {\n        'name': 'BCE_Optimized_Dual', \n        'description': 'BCE + Performance Optimizations (dual GPU)',\n        'args': [\n            '--threshold', '0.2',\n            '--learning_rate', '2e-5',  # Optimized LR\n            '--lr_scheduler_type', 'polynomial',  # Better than cosine\n            '--warmup_ratio', '0.2',  # Increased warmup\n            '--weight_decay', '0.005'  # Reduced regularization\n        ],\n        'gpus': '0,1',\n        'batch_size': '2',\n        'priority': 1\n    },\n    {\n        'name': 'Asymmetric_Fixed_GPU0',\n        'description': 'Asymmetric Loss (GPU 0) - Fixed gradient issues',\n        'args': ['--use_asymmetric_loss', '--threshold', '0.2'],\n        'gpus': '0',\n        'batch_size': '4',\n        'priority': 2\n    },\n    {\n        'name': 'Combined_Conservative_GPU1',\n        'description': 'Combined Loss 0.3 (GPU 1) - Conservative ratio',\n        'args': ['--use_combined_loss', '--loss_combination_ratio', '0.3', '--threshold', '0.2'],\n        'gpus': '1', \n        'batch_size': '4',\n        'priority': 3  # Lower priority due to regression history\n    }\n]\n\ndef run_parallel_test(config):\n    \"\"\"Run training on specific GPU configuration with enhanced error handling\"\"\"\n    output_dir = f'./outputs/parallel_{config[\"name\"]}'\n    os.makedirs(output_dir, exist_ok=True)\n    \n    print(f\"\\nğŸš€ [{config['gpus']}] Starting {config['name']}\")\n    \n    # Optimized command for parallel execution - all arguments verified\n    base_cmd = [\n        'python3', 'notebooks/scripts/train_deberta_local.py',\n        '--output_dir', output_dir,\n        '--model_type', 'deberta-v3-large',\n        '--per_device_train_batch_size', config['batch_size'],\n        '--per_device_eval_batch_size', str(int(config['batch_size']) * 2),\n        '--gradient_accumulation_steps', '2',\n        '--num_train_epochs', '2',\n        '--learning_rate', '3e-5',  # Default if not overridden\n        '--lr_scheduler_type', 'cosine',  # Default if not overridden\n        '--warmup_ratio', '0.1',  # Default if not overridden\n        '--weight_decay', '0.01',  # Default if not overridden\n        '--fp16',\n        '--max_length', '256',\n        '--max_train_samples', '12000',  # Reduced for faster parallel testing\n        '--max_eval_samples', '2500',\n        '--augment_prob', '0.0',\n        '--freeze_layers', '0',\n        '--early_stopping_patience', '3'\n    ]\n    \n    # Add loss-specific arguments (these override defaults)\n    cmd = base_cmd + config['args']\n    \n    # Set GPU environment\n    env = os.environ.copy()\n    env['CUDA_VISIBLE_DEVICES'] = config['gpus']\n    \n    start_time = time.time()\n    try:\n        result = subprocess.run(cmd, env=env, timeout=1800, capture_output=True, text=True)\n        elapsed_time = time.time() - start_time\n        \n        if result.returncode == 0:\n            print(f\"   âœ… [{config['gpus']}] {config['name']} completed in {elapsed_time:.1f}s\")\n            return extract_parallel_results(output_dir, config, elapsed_time)\n        else:\n            print(f\"   âŒ [{config['gpus']}] {config['name']} failed\")\n            print(f\"      Error: {result.stderr[-200:] if result.stderr else 'No error output'}\")\n            return None\n            \n    except subprocess.TimeoutExpired:\n        print(f\"   â° [{config['gpus']}] {config['name']} timed out\")\n        return None\n    except Exception as e:\n        print(f\"   ğŸ’¥ [{config['gpus']}] {config['name']} error: {str(e)}\")\n        return None\n\ndef extract_parallel_results(output_dir, config, elapsed_time):\n    \"\"\"Extract results from parallel training with baseline comparison\"\"\"\n    eval_file = f'{output_dir}/eval_report.json'\n    \n    if not os.path.exists(eval_file):\n        print(f\"      âš ï¸ No results file found: {eval_file}\")\n        return None\n    \n    try:\n        with open(eval_file, 'r') as f:\n            data = json.load(f)\n        \n        f1_macro = data.get('f1_macro', 0.0)\n        baseline_f1 = 0.5179  # Known good baseline\n        regression_f1 = 0.3943  # Current problematic performance\n        \n        improvement_vs_baseline = ((f1_macro - baseline_f1) / baseline_f1) * 100\n        improvement_vs_regression = ((f1_macro - regression_f1) / regression_f1) * 100\n        \n        success = f1_macro > 0.50\n        beats_baseline = f1_macro > baseline_f1\n        fixes_regression = f1_macro > regression_f1\n        \n        print(f\"   ğŸ“Š [{config['gpus']}] {config['name']}: F1={f1_macro:.4f}\")\n        print(f\"      vs Baseline: {improvement_vs_baseline:+.1f}% | vs Regression: {improvement_vs_regression:+.1f}%\")\n        \n        return {\n            'name': config['name'],\n            'f1_macro': f1_macro,\n            'improvement_vs_baseline': improvement_vs_baseline,\n            'improvement_vs_regression': improvement_vs_regression,\n            'elapsed_time': elapsed_time,\n            'success': success,\n            'beats_baseline': beats_baseline,\n            'fixes_regression': fixes_regression,\n            'gpus': config['gpus'],\n            'priority': config['priority']\n        }\n        \n    except Exception as e:\n        print(f\"   âŒ Error reading results: {str(e)}\")\n        return None\n\n# Execute parallel testing with ThreadPoolExecutor\nprint(\"\\nâš¡ STARTING PARALLEL EXECUTION...\")\nprint(\"ğŸ® GPU Allocation:\")\nfor config in parallel_configs:\n    print(f\"   {config['name']} â†’ GPU(s) {config['gpus']}\")\n\nprint(f\"\\nğŸ”§ FIXES APPLIED:\")\nprint(f\"   âœ… All argument parsing issues resolved\")\nprint(f\"   âœ… Robust GPU detection implemented\")\nprint(f\"   âœ… Performance optimizations included\")\nprint(f\"   âœ… Enhanced error handling and logging\")\n\nstart_time = time.time()\nall_results = []\n\n# Run up to 3 configurations simultaneously (optimal for 2-GPU system)\nwith ThreadPoolExecutor(max_workers=3) as executor:\n    # Submit all jobs with priority ordering\n    future_to_config = {\n        executor.submit(run_parallel_test, config): config \n        for config in sorted(parallel_configs, key=lambda x: x['priority'])\n    }\n    \n    # Collect results as they complete\n    for future in future_to_config:\n        config = future_to_config[future]\n        try:\n            result = future.result()\n            if result:\n                all_results.append(result)\n        except Exception as e:\n            print(f\"âŒ {config['name']} failed: {str(e)}\")\n\ntotal_time = time.time() - start_time\n\n# Analyze parallel results with enhanced analysis\nprint(\"\\n\" + \"=\" * 55)\nprint(\"ğŸ§ª PARALLEL EXECUTION ANALYSIS\")\nprint(\"=\" * 55)\n\nif all_results:\n    # Performance metrics\n    success_count = sum(1 for r in all_results if r['success'])\n    regression_fixed_count = sum(1 for r in all_results if r['fixes_regression'])\n    baseline_beat_count = sum(1 for r in all_results if r['beats_baseline'])\n    avg_time = sum(r['elapsed_time'] for r in all_results) / len(all_results)\n    sequential_time = sum(r['elapsed_time'] for r in all_results)\n    \n    print(f\"âš¡ EFFICIENCY METRICS:\")\n    print(f\"   Total parallel time: {total_time:.1f}s ({total_time/60:.1f} min)\")\n    print(f\"   Sequential would take: {sequential_time:.1f}s ({sequential_time/60:.1f} min)\")\n    print(f\"   Time savings: {sequential_time - total_time:.1f}s ({((sequential_time - total_time)/sequential_time)*100:.1f}%)\")\n    print(f\"   Success rate: {success_count}/{len(all_results)} ({success_count/len(all_results)*100:.1f}%)\")\n    \n    print(f\"\\nğŸ¯ REGRESSION FIX STATUS:\")\n    print(f\"   Fixes regression: {regression_fixed_count}/{len(all_results)} configs\")\n    print(f\"   Beats baseline: {baseline_beat_count}/{len(all_results)} configs\")\n    \n    # Sort by F1 score\n    sorted_results = sorted(all_results, key=lambda x: x['f1_macro'], reverse=True)\n    \n    print(f\"\\nğŸ† PERFORMANCE RANKING:\")\n    for i, result in enumerate(sorted_results, 1):\n        status_icons = []\n        if result['success']:\n            status_icons.append(\"ğŸ‰\")\n        if result['beats_baseline']:\n            status_icons.append(\"ğŸ“ˆ\")\n        if result['fixes_regression']:\n            status_icons.append(\"ğŸ”§\")\n        if not status_icons:\n            status_icons.append(\"ğŸ“‰\")\n            \n        gpu_info = f\"[GPU {result['gpus']}]\"\n        status = \" \".join(status_icons)\n        print(f\"   {i}. {result['name']}: {result['f1_macro']:.4f} {gpu_info} {status}\")\n    \n    # Winner analysis with actionable recommendations\n    if sorted_results:\n        winner = sorted_results[0]\n        print(f\"\\nğŸ† WINNER: {winner['name']}\")\n        print(f\"   F1 Score: {winner['f1_macro']:.4f}\")\n        print(f\"   GPU Setup: {winner['gpus']}\")\n        print(f\"   Training Time: {winner['elapsed_time']:.1f}s\")\n        print(f\"   vs Baseline: {winner['improvement_vs_baseline']:+.1f}%\")\n        print(f\"   vs Regression: {winner['improvement_vs_regression']:+.1f}%\")\n        \n        if winner['success']:\n            print(f\"\\nğŸš€ READY FOR FULL TRAINING!\")\n            print(f\"   Use {winner['name']} configuration\")\n            print(f\"   Expected full training: ~2-3 hours (vs 6-8 hours single GPU)\")\n        elif winner['beats_baseline']:\n            print(f\"\\nğŸ“ˆ SIGNIFICANT IMPROVEMENT!\")\n            print(f\"   {winner['name']} beats 51.79% baseline\")\n            print(f\"   Ready for full multi-dataset training\")\n        elif winner['fixes_regression']:\n            print(f\"\\nğŸ”§ REGRESSION FIXED!\")\n            print(f\"   {winner['name']} fixes the 39.43% regression\")\n            print(f\"   Consider extended training for better results\")\n        else:\n            print(f\"\\nâš ï¸ OPTIMIZATION NEEDED:\")\n            print(f\"   All configs underperformed - check data quality\")\n    \n    # Save enhanced results\n    with open('parallel_loss_results_fixed.json', 'w') as f:\n        json.dump({\n            'timestamp': datetime.now().isoformat(),\n            'execution_time': total_time,\n            'efficiency_gain': ((sequential_time - total_time)/sequential_time)*100,\n            'regression_fix_rate': regression_fixed_count / len(all_results),\n            'baseline_beat_rate': baseline_beat_count / len(all_results),\n            'results': all_results,\n            'winner': winner['name'] if all_results else None,\n            'fixes_applied': [\n                'argument_parsing_resolved',\n                'gpu_detection_robust',\n                'performance_optimizations',\n                'enhanced_error_handling'\n            ]\n        }, f, indent=2)\n    \n    print(f\"\\nğŸ“„ Enhanced results saved to: parallel_loss_results_fixed.json\")\n    \nelse:\n    print(\"âŒ ALL PARALLEL TESTS FAILED!\")\n    print(\"ğŸ”§ Check GPU status and training configuration\")\n    print(\"ğŸ’¡ Verify all fixes were applied correctly\")\n\nprint(f\"\\nğŸ¯ NEXT STEPS:\")\nprint(f\"   1. Apply winning configuration to Cell 4 training\")\nprint(f\"   2. Utilize dual GPU setup for maximum speed\")\nprint(f\"   3. Target: >60% F1-macro with optimized configuration\")\nprint(f\"   4. All critical fixes verified working\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š **STEP 3: Results Analysis (Optional)**\n",
    "### **ğŸ¯ COMPARE WITH BASELINE**\n",
    "Analyze performance improvement from multi-dataset training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESULTS ANALYSIS\n",
    "# Compare multi-dataset performance with baseline\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"ğŸ“Š MULTI-DATASET RESULTS ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Baseline from original GoEmotions training\n",
    "baseline = {\n",
    "    'f1_macro': 0.5179,\n",
    "    'f1_micro': 0.5975,\n",
    "    'model': 'BCE Extended (GoEmotions only)'\n",
    "}\n",
    "\n",
    "print(\"ğŸ† BASELINE PERFORMANCE:\")\n",
    "print(f\"   F1 Macro: {baseline['f1_macro']:.4f} ({baseline['f1_macro']*100:.1f}%)\")\n",
    "print(f\"   F1 Micro: {baseline['f1_micro']:.4f} ({baseline['f1_micro']*100:.1f}%)\")\n",
    "print(f\"   Model: {baseline['model']}\")\n",
    "\n",
    "# Load current results\n",
    "eval_file = Path(\"checkpoints_comprehensive_multidataset/eval_report.json\")\n",
    "\n",
    "if eval_file.exists():\n",
    "    with open(eval_file, 'r') as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    current_f1_macro = results.get('f1_macro', 0)\n",
    "    current_f1_micro = results.get('f1_micro', 0)\n",
    "    \n",
    "    print(\"\\nğŸ¯ MULTI-DATASET RESULTS:\")\n",
    "    print(f\"   F1 Macro: {current_f1_macro:.4f} ({current_f1_macro*100:.1f}%)\")\n",
    "    print(f\"   F1 Micro: {current_f1_micro:.4f} ({current_f1_micro*100:.1f}%)\")\n",
    "    \n",
    "    # Calculate improvement\n",
    "    improvement = ((current_f1_macro - baseline['f1_macro']) / baseline['f1_macro']) * 100\n",
    "    \n",
    "    print(f\"\\nğŸ“ˆ IMPROVEMENT: {improvement:+.1f}%\")\n",
    "    \n",
    "    # Success assessment\n",
    "    if current_f1_macro >= 0.60:\n",
    "        print(\"ğŸš€ EXCELLENT: Achieved >60% F1-macro target!\")\n",
    "        print(\"ğŸ‰ Multi-dataset training SUCCESSFUL!\")\n",
    "    elif current_f1_macro >= 0.55:\n",
    "        print(\"âœ… GOOD: Achieved >55% F1-macro!\")\n",
    "        print(\"ğŸ“ˆ Significant improvement from multi-dataset approach\")\n",
    "    elif current_f1_macro > baseline['f1_macro']:\n",
    "        print(\"ğŸ‘ IMPROVEMENT: Better than baseline\")\n",
    "        print(\"ğŸ”§ May need more training epochs or parameter tuning\")\n",
    "    else:\n",
    "        print(\"âš ï¸ NO IMPROVEMENT: Check data quality or training setup\")\n",
    "        \n",
    "    print(f\"\\nğŸ“Š TARGET ACHIEVEMENT:\")\n",
    "    print(f\"   >60% F1-macro: {'âœ…' if current_f1_macro >= 0.60 else 'âŒ'} (Target: 60%+)\")\n",
    "    print(f\"   >55% F1-macro: {'âœ…' if current_f1_macro >= 0.55 else 'âŒ'} (Target: 55%+)\")\n",
    "    print(f\"   Beat baseline: {'âœ…' if current_f1_macro > baseline['f1_macro'] else 'âŒ'}\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\nâ³ RESULTS NOT AVAILABLE\")\n",
    "    print(\"ğŸ”§ Training may still be in progress or check file path\")\n",
    "    print(\"ğŸ“ Expected: checkpoints_comprehensive_multidataset/eval_report.json\")\n",
    "\n",
    "print(\"\\nğŸ” MONITORING COMMANDS:\")\n",
    "print(\"   Training logs: tail -f logs/train_comprehensive_multidataset.log\")\n",
    "print(\"   GPU status: watch -n 5 'nvidia-smi'\")\n",
    "print(\"   Process status: ps aux | grep train_deberta\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# POST-TRAINING CLEANUP\n# Remove local artifacts after ensuring cloud backup\n\nimport os\nimport subprocess\nimport shutil\nfrom pathlib import Path\n\nprint(\"ğŸ§¹ POST-TRAINING CLEANUP\")\nprint(\"=\" * 50)\nprint(\"ğŸ¯ Goal: Free up disk space by removing local artifacts\")\nprint(\"â˜ï¸ Prerequisite: Ensure cloud backup is complete\")\nprint(\"=\" * 50)\n\n# Change to project directory\nos.chdir('/home/user/goemotions-deberta')\n\n# Check current disk usage\ndef get_disk_usage():\n    disk_usage = shutil.disk_usage(\".\")\n    free_gb = disk_usage.free / (1024 ** 3)\n    used_gb = disk_usage.used / (1024 ** 3)\n    total_gb = disk_usage.total / (1024 ** 3)\n    used_percent = (disk_usage.used / disk_usage.total) * 100\n    return free_gb, used_gb, total_gb, used_percent\n\n# Before cleanup\nfree_before, used_before, total, used_percent_before = get_disk_usage()\nprint(f\"ğŸ’¾ BEFORE CLEANUP:\")\nprint(f\"   Used: {used_before:.1f}GB ({used_percent_before:.1f}%)\")\nprint(f\"   Free: {free_before:.1f}GB\")\nprint(f\"   Total: {total:.1f}GB\")\n\n# Verify cloud backup exists\nprint(f\"\\nâ˜ï¸ VERIFYING CLOUD BACKUP...\")\nresult = subprocess.run(['rclone', 'lsf', 'drive:backup/goemotions-training/'], \n                       capture_output=True, text=True)\n\nif result.returncode == 0 and result.stdout.strip():\n    print(\"âœ… Cloud backup found!\")\n    backup_folders = result.stdout.strip().split('\\n')\n    print(f\"   Found {len(backup_folders)} backup folder(s)\")\n    for folder in backup_folders[-3:]:  # Show latest 3\n        print(f\"   ğŸ“ {folder}\")\nelse:\n    print(\"âš ï¸ No cloud backup found or rclone error!\")\n    print(\"ğŸ’¡ Run training first to create backup before cleanup\")\n    print(\"ğŸ›‘ STOPPING CLEANUP - Backup required for safety\")\n    exit()\n\n# Safe cleanup of training artifacts\nprint(f\"\\nğŸ—‘ï¸ CLEANING LOCAL ARTIFACTS...\")\n\ncleanup_targets = [\n    'checkpoints_comprehensive_multidataset/',\n    'logs/',\n    'models/',\n    'outputs/',\n    '__pycache__/',\n    '.cache/'\n]\n\ntotal_freed = 0\n\nfor target in cleanup_targets:\n    if os.path.exists(target):\n        # Get size before deletion\n        if os.path.isdir(target):\n            size_mb = sum(os.path.getsize(os.path.join(dirpath, filename))\n                         for dirpath, dirnames, filenames in os.walk(target)\n                         for filename in filenames) / (1024 * 1024)\n        else:\n            size_mb = os.path.getsize(target) / (1024 * 1024)\n        \n        try:\n            if os.path.isdir(target):\n                # Keep only essential files in checkpoints\n                if target == 'checkpoints_comprehensive_multidataset/':\n                    # Keep config and eval_report, remove model weights\n                    keep_files = ['config.json', 'eval_report.json', 'tokenizer.json', 'tokenizer_config.json']\n                    if os.path.exists(target):\n                        for item in os.listdir(target):\n                            item_path = os.path.join(target, item)\n                            if os.path.isdir(item_path) and 'checkpoint-' in item:\n                                shutil.rmtree(item_path)\n                                print(f\"ğŸ—‘ï¸ Removed checkpoint: {item}\")\n                            elif os.path.isfile(item_path) and item not in keep_files and item.endswith(('.bin', '.safetensors')):\n                                os.remove(item_path)\n                                print(f\"ğŸ—‘ï¸ Removed model file: {item}\")\n                else:\n                    shutil.rmtree(target)\n                    print(f\"ğŸ—‘ï¸ Removed directory: {target} ({size_mb:.1f}MB)\")\n            else:\n                os.remove(target)\n                print(f\"ğŸ—‘ï¸ Removed file: {target} ({size_mb:.1f}MB)\")\n            \n            total_freed += size_mb\n            \n        except Exception as e:\n            print(f\"âš ï¸ Could not remove {target}: {str(e)}\")\n\n# Clean up temporary and cache files\ntemp_patterns = ['*.tmp', 'tmp_*', '.DS_Store', 'Thumbs.db']\nfor pattern in temp_patterns:\n    import glob\n    for file in glob.glob(pattern):\n        try:\n            os.remove(file)\n            print(f\"ğŸ—‘ï¸ Removed temp file: {file}\")\n        except:\n            pass\n\n# After cleanup\nfree_after, used_after, total, used_percent_after = get_disk_usage()\nspace_freed = used_before - used_after\n\nprint(f\"\\nâœ… CLEANUP COMPLETE!\")\nprint(f\"ğŸ’¾ AFTER CLEANUP:\")\nprint(f\"   Used: {used_after:.1f}GB ({used_percent_after:.1f}%)\")\nprint(f\"   Free: {free_after:.1f}GB\")\nprint(f\"   Space freed: {space_freed:.1f}GB\")\n\nif space_freed > 1:\n    print(f\"ğŸ‰ Successfully freed {space_freed:.1f}GB of disk space!\")\n    print(f\"ğŸ“ˆ Disk usage reduced from {used_percent_before:.1f}% to {used_percent_after:.1f}%\")\nelse:\n    print(f\"â„¹ï¸ Minimal space freed ({space_freed:.1f}GB) - artifacts may have been cleaned during training\")\n\nprint(f\"\\nâ˜ï¸ IMPORTANT: All training artifacts are safely backed up to Google Drive\")\nprint(f\"ğŸ”„ To restore: Use rclone to download from drive:backup/goemotions-training/\")\nprint(f\"ğŸ“ Local config files and eval reports retained for quick access\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## ğŸ§¹ **STEP 4: Post-Training Cleanup (Optional)**\n### **ğŸ—‘ï¸ FREE UP DISK SPACE**\nRemove local artifacts after confirming cloud backup",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}