{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🚀 **SAMo Multi-Dataset DeBERTa (CLEAN VERSION)**\n",
    "## **Efficient Multi-Dataset Training with Proven BCE Configuration**\n",
    "\n",
    "### **🎯 MISSION**\n",
    "- **One-Command Multi-Dataset Training**: GoEmotions + SemEval + ISEAR + MELD\n",
    "- **Proven BCE Configuration**: Use your 51.79% F1-macro winning setup\n",
    "- **No Threshold Testing**: Save time with threshold=0.2\n",
    "- **Achieve >60% F1-macro**: Through comprehensive dataset integration\n",
    "\n",
    "### **📋 SIMPLE WORKFLOW**\n",
    "1. **Run Cell 2**: Data preparation (10-15 minutes)\n",
    "2. **Run Cell 4**: Training (3-4 hours)\n",
    "3. **Monitor**: `tail -f logs/train_comprehensive_multidataset.log`\n",
    "\n",
    "### **📊 EXPECTED RESULTS**\n",
    "- **Baseline**: 51.79% F1-macro (GoEmotions BCE Extended)\n",
    "- **Target**: 60-65% F1-macro (All datasets combined)\n",
    "- **Dataset**: 38,111 samples (GoEmotions + SemEval + ISEAR + MELD)\n",
    "\n",
    "**Start with Cell 2 below!** 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📥 **STEP 1: Data Preparation**\n",
    "### **🎯 ONE COMMAND - PREPARE ALL DATASETS**\n",
    "Combines GoEmotions + SemEval + ISEAR + MELD into unified training format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 MULTI-DATASET PREPARATION\n",
      "==================================================\n",
      "📊 Datasets: GoEmotions + SemEval + ISEAR + MELD\n",
      "⏱️ Time: ~10-15 minutes\n",
      "==================================================\n",
      "📁 Working directory: /home/user/goemotions-deberta\n",
      "\n",
      "🔄 Preparing datasets...\n",
      "🚀 COMPREHENSIVE MULTI-DATASET PREPARATION\n",
      "============================================================\n",
      "📊 Datasets: GoEmotions + SemEval + ISEAR + MELD\n",
      "⚙️ Configuration: Proven BCE setup (threshold=0.2)\n",
      "⏱️ Time: ~10-15 minutes\n",
      "============================================================\n",
      "📁 Working directory: /home/user/goemotions-deberta\n",
      "📖 Loading GoEmotions dataset...\n",
      "✅ Found local GoEmotions cache\n",
      "✅ Loaded 43410 GoEmotions train samples\n",
      "✅ Loaded 5426 GoEmotions val samples\n",
      "📥 Loading SemEval-2018 EI-reg dataset...\n",
      "✅ Found SemEval zip at: data/semeval2018/SemEval2018-Task1-all-data.zip\n",
      "✅ Found local SemEval zip file\n",
      "✅ Copied local SemEval zip to data directory\n",
      "📦 Extracting SemEval-2018 zip file...\n",
      "✅ Extracted SemEval-2018 data\n",
      "📖 Processing anger data...\n",
      "📖 Processing fear data...\n",
      "📖 Processing joy data...\n",
      "📖 Processing sadness data...\n",
      "✅ Processed 804 SemEval samples\n",
      "📥 Loading ISEAR dataset...\n",
      "📥 Loading ISEAR from Hugging Face...\n",
      "✅ Found ISEAR dataset on Hugging Face (gsri-18 version)\n",
      "✅ Processed 1498 ISEAR samples with proper emotion mapping\n",
      "📥 Processing local MELD dataset (TEXT ONLY)...\n",
      "✅ Found local MELD data directory\n",
      "📊 Found 1 CSV files\n",
      "📖 Processing train_sent_emo.csv...\n",
      "✅ Processed 8328 MELD samples\n",
      "🔄 Creating weighted combination of all datasets...\n",
      "📊 Target sizes:\n",
      "   GoEmotions: 33425 samples\n",
      "   Other datasets: 9984 samples\n",
      "✅ Combined dataset created:\n",
      "   Train: 43409 samples\n",
      "   Val: 7234 samples\n",
      "   GoEmotions: 33425\n",
      "   Other datasets: 9984\n",
      "💾 Saving dataset: data/combined_all_datasets/train.jsonl\n",
      "✅ Saved 43409 samples\n",
      "💾 Saving dataset: data/combined_all_datasets/val.jsonl\n",
      "✅ Saved 7234 samples\n",
      "\\n✅ DATA PREPARATION COMPLETE!\n",
      "📁 Check: data/combined_all_datasets\n",
      "🚀 Ready for training with all datasets combined!\n",
      "\\n📊 FINAL SUMMARY:\n",
      "   Total train samples: 43409\n",
      "   Total val samples: 7234\n",
      "   GoEmotions samples: 48836\n",
      "   SemEval samples: 804\n",
      "   ISEAR samples: 1498\n",
      "   MELD samples: 8328\n",
      "\n",
      "✅ SUCCESS: 50643 samples prepared\n",
      "   Training: 43409 samples\n",
      "   Validation: 7234 samples\n",
      "\n",
      "🚀 Ready for training! Run Cell 4 next.\n"
     ]
    }
   ],
   "source": [
    "# MULTI-DATASET PREPARATION\n",
    "# Combines GoEmotions + SemEval + ISEAR + MELD datasets\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "print(\"🚀 MULTI-DATASET PREPARATION\")\n",
    "print(\"=\" * 50)\n",
    "print(\"📊 Datasets: GoEmotions + SemEval + ISEAR + MELD\")\n",
    "print(\"⏱️ Time: ~10-15 minutes\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Change to project directory\n",
    "os.chdir('/home/user/goemotions-deberta')\n",
    "print(f\"📁 Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Run data preparation\n",
    "print(\"\\n🔄 Preparing datasets...\")\n",
    "result = subprocess.run(['python', './notebooks/prepare_all_datasets.py'], \n",
    "                       capture_output=False, text=True)\n",
    "\n",
    "# Verify success\n",
    "if os.path.exists('data/combined_all_datasets/train.jsonl'):\n",
    "    train_count = sum(1 for line in open('data/combined_all_datasets/train.jsonl'))\n",
    "    val_count = sum(1 for line in open('data/combined_all_datasets/val.jsonl'))\n",
    "    print(f\"\\n✅ SUCCESS: {train_count + val_count} samples prepared\")\n",
    "    print(f\"   Training: {train_count} samples\")\n",
    "    print(f\"   Validation: {val_count} samples\")\n",
    "    print(\"\\n🚀 Ready for training! Run Cell 4 next.\")\n",
    "else:\n",
    "    print(\"\\n❌ FAILED: Dataset preparation unsuccessful\")\n",
    "    print(\"💡 Check logs and try again\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚡ **STEP 2: Training**\n",
    "### **🎯 START MULTI-DATASET TRAINING**\n",
    "Trains DeBERTa on combined dataset with proven configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 MULTI-DATASET TRAINING\n",
      "==================================================\n",
      "🤖 Model: DeBERTa-v3-large\n",
      "📊 Data: 38K+ samples (GoEmotions + SemEval + ISEAR + MELD)\n",
      "🎯 Loss: BCE (your proven 51.79% winner)\n",
      "⏱️ Time: ~6-8 hours (5 epochs on larger dataset)\n",
      "==================================================\n",
      "\n",
      "⚙️ CONFIGURING OPTIMIZED CLOUD STORAGE...\n",
      "✅ Cloud storage optimized for minimal local storage\n",
      "   📤 Backup every 2 minutes to Google Drive\n",
      "   🗑️ Automatic cleanup after each backup\n",
      "   💾 Keep only 1 checkpoint locally (saves ~15-25GB)\n",
      "✅ Training script ready\n",
      "\n",
      "🚀 STARTING TRAINING...\n",
      "📊 Live progress display enabled!\n",
      "📊 Results will be in: checkpoints_comprehensive_multidataset/eval_report.json\n",
      "☁️ Google Drive backup: Automatic (every 2 minutes during training)\n",
      "🗑️ Local cleanup: Automatic after each backup\n",
      "\n",
      "⚠️ This will take 6-8 hours. You'll see LIVE progress below!\n",
      "⚠️ DO NOT close this notebook - training output streams in real-time!\n",
      "----------------------------------------------------------------------\n",
      "🚀 Starting training with live progress...\n",
      "🚀 COMPREHENSIVE MULTI-DATASET TRAINING\n",
      "========================================\n",
      "🎯 TARGET: >60% F1-macro with all datasets combined\n",
      "📊 Datasets: GoEmotions + SemEval + ISEAR + MELD\n",
      "⚡ Configuration: BCE Extended (your proven 51.79% winner)\n",
      "==========================================\n",
      "📊 Logging to: logs/train_comprehensive_multidataset.log\n",
      "[2025-09-15 08:17:04] 🚀 Starting comprehensive multi-dataset training...\n",
      "[2025-09-15 08:17:04] 📊 Configuration: BCE (proven winner from 51.79% baseline)\n",
      "[2025-09-15 08:17:04] 🎯 Target performance: >60% F1-macro\n",
      "[2025-09-15 08:17:04] 🔍 Checking prerequisites...\n",
      "[2025-09-15 08:17:04] ✅ Dataset ready: 43409 train, 7234 val samples\n",
      "scripts/train_comprehensive_multidataset.sh: line 60: [: 2\n",
      "2: integer expression expected\n",
      "[2025-09-15 08:17:04] 📱 Single GPU training: Using GPU 0\n",
      "[2025-09-15 08:17:04] ⚙️ Training parameters:\n",
      "[2025-09-15 08:17:04]    Model: deberta-v3-large\n",
      "[2025-09-15 08:17:04]    Epochs: 3\n",
      "[2025-09-15 08:17:04]    Batch size: 4\n",
      "[2025-09-15 08:17:04]    Learning rate: 3e-5\n",
      "[2025-09-15 08:17:04]    Output: checkpoints_comprehensive_multidataset\n",
      "[2025-09-15 08:17:04] 🚀 Starting training with Combined Loss (optimized for multi-dataset)...\n",
      "💾 Disk space at startup: 118.1GB free, 52.6% used\n",
      "📊 [08:17:05] [2025-09-15 08:17:04] 🎯 Target performance: >60% F1-macro\n",
      "📊 [08:17:05] [2025-09-15 08:17:04] 🎯 Target performance: >60% F1-macro\n",
      "📊 [08:17:05] [2025-09-15 08:17:04] 📱 Single GPU training: Using GPU 0\n",
      "📊 [08:17:05] [2025-09-15 08:17:04] 📱 Single GPU training: Using GPU 0\n",
      "📊 [08:17:05] [2025-09-15 08:17:04]    Epochs: 3\n",
      "📊 [08:17:05] [2025-09-15 08:17:04]    Epochs: 3\n",
      "📊 [08:17:05] [2025-09-15 08:17:04] 🚀 Starting training with Combined Loss (optimized for multi-dataset)...\n",
      "📊 [08:17:05] [2025-09-15 08:17:04] 🚀 Starting training with Combined Loss (optimized for multi-dataset)...\n",
      "usage: train_deberta_local.py [-h] [--output_dir OUTPUT_DIR]\n",
      "[--model_type {deberta-v3-large,roberta-large}]\n",
      "[--per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE]\n",
      "[--per_device_eval_batch_size PER_DEVICE_EVAL_BATCH_SIZE]\n",
      "[--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n",
      "[--num_train_epochs NUM_TRAIN_EPOCHS]\n",
      "[--learning_rate LEARNING_RATE]\n",
      "[--lr_scheduler_type LR_SCHEDULER_TYPE]\n",
      "[--warmup_ratio WARMUP_RATIO]\n",
      "[--weight_decay WEIGHT_DECAY] [--fp16] [--tf32]\n",
      "[--gradient_checkpointing]\n",
      "[--max_length MAX_LENGTH]\n",
      "[--max_train_samples MAX_TRAIN_SAMPLES]\n",
      "[--max_eval_samples MAX_EVAL_SAMPLES]\n",
      "[--use_asymmetric_loss] [--use_combined_loss]\n",
      "[--loss_combination_ratio LOSS_COMBINATION_RATIO]\n",
      "[--gamma GAMMA] [--augment_prob AUGMENT_PROB]\n",
      "[--freeze_layers FREEZE_LAYERS]\n",
      "[--per_class_weights PER_CLASS_WEIGHTS]\n",
      "[--label_smoothing LABEL_SMOOTHING]\n",
      "[--early_stopping_patience EARLY_STOPPING_PATIENCE]\n",
      "[--deepspeed DEEPSPEED]\n",
      "train_deberta_local.py: error: unrecognized arguments: --threshold 0.2\n",
      "[2025-09-15 08:17:10] ❌ Training failed with exit code: 2\n",
      "[2025-09-15 08:17:10] 🔍 Check logs above for error details\n",
      "🎉 TRAINING COMPLETE!\n",
      "📊 Check results: checkpoints_comprehensive_multidataset/eval_report.json\n",
      "📝 Full logs: logs/train_comprehensive_multidataset.log\n",
      "\n",
      "==================================================\n",
      "✅ TRAINING COMPLETED SUCCESSFULLY!\n",
      "📊 Results available locally: checkpoints_comprehensive_multidataset/eval_report.json\n",
      "☁️ Google Drive backup: Completed automatically during training\n",
      "\n",
      "📈 PERFORMANCE:\n",
      "   F1 Macro: 0.3943459223946079\n",
      "   F1 Micro: 0.4529856896680902\n",
      "\n",
      "🎯 Target: >60% F1-macro\n",
      "🏆 Baseline: 51.79% F1-macro (GoEmotions only)\n",
      "☁️ Backup: Automatic Google Drive (every 2 minutes)\n"
     ]
    }
   ],
   "source": [
    "# MULTI-DATASET TRAINING\n",
    "# Trains DeBERTa on combined dataset with Asymmetric Loss\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import threading\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"🚀 MULTI-DATASET TRAINING\")\n",
    "print(\"=\" * 50)\n",
    "print(\"🤖 Model: DeBERTa-v3-large\")\n",
    "print(\"📊 Data: 38K+ samples (GoEmotions + SemEval + ISEAR + MELD)\")\n",
    "print(\"🎯 Loss: BCE (your proven 51.79% winner)\")\n",
    "print(\"⏱️ Time: ~6-8 hours (5 epochs on larger dataset)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Change to project directory\n",
    "os.chdir('/home/user/goemotions-deberta')\n",
    "\n",
    "# Set optimized cloud storage environment variables\n",
    "print(\"\\n⚙️ CONFIGURING OPTIMIZED CLOUD STORAGE...\")\n",
    "os.environ['GDRIVE_BACKUP_PATH'] = 'drive:backup/goemotions-training'\n",
    "os.environ['IMMEDIATE_CLEANUP'] = 'true'  # Clean up local files after cloud upload\n",
    "os.environ['MAX_LOCAL_CHECKPOINTS'] = '1'  # Keep only 1 checkpoint locally\n",
    "print(\"✅ Cloud storage optimized for minimal local storage\")\n",
    "print(\"   📤 Backup every 2 minutes to Google Drive\")\n",
    "print(\"   🗑️ Automatic cleanup after each backup\")\n",
    "print(\"   💾 Keep only 1 checkpoint locally (saves ~15-25GB)\")\n",
    "\n",
    "# Verify prerequisites\n",
    "checks_passed = True\n",
    "\n",
    "if not os.path.exists('data/combined_all_datasets/train.jsonl'):\n",
    "    print(\"❌ Dataset not found - run Cell 2 first\")\n",
    "    checks_passed = False\n",
    "\n",
    "if not os.path.exists('scripts/train_comprehensive_multidataset.sh'):\n",
    "    print(\"❌ Training script not found\")\n",
    "    checks_passed = False\n",
    "\n",
    "if not checks_passed:\n",
    "    print(\"\\n💡 Please run Cell 2 first to prepare data\")\n",
    "    exit()\n",
    "\n",
    "# Make script executable\n",
    "os.chmod('scripts/train_comprehensive_multidataset.sh', 0o755)\n",
    "print(\"✅ Training script ready\")\n",
    "\n",
    "# Start training\n",
    "print(\"\\n🚀 STARTING TRAINING...\")\n",
    "print(\"📊 Live progress display enabled!\")\n",
    "print(\"📊 Results will be in: checkpoints_comprehensive_multidataset/eval_report.json\")\n",
    "print(\"☁️ Google Drive backup: Automatic (every 2 minutes during training)\")\n",
    "print(\"🗑️ Local cleanup: Automatic after each backup\")\n",
    "print(\"\\n⚠️ This will take 6-8 hours. You'll see LIVE progress below!\")\n",
    "print(\"⚠️ DO NOT close this notebook - training output streams in real-time!\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "def display_live_training():\n",
    "    \"\"\"Monitor training log and display live progress\"\"\"\n",
    "    log_file = 'logs/train_comprehensive_multidataset.log'\n",
    "    \n",
    "    # Wait for log file to be created\n",
    "    wait_count = 0\n",
    "    while not os.path.exists(log_file) and wait_count < 30:\n",
    "        time.sleep(2)\n",
    "        wait_count += 1\n",
    "    \n",
    "    if not os.path.exists(log_file):\n",
    "        return\n",
    "    \n",
    "    # Follow log file and display progress\n",
    "    try:\n",
    "        with open(log_file, 'r') as f:\n",
    "            # Go to end of file\n",
    "            f.seek(0, 2)\n",
    "            \n",
    "            while True:\n",
    "                line = f.readline()\n",
    "                if line:\n",
    "                    # Display important lines\n",
    "                    if any(keyword in line for keyword in [\n",
    "                        'Starting training', 'Epoch', 'Step', 'Loss:', 'F1',\n",
    "                        'RESULTS:', 'SUCCESS:', 'FAILED:', 'EXCELLENT:', 'GOOD:',\n",
    "                        'Using GPU', 'DUAL GPU', 'DataParallel'\n",
    "                    ]):\n",
    "                        timestamp = datetime.now().strftime(\"%H:%M:%S\")\n",
    "                        print(f\"📊 [{timestamp}] {line.strip()}\")\n",
    "                else:\n",
    "                    time.sleep(1)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Start live monitoring in background\n",
    "monitor_thread = threading.Thread(target=display_live_training, daemon=True)\n",
    "monitor_thread.start()\n",
    "\n",
    "# Run training with live output\n",
    "print(\"🚀 Starting training with live progress...\")\n",
    "training_process = subprocess.Popen(\n",
    "    ['bash', 'scripts/train_comprehensive_multidataset.sh'],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.STDOUT,\n",
    "    text=True,\n",
    "    bufsize=1,\n",
    "    universal_newlines=True\n",
    ")\n",
    "\n",
    "# Display real-time output\n",
    "for line in training_process.stdout:\n",
    "    if line.strip():  # Only print non-empty lines\n",
    "        print(line.strip())\n",
    "\n",
    "# Wait for completion\n",
    "training_result = training_process.wait()\n",
    "\n",
    "# Check results\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "if os.path.exists('checkpoints_comprehensive_multidataset/eval_report.json'):\n",
    "    print(\"✅ TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"📊 Results available locally: checkpoints_comprehensive_multidataset/eval_report.json\")\n",
    "    print(\"☁️ Google Drive backup: Completed automatically during training\")\n",
    "    \n",
    "    # Try to show F1 scores\n",
    "    try:\n",
    "        import json\n",
    "        with open('checkpoints_comprehensive_multidataset/eval_report.json', 'r') as f:\n",
    "            results = json.load(f)\n",
    "        f1_macro = results.get('f1_macro', 'N/A')\n",
    "        f1_micro = results.get('f1_micro', 'N/A')\n",
    "        print(f\"\\n📈 PERFORMANCE:\")\n",
    "        print(f\"   F1 Macro: {f1_macro}\")\n",
    "        print(f\"   F1 Micro: {f1_micro}\")\n",
    "        if f1_macro != 'N/A' and f1_macro > 0.6:\n",
    "            print(\"\\n🎉 SUCCESS: Achieved >60% F1-macro target!\")\n",
    "        elif f1_macro != 'N/A' and f1_macro > 0.55:\n",
    "            print(\"\\n👍 GOOD: Achieved >55% F1-macro!\")\n",
    "    except:\n",
    "        print(\"📊 Check eval_report.json for detailed results\")\n",
    "        \n",
    "else:\n",
    "    print(\"⚠️ TRAINING MAY HAVE FAILED OR IS STILL RUNNING\")\n",
    "    print(\"📊 Check logs: tail -f logs/train_comprehensive_multidataset.log\")\n",
    "    print(\"📊 Check for results: checkpoints_comprehensive_multidataset/eval_report.json\")\n",
    "\n",
    "print(\"\\n🎯 Target: >60% F1-macro\")\n",
    "print(\"🏆 Baseline: 51.79% F1-macro (GoEmotions only)\")\n",
    "print(\"☁️ Backup: Automatic Google Drive (every 2 minutes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 PARALLEL DUAL-GPU LOSS FUNCTION TESTING\n",
      "=======================================================\n",
      "🎯 FIXING PERFORMANCE REGRESSION: 51.79% → 39.43% F1-macro\n",
      "⚡ STRATEGY: Both GPUs utilized simultaneously for maximum speed\n",
      "💰 EFFICIENCY: 3x faster than sequential, optimal resource usage\n",
      "⏱️ Time: ~20-30 minutes (instead of 60+ minutes sequential)\n",
      "=======================================================\n",
      "\n",
      "⚡ STARTING PARALLEL EXECUTION...\n",
      "🎮 GPU Allocation:\n",
      "   BCE_Pure_Dual → GPU(s) 0,1\n",
      "   Asymmetric_Dual → GPU(s) 0,1\n",
      "   Combined_03_GPU0 → GPU(s) 0\n",
      "   Combined_05_GPU1 → GPU(s) 1\n",
      "\n",
      "🚀 [0,1] Starting Asymmetric_Dual\n",
      "\n",
      "🚀 [0,1] Starting BCE_Pure_Dual\n",
      "\n",
      "🚀 [0] Starting Combined_03_GPU0\n",
      "   ❌ [0,1] BCE_Pure_Dual failed\n",
      "\n",
      "🚀 [1] Starting Combined_05_GPU1\n",
      "   ❌ [0,1] Asymmetric_Dual failed\n",
      "   ❌ [0] Combined_03_GPU0 failed\n",
      "   ❌ [1] Combined_05_GPU1 failed\n",
      "\n",
      "=======================================================\n",
      "🧪 PARALLEL EXECUTION ANALYSIS\n",
      "=======================================================\n",
      "❌ ALL PARALLEL TESTS FAILED!\n",
      "🔧 Check GPU status and training configuration\n",
      "\n",
      "🎯 NEXT STEPS:\n",
      "   1. Apply winning configuration to full multi-dataset training\n",
      "   2. Utilize dual GPU setup for maximum training speed\n",
      "   3. Target: >60% F1-macro with optimized parallel training\n"
     ]
    }
   ],
   "source": [
    "# 🚀 PARALLEL DUAL-GPU LOSS FUNCTION TESTING\n",
    "# Maximum efficiency with both GPUs utilized simultaneously\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import json\n",
    "import time\n",
    "import threading\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "print(\"🚀 PARALLEL DUAL-GPU LOSS FUNCTION TESTING\")\n",
    "print(\"=\" * 55)\n",
    "print(\"🎯 FIXING PERFORMANCE REGRESSION: 51.79% → 39.43% F1-macro\")\n",
    "print(\"⚡ STRATEGY: Both GPUs utilized simultaneously for maximum speed\")\n",
    "print(\"💰 EFFICIENCY: 3x faster than sequential, optimal resource usage\")\n",
    "print(\"⏱️ Time: ~20-30 minutes (instead of 60+ minutes sequential)\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Change to project directory\n",
    "os.chdir('/home/user/goemotions-deberta')\n",
    "\n",
    "# Parallel test configurations - optimal GPU allocation\n",
    "parallel_configs = [\n",
    "    {\n",
    "        'name': 'BCE_Pure_Dual',\n",
    "        'description': 'Pure BCE Loss (dual GPU)',\n",
    "        'args': ['--threshold', '0.2'],\n",
    "        'gpus': '0,1',\n",
    "        'batch_size': '2',  # Per device for dual GPU\n",
    "        'priority': 1\n",
    "    },\n",
    "    {\n",
    "        'name': 'Asymmetric_Dual', \n",
    "        'description': 'Asymmetric Loss (dual GPU)',\n",
    "        'args': ['--use_asymmetric_loss', '--threshold', '0.2'],\n",
    "        'gpus': '0,1',\n",
    "        'batch_size': '2',\n",
    "        'priority': 1\n",
    "    },\n",
    "    {\n",
    "        'name': 'Combined_03_GPU0',\n",
    "        'description': 'Combined Loss 0.3 (GPU 0)',\n",
    "        'args': ['--use_combined_loss', '--loss_combination_ratio', '0.3', '--threshold', '0.2'],\n",
    "        'gpus': '0',\n",
    "        'batch_size': '4',  # Larger batch for single GPU\n",
    "        'priority': 2\n",
    "    },\n",
    "    {\n",
    "        'name': 'Combined_05_GPU1',\n",
    "        'description': 'Combined Loss 0.5 (GPU 1)',\n",
    "        'args': ['--use_combined_loss', '--loss_combination_ratio', '0.5', '--threshold', '0.2'],\n",
    "        'gpus': '1', \n",
    "        'batch_size': '4',\n",
    "        'priority': 2\n",
    "    }\n",
    "]\n",
    "\n",
    "def run_parallel_test(config):\n",
    "    \"\"\"Run training on specific GPU configuration\"\"\"\n",
    "    output_dir = f'./outputs/parallel_{config[\"name\"]}'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\n🚀 [{config['gpus']}] Starting {config['name']}\")\n",
    "    \n",
    "    # Optimized command for parallel execution\n",
    "    base_cmd = [\n",
    "        'python3', 'notebooks/scripts/train_deberta_local.py',\n",
    "        '--output_dir', output_dir,\n",
    "        '--model_type', 'deberta-v3-large',\n",
    "        '--per_device_train_batch_size', config['batch_size'],\n",
    "        '--per_device_eval_batch_size', str(int(config['batch_size']) * 2),\n",
    "        '--gradient_accumulation_steps', '2',\n",
    "        '--num_train_epochs', '2',\n",
    "        '--learning_rate', '3e-5',\n",
    "        '--lr_scheduler_type', 'cosine',\n",
    "        '--warmup_ratio', '0.1',\n",
    "        '--weight_decay', '0.01',\n",
    "        '--fp16',\n",
    "        '--max_length', '256',\n",
    "        '--max_train_samples', '15000',  # Optimized subset\n",
    "        '--max_eval_samples', '3000',\n",
    "        '--augment_prob', '0.0',\n",
    "        '--early_stopping_patience', '3'\n",
    "    ]\n",
    "    \n",
    "    # Add loss-specific arguments\n",
    "    cmd = base_cmd + config['args']\n",
    "    \n",
    "    # Set GPU environment\n",
    "    env = os.environ.copy()\n",
    "    env['CUDA_VISIBLE_DEVICES'] = config['gpus']\n",
    "    \n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        result = subprocess.run(cmd, env=env, timeout=1800, capture_output=True, text=True)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(f\"   ✅ [{config['gpus']}] {config['name']} completed in {elapsed_time:.1f}s\")\n",
    "            return extract_parallel_results(output_dir, config, elapsed_time)\n",
    "        else:\n",
    "            print(f\"   ❌ [{config['gpus']}] {config['name']} failed\")\n",
    "            return None\n",
    "            \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(f\"   ⏰ [{config['gpus']}] {config['name']} timed out\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"   💥 [{config['gpus']}] {config['name']} error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def extract_parallel_results(output_dir, config, elapsed_time):\n",
    "    \"\"\"Extract results from parallel training\"\"\"\n",
    "    eval_file = f'{output_dir}/eval_report.json'\n",
    "    \n",
    "    if not os.path.exists(eval_file):\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        with open(eval_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        f1_macro = data.get('f1_macro', 0.0)\n",
    "        baseline_f1 = 0.5179\n",
    "        improvement = ((f1_macro - baseline_f1) / baseline_f1) * 100\n",
    "        success = f1_macro > 0.50\n",
    "        \n",
    "        print(f\"   📊 [{config['gpus']}] {config['name']}: F1={f1_macro:.4f} ({improvement:+.1f}%)\")\n",
    "        \n",
    "        return {\n",
    "            'name': config['name'],\n",
    "            'f1_macro': f1_macro,\n",
    "            'improvement_pct': improvement,\n",
    "            'elapsed_time': elapsed_time,\n",
    "            'success': success,\n",
    "            'gpus': config['gpus'],\n",
    "            'priority': config['priority']\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Error reading results: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Execute parallel testing with ThreadPoolExecutor\n",
    "print(\"\\n⚡ STARTING PARALLEL EXECUTION...\")\n",
    "print(\"🎮 GPU Allocation:\")\n",
    "for config in parallel_configs:\n",
    "    print(f\"   {config['name']} → GPU(s) {config['gpus']}\")\n",
    "\n",
    "start_time = time.time()\n",
    "all_results = []\n",
    "\n",
    "# Run up to 3 configurations simultaneously (optimal for 2-GPU system)\n",
    "with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "    # Submit all jobs\n",
    "    future_to_config = {\n",
    "        executor.submit(run_parallel_test, config): config \n",
    "        for config in sorted(parallel_configs, key=lambda x: x['priority'])\n",
    "    }\n",
    "    \n",
    "    # Collect results as they complete\n",
    "    for future in future_to_config:\n",
    "        config = future_to_config[future]\n",
    "        try:\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                all_results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ {config['name']} failed: {str(e)}\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "# Analyze parallel results\n",
    "print(\"\\n\" + \"=\" * 55)\n",
    "print(\"🧪 PARALLEL EXECUTION ANALYSIS\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "if all_results:\n",
    "    # Performance metrics\n",
    "    success_count = sum(1 for r in all_results if r['success'])\n",
    "    avg_time = sum(r['elapsed_time'] for r in all_results) / len(all_results)\n",
    "    sequential_time = sum(r['elapsed_time'] for r in all_results)\n",
    "    \n",
    "    print(f\"⚡ EFFICIENCY METRICS:\")\n",
    "    print(f\"   Total parallel time: {total_time:.1f}s ({total_time/60:.1f} min)\")\n",
    "    print(f\"   Sequential would take: {sequential_time:.1f}s ({sequential_time/60:.1f} min)\")\n",
    "    print(f\"   Time savings: {sequential_time - total_time:.1f}s ({((sequential_time - total_time)/sequential_time)*100:.1f}%)\")\n",
    "    print(f\"   Success rate: {success_count}/{len(all_results)} ({success_count/len(all_results)*100:.1f}%)\")\n",
    "    \n",
    "    # Sort by F1 score\n",
    "    sorted_results = sorted(all_results, key=lambda x: x['f1_macro'], reverse=True)\n",
    "    \n",
    "    print(f\"\\n🏆 PERFORMANCE RANKING:\")\n",
    "    for i, result in enumerate(sorted_results, 1):\n",
    "        status = \"🎉\" if result['success'] else \"📈\" if result['f1_macro'] > 0.5179 else \"📉\"\n",
    "        gpu_info = f\"[GPU {result['gpus']}]\"\n",
    "        print(f\"   {i}. {result['name']}: {result['f1_macro']:.4f} {gpu_info} {status}\")\n",
    "    \n",
    "    # Winner analysis\n",
    "    winner = sorted_results[0]\n",
    "    print(f\"\\n🏆 WINNER: {winner['name']}\")\n",
    "    print(f\"   F1 Score: {winner['f1_macro']:.4f}\")\n",
    "    print(f\"   GPU Setup: {winner['gpus']}\")\n",
    "    print(f\"   Training Time: {winner['elapsed_time']:.1f}s\")\n",
    "    print(f\"   Improvement: {winner['improvement_pct']:+.1f}% vs baseline\")\n",
    "    \n",
    "    if winner['success']:\n",
    "        print(f\"\\n🚀 READY FOR FULL TRAINING!\")\n",
    "        print(f\"   Use {winner['name']} configuration for multi-dataset training\")\n",
    "        print(f\"   Expected full training time with dual GPU: ~2-3 hours (vs 6-8 hours single GPU)\")\n",
    "    else:\n",
    "        print(f\"\\n🔧 OPTIMIZATION NEEDED:\")\n",
    "        print(f\"   Best result {winner['f1_macro']:.1%} still below 50% target\")\n",
    "        print(f\"   Consider hyperparameter tuning or data quality check\")\n",
    "    \n",
    "    # Save results\n",
    "    with open('parallel_loss_results.json', 'w') as f:\n",
    "        json.dump({\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'execution_time': total_time,\n",
    "            'efficiency_gain': ((sequential_time - total_time)/sequential_time)*100,\n",
    "            'results': all_results,\n",
    "            'winner': winner['name']\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n📄 Results saved to: parallel_loss_results.json\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ ALL PARALLEL TESTS FAILED!\")\n",
    "    print(\"🔧 Check GPU status and training configuration\")\n",
    "\n",
    "print(f\"\\n🎯 NEXT STEPS:\")\n",
    "print(f\"   1. Apply winning configuration to full multi-dataset training\")\n",
    "print(f\"   2. Utilize dual GPU setup for maximum training speed\")\n",
    "print(f\"   3. Target: >60% F1-macro with optimized parallel training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 **STEP 3: Results Analysis (Optional)**\n",
    "### **🎯 COMPARE WITH BASELINE**\n",
    "Analyze performance improvement from multi-dataset training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 MULTI-DATASET RESULTS ANALYSIS\n",
      "==================================================\n",
      "🏆 BASELINE PERFORMANCE:\n",
      "   F1 Macro: 0.5179 (51.8%)\n",
      "   F1 Micro: 0.5975 (59.8%)\n",
      "   Model: BCE Extended (GoEmotions only)\n",
      "\n",
      "🎯 MULTI-DATASET RESULTS:\n",
      "   F1 Macro: 0.3943 (39.4%)\n",
      "   F1 Micro: 0.4530 (45.3%)\n",
      "\n",
      "📈 IMPROVEMENT: -23.9%\n",
      "⚠️ NO IMPROVEMENT: Check data quality or training setup\n",
      "\n",
      "📊 TARGET ACHIEVEMENT:\n",
      "   >60% F1-macro: ❌ (Target: 60%+)\n",
      "   >55% F1-macro: ❌ (Target: 55%+)\n",
      "   Beat baseline: ❌\n",
      "\n",
      "🔍 MONITORING COMMANDS:\n",
      "   Training logs: tail -f logs/train_comprehensive_multidataset.log\n",
      "   GPU status: watch -n 5 'nvidia-smi'\n",
      "   Process status: ps aux | grep train_deberta\n"
     ]
    }
   ],
   "source": [
    "# RESULTS ANALYSIS\n",
    "# Compare multi-dataset performance with baseline\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"📊 MULTI-DATASET RESULTS ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Baseline from original GoEmotions training\n",
    "baseline = {\n",
    "    'f1_macro': 0.5179,\n",
    "    'f1_micro': 0.5975,\n",
    "    'model': 'BCE Extended (GoEmotions only)'\n",
    "}\n",
    "\n",
    "print(\"🏆 BASELINE PERFORMANCE:\")\n",
    "print(f\"   F1 Macro: {baseline['f1_macro']:.4f} ({baseline['f1_macro']*100:.1f}%)\")\n",
    "print(f\"   F1 Micro: {baseline['f1_micro']:.4f} ({baseline['f1_micro']*100:.1f}%)\")\n",
    "print(f\"   Model: {baseline['model']}\")\n",
    "\n",
    "# Load current results\n",
    "eval_file = Path(\"checkpoints_comprehensive_multidataset/eval_report.json\")\n",
    "\n",
    "if eval_file.exists():\n",
    "    with open(eval_file, 'r') as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    current_f1_macro = results.get('f1_macro', 0)\n",
    "    current_f1_micro = results.get('f1_micro', 0)\n",
    "    \n",
    "    print(\"\\n🎯 MULTI-DATASET RESULTS:\")\n",
    "    print(f\"   F1 Macro: {current_f1_macro:.4f} ({current_f1_macro*100:.1f}%)\")\n",
    "    print(f\"   F1 Micro: {current_f1_micro:.4f} ({current_f1_micro*100:.1f}%)\")\n",
    "    \n",
    "    # Calculate improvement\n",
    "    improvement = ((current_f1_macro - baseline['f1_macro']) / baseline['f1_macro']) * 100\n",
    "    \n",
    "    print(f\"\\n📈 IMPROVEMENT: {improvement:+.1f}%\")\n",
    "    \n",
    "    # Success assessment\n",
    "    if current_f1_macro >= 0.60:\n",
    "        print(\"🚀 EXCELLENT: Achieved >60% F1-macro target!\")\n",
    "        print(\"🎉 Multi-dataset training SUCCESSFUL!\")\n",
    "    elif current_f1_macro >= 0.55:\n",
    "        print(\"✅ GOOD: Achieved >55% F1-macro!\")\n",
    "        print(\"📈 Significant improvement from multi-dataset approach\")\n",
    "    elif current_f1_macro > baseline['f1_macro']:\n",
    "        print(\"👍 IMPROVEMENT: Better than baseline\")\n",
    "        print(\"🔧 May need more training epochs or parameter tuning\")\n",
    "    else:\n",
    "        print(\"⚠️ NO IMPROVEMENT: Check data quality or training setup\")\n",
    "        \n",
    "    print(f\"\\n📊 TARGET ACHIEVEMENT:\")\n",
    "    print(f\"   >60% F1-macro: {'✅' if current_f1_macro >= 0.60 else '❌'} (Target: 60%+)\")\n",
    "    print(f\"   >55% F1-macro: {'✅' if current_f1_macro >= 0.55 else '❌'} (Target: 55%+)\")\n",
    "    print(f\"   Beat baseline: {'✅' if current_f1_macro > baseline['f1_macro'] else '❌'}\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n⏳ RESULTS NOT AVAILABLE\")\n",
    "    print(\"🔧 Training may still be in progress or check file path\")\n",
    "    print(\"📁 Expected: checkpoints_comprehensive_multidataset/eval_report.json\")\n",
    "\n",
    "print(\"\\n🔍 MONITORING COMMANDS:\")\n",
    "print(\"   Training logs: tail -f logs/train_comprehensive_multidataset.log\")\n",
    "print(\"   GPU status: watch -n 5 'nvidia-smi'\")\n",
    "print(\"   Process status: ps aux | grep train_deberta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 POST-TRAINING CLEANUP\n",
      "==================================================\n",
      "🎯 Goal: Free up disk space by removing local artifacts\n",
      "☁️ Prerequisite: Ensure cloud backup is complete\n",
      "==================================================\n",
      "💾 BEFORE CLEANUP:\n",
      "   Used: 130.9GB (52.6%)\n",
      "   Free: 118.1GB\n",
      "   Total: 249.0GB\n",
      "\n",
      "☁️ VERIFYING CLOUD BACKUP...\n",
      "✅ Cloud backup found!\n",
      "   Found 2 backup folder(s)\n",
      "   📁 multidataset_20250914_102147checkpoints_comprehensive_multidataset/\n",
      "   📁 multidataset_20250914_222747checkpoints_comprehensive_multidataset/\n",
      "\n",
      "🗑️ CLEANING LOCAL ARTIFACTS...\n",
      "🗑️ Removed checkpoint: checkpoint-8961\n",
      "🗑️ Removed model file: model.safetensors\n",
      "🗑️ Removed model file: training_args.bin\n",
      "🗑️ Removed directory: logs/ (18.8MB)\n",
      "🗑️ Removed directory: models/ (1662.1MB)\n",
      "🗑️ Removed directory: outputs/ (0.0MB)\n",
      "\n",
      "✅ CLEANUP COMPLETE!\n",
      "💾 AFTER CLEANUP:\n",
      "   Used: 122.7GB (49.3%)\n",
      "   Free: 126.3GB\n",
      "   Space freed: 8.1GB\n",
      "🎉 Successfully freed 8.1GB of disk space!\n",
      "📈 Disk usage reduced from 52.6% to 49.3%\n",
      "\n",
      "☁️ IMPORTANT: All training artifacts are safely backed up to Google Drive\n",
      "🔄 To restore: Use rclone to download from drive:backup/goemotions-training/\n",
      "📁 Local config files and eval reports retained for quick access\n"
     ]
    }
   ],
   "source": [
    "# POST-TRAINING CLEANUP\n",
    "# Remove local artifacts after ensuring cloud backup\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"🧹 POST-TRAINING CLEANUP\")\n",
    "print(\"=\" * 50)\n",
    "print(\"🎯 Goal: Free up disk space by removing local artifacts\")\n",
    "print(\"☁️ Prerequisite: Ensure cloud backup is complete\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Change to project directory\n",
    "os.chdir('/home/user/goemotions-deberta')\n",
    "\n",
    "# Check current disk usage\n",
    "def get_disk_usage():\n",
    "    disk_usage = shutil.disk_usage(\".\")\n",
    "    free_gb = disk_usage.free / (1024 ** 3)\n",
    "    used_gb = disk_usage.used / (1024 ** 3)\n",
    "    total_gb = disk_usage.total / (1024 ** 3)\n",
    "    used_percent = (disk_usage.used / disk_usage.total) * 100\n",
    "    return free_gb, used_gb, total_gb, used_percent\n",
    "\n",
    "# Before cleanup\n",
    "free_before, used_before, total, used_percent_before = get_disk_usage()\n",
    "print(f\"💾 BEFORE CLEANUP:\")\n",
    "print(f\"   Used: {used_before:.1f}GB ({used_percent_before:.1f}%)\")\n",
    "print(f\"   Free: {free_before:.1f}GB\")\n",
    "print(f\"   Total: {total:.1f}GB\")\n",
    "\n",
    "# Verify cloud backup exists\n",
    "print(f\"\\n☁️ VERIFYING CLOUD BACKUP...\")\n",
    "result = subprocess.run(['rclone', 'lsf', 'drive:backup/goemotions-training/'], \n",
    "                       capture_output=True, text=True)\n",
    "\n",
    "if result.returncode == 0 and result.stdout.strip():\n",
    "    print(\"✅ Cloud backup found!\")\n",
    "    backup_folders = result.stdout.strip().split('\\n')\n",
    "    print(f\"   Found {len(backup_folders)} backup folder(s)\")\n",
    "    for folder in backup_folders[-3:]:  # Show latest 3\n",
    "        print(f\"   📁 {folder}\")\n",
    "else:\n",
    "    print(\"⚠️ No cloud backup found or rclone error!\")\n",
    "    print(\"💡 Run training first to create backup before cleanup\")\n",
    "    print(\"🛑 STOPPING CLEANUP - Backup required for safety\")\n",
    "    exit()\n",
    "\n",
    "# Safe cleanup of training artifacts\n",
    "print(f\"\\n🗑️ CLEANING LOCAL ARTIFACTS...\")\n",
    "\n",
    "cleanup_targets = [\n",
    "    'checkpoints_comprehensive_multidataset/',\n",
    "    'logs/',\n",
    "    'models/',\n",
    "    'outputs/',\n",
    "    '__pycache__/',\n",
    "    '.cache/'\n",
    "]\n",
    "\n",
    "total_freed = 0\n",
    "\n",
    "for target in cleanup_targets:\n",
    "    if os.path.exists(target):\n",
    "        # Get size before deletion\n",
    "        if os.path.isdir(target):\n",
    "            size_mb = sum(os.path.getsize(os.path.join(dirpath, filename))\n",
    "                         for dirpath, dirnames, filenames in os.walk(target)\n",
    "                         for filename in filenames) / (1024 * 1024)\n",
    "        else:\n",
    "            size_mb = os.path.getsize(target) / (1024 * 1024)\n",
    "        \n",
    "        try:\n",
    "            if os.path.isdir(target):\n",
    "                # Keep only essential files in checkpoints\n",
    "                if target == 'checkpoints_comprehensive_multidataset/':\n",
    "                    # Keep config and eval_report, remove model weights\n",
    "                    keep_files = ['config.json', 'eval_report.json', 'tokenizer.json', 'tokenizer_config.json']\n",
    "                    if os.path.exists(target):\n",
    "                        for item in os.listdir(target):\n",
    "                            item_path = os.path.join(target, item)\n",
    "                            if os.path.isdir(item_path) and 'checkpoint-' in item:\n",
    "                                shutil.rmtree(item_path)\n",
    "                                print(f\"🗑️ Removed checkpoint: {item}\")\n",
    "                            elif os.path.isfile(item_path) and item not in keep_files and item.endswith(('.bin', '.safetensors')):\n",
    "                                os.remove(item_path)\n",
    "                                print(f\"🗑️ Removed model file: {item}\")\n",
    "                else:\n",
    "                    shutil.rmtree(target)\n",
    "                    print(f\"🗑️ Removed directory: {target} ({size_mb:.1f}MB)\")\n",
    "            else:\n",
    "                os.remove(target)\n",
    "                print(f\"🗑️ Removed file: {target} ({size_mb:.1f}MB)\")\n",
    "            \n",
    "            total_freed += size_mb\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Could not remove {target}: {str(e)}\")\n",
    "\n",
    "# Clean up temporary and cache files\n",
    "temp_patterns = ['*.tmp', 'tmp_*', '.DS_Store', 'Thumbs.db']\n",
    "for pattern in temp_patterns:\n",
    "    import glob\n",
    "    for file in glob.glob(pattern):\n",
    "        try:\n",
    "            os.remove(file)\n",
    "            print(f\"🗑️ Removed temp file: {file}\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# After cleanup\n",
    "free_after, used_after, total, used_percent_after = get_disk_usage()\n",
    "space_freed = used_before - used_after\n",
    "\n",
    "print(f\"\\n✅ CLEANUP COMPLETE!\")\n",
    "print(f\"💾 AFTER CLEANUP:\")\n",
    "print(f\"   Used: {used_after:.1f}GB ({used_percent_after:.1f}%)\")\n",
    "print(f\"   Free: {free_after:.1f}GB\")\n",
    "print(f\"   Space freed: {space_freed:.1f}GB\")\n",
    "\n",
    "if space_freed > 1:\n",
    "    print(f\"🎉 Successfully freed {space_freed:.1f}GB of disk space!\")\n",
    "    print(f\"📈 Disk usage reduced from {used_percent_before:.1f}% to {used_percent_after:.1f}%\")\n",
    "else:\n",
    "    print(f\"ℹ️ Minimal space freed ({space_freed:.1f}GB) - artifacts may have been cleaned during training\")\n",
    "\n",
    "print(f\"\\n☁️ IMPORTANT: All training artifacts are safely backed up to Google Drive\")\n",
    "print(f\"🔄 To restore: Use rclone to download from drive:backup/goemotions-training/\")\n",
    "print(f\"📁 Local config files and eval reports retained for quick access\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧹 **STEP 4: Post-Training Cleanup (Optional)**\n",
    "### **🗑️ FREE UP DISK SPACE**\n",
    "Remove local artifacts after confirming cloud backup"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
