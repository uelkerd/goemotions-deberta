{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🚀 **SAMo Multi-Dataset DeBERTa (CLEAN VERSION)**\n",
    "## **Efficient Multi-Dataset Training with Proven BCE Configuration**\n",
    "\n",
    "### **🎯 MISSION**\n",
    "- **One-Command Multi-Dataset Training**: GoEmotions + SemEval + ISEAR + MELD\n",
    "- **Proven BCE Configuration**: Use your 51.79% F1-macro winning setup\n",
    "- **No Threshold Testing**: Save time with threshold=0.2\n",
    "- **Achieve >60% F1-macro**: Through comprehensive dataset integration\n",
    "\n",
    "### **📋 SIMPLE WORKFLOW**\n",
    "1. **Run Cell 2**: Data preparation (10-15 minutes)\n",
    "2. **Run Cell 4**: Training (3-4 hours)\n",
    "3. **Monitor**: `tail -f logs/train_comprehensive_multidataset.log`\n",
    "\n",
    "### **📊 EXPECTED RESULTS**\n",
    "- **Baseline**: 51.79% F1-macro (GoEmotions BCE Extended)\n",
    "- **Target**: 60-65% F1-macro (All datasets combined)\n",
    "- **Dataset**: 38,111 samples (GoEmotions + SemEval + ISEAR + MELD)\n",
    "\n",
    "**Start with Cell 2 below!** 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📥 **STEP 1: Data Preparation**\n",
    "### **🎯 ONE COMMAND - PREPARE ALL DATASETS**\n",
    "Combines GoEmotions + SemEval + ISEAR + MELD into unified training format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 MULTI-DATASET PREPARATION\n",
      "==================================================\n",
      "📊 Datasets: GoEmotions + SemEval + ISEAR + MELD\n",
      "⏱️ Time: ~10-15 minutes\n",
      "==================================================\n",
      "📁 Working directory: /home/user/goemotions-deberta\n",
      "\n",
      "🔄 Preparing datasets...\n",
      "🚀 COMPREHENSIVE MULTI-DATASET PREPARATION\n",
      "============================================================\n",
      "📊 Datasets: GoEmotions + SemEval + ISEAR + MELD\n",
      "⚙️ Configuration: Proven BCE setup (threshold=0.2)\n",
      "⏱️ Time: ~10-15 minutes\n",
      "============================================================\n",
      "📁 Working directory: /home/user/goemotions-deberta\n",
      "📖 Loading GoEmotions dataset...\n",
      "✅ Loaded 43410 GoEmotions train samples\n",
      "✅ Loaded 5426 GoEmotions val samples\n",
      "📥 Processing local SemEval-2018 EI-reg dataset...\n",
      "✅ Found local SemEval zip file\n",
      "✅ Copied local SemEval zip to data directory\n",
      "📦 Extracting SemEval-2018 zip file...\n",
      "✅ Extracted SemEval-2018 data\n",
      "📖 Processing anger data...\n",
      "📖 Processing fear data...\n",
      "📖 Processing joy data...\n",
      "📖 Processing sadness data...\n",
      "✅ Processed 802 SemEval samples\n",
      "📥 Downloading ISEAR dataset...\n",
      "📥 Loading ISEAR from Hugging Face...\n",
      "✅ Processed 7516 ISEAR samples\n",
      "📥 Processing local MELD dataset (TEXT ONLY)...\n",
      "✅ Found local MELD data directory\n",
      "📊 Found 3 CSV files\n",
      "📖 Processing train_sent_emo.csv...\n",
      "📖 Processing test_sent_emo.csv...\n",
      "📖 Processing dev_sent_emo.csv...\n",
      "✅ Processed 13708 MELD samples\n",
      "🔄 Creating weighted combination of all datasets...\n",
      "📊 Target sizes:\n",
      "   GoEmotions: 29301 samples\n",
      "   Other datasets: 8810 samples\n",
      "✅ Combined dataset created:\n",
      "   Train: 30489 samples\n",
      "   Val: 7622 samples\n",
      "   GoEmotions: 29301\n",
      "   Other datasets: 8810\n",
      "💾 Saving dataset: data/combined_all_datasets/train.jsonl\n",
      "✅ Saved 30489 samples\n",
      "💾 Saving dataset: data/combined_all_datasets/val.jsonl\n",
      "✅ Saved 7622 samples\n",
      "\n",
      "✅ DATA PREPARATION COMPLETE!\n",
      "📁 Check: data/combined_all_datasets\n",
      "🚀 Ready for training with all datasets combined!\n",
      "\n",
      "📊 FINAL SUMMARY:\n",
      "   Total train samples: 30489\n",
      "   Total val samples: 7622\n",
      "   GoEmotions samples: 48836\n",
      "   SemEval samples: 802\n",
      "   ISEAR samples: 7516\n",
      "   MELD samples: 13708\n",
      "\n",
      "✅ SUCCESS: 38111 samples prepared\n",
      "   Training: 30489 samples\n",
      "   Validation: 7622 samples\n",
      "\n",
      "🚀 Ready for training! Run Cell 4 next.\n"
     ]
    }
   ],
   "source": [
    "# MULTI-DATASET PREPARATION\n",
    "# Combines GoEmotions + SemEval + ISEAR + MELD datasets\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "print(\"🚀 MULTI-DATASET PREPARATION\")\n",
    "print(\"=\" * 50)\n",
    "print(\"📊 Datasets: GoEmotions + SemEval + ISEAR + MELD\")\n",
    "print(\"⏱️ Time: ~10-15 minutes\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Change to project directory\n",
    "os.chdir('/home/user/goemotions-deberta')\n",
    "print(f\"📁 Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Run data preparation\n",
    "print(\"\\n🔄 Preparing datasets...\")\n",
    "result = subprocess.run(['python', './notebooks/prepare_all_datasets.py'], \n",
    "                       capture_output=False, text=True)\n",
    "\n",
    "# Verify success\n",
    "if os.path.exists('data/combined_all_datasets/train.jsonl'):\n",
    "    train_count = sum(1 for line in open('data/combined_all_datasets/train.jsonl'))\n",
    "    val_count = sum(1 for line in open('data/combined_all_datasets/val.jsonl'))\n",
    "    print(f\"\\n✅ SUCCESS: {train_count + val_count} samples prepared\")\n",
    "    print(f\"   Training: {train_count} samples\")\n",
    "    print(f\"   Validation: {val_count} samples\")\n",
    "    print(\"\\n🚀 Ready for training! Run Cell 4 next.\")\n",
    "else:\n",
    "    print(\"\\n❌ FAILED: Dataset preparation unsuccessful\")\n",
    "    print(\"💡 Check logs and try again\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚡ **STEP 2: Training**\n",
    "### **🎯 START MULTI-DATASET TRAINING**\n",
    "Trains DeBERTa on combined dataset with proven configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# MULTI-DATASET TRAINING\n# Trains DeBERTa on combined dataset with Asymmetric Loss\n\nimport os\nimport subprocess\nfrom pathlib import Path\n\nprint(\"🚀 MULTI-DATASET TRAINING\")\nprint(\"=\" * 50)\nprint(\"🤖 Model: DeBERTa-v3-large\")\nprint(\"📊 Data: 38K+ samples (GoEmotions + SemEval + ISEAR + MELD)\")\nprint(\"🎯 Loss: BCE (your proven 51.79% winner)\")\nprint(\"⏱️ Time: ~6-8 hours (5 epochs on larger dataset)\")\nprint(\"=\" * 50)\n\n# Change to project directory\nos.chdir('/home/user/goemotions-deberta')\n\n# Set optimized cloud storage environment variables\nprint(\"\\n⚙️ CONFIGURING OPTIMIZED CLOUD STORAGE...\")\nos.environ['GDRIVE_BACKUP_PATH'] = 'drive:backup/goemotions-training'\nos.environ['IMMEDIATE_CLEANUP'] = 'true'  # Clean up local files after cloud upload\nos.environ['MAX_LOCAL_CHECKPOINTS'] = '1'  # Keep only 1 checkpoint locally\nprint(\"✅ Cloud storage optimized for minimal local storage\")\nprint(\"   📤 Backup every 2 minutes to Google Drive\")\nprint(\"   🗑️ Automatic cleanup after each backup\")\nprint(\"   💾 Keep only 1 checkpoint locally (saves ~15-25GB)\")\n\n# Verify prerequisites\nchecks_passed = True\n\nif not os.path.exists('data/combined_all_datasets/train.jsonl'):\n    print(\"❌ Dataset not found - run Cell 2 first\")\n    checks_passed = False\n\nif not os.path.exists('scripts/train_comprehensive_multidataset.sh'):\n    print(\"❌ Training script not found\")\n    checks_passed = False\n\nif not checks_passed:\n    print(\"\\n💡 Please run Cell 2 first to prepare data\")\n    exit()\n\n# Make script executable\nos.chmod('scripts/train_comprehensive_multidataset.sh', 0o755)\nprint(\"✅ Training script ready\")\n\n# Start training\nprint(\"\\n🚀 STARTING TRAINING...\")\nprint(\"📊 Monitor progress: tail -f logs/train_comprehensive_multidataset.log\")\nprint(\"📊 Results will be in: checkpoints_comprehensive_multidataset/eval_report.json\")\nprint(\"☁️ Google Drive backup: Automatic (every 2 minutes during training)\")\nprint(\"🗑️ Local cleanup: Automatic after each backup\")\nprint(\"\\n⚠️ This will take 6-8 hours. Training runs with VISIBLE progress!\")\nprint(\"⚠️ DO NOT close this notebook - you'll see live progress bars!\")\nprint(\"-\" * 70)\n\n# Run training\ntraining_result = subprocess.run(['bash', 'scripts/train_comprehensive_multidataset.sh'], \n                                capture_output=False, text=True)\n\n# Check results\nprint(\"\\n\" + \"=\" * 50)\nif os.path.exists('checkpoints_comprehensive_multidataset/eval_report.json'):\n    print(\"✅ TRAINING COMPLETED SUCCESSFULLY!\")\n    print(\"📊 Results available locally: checkpoints_comprehensive_multidataset/eval_report.json\")\n    print(\"☁️ Google Drive backup: Completed automatically during training\")\n    \n    # Try to show F1 scores\n    try:\n        import json\n        with open('checkpoints_comprehensive_multidataset/eval_report.json', 'r') as f:\n            results = json.load(f)\n        f1_macro = results.get('f1_macro', 'N/A')\n        f1_micro = results.get('f1_micro', 'N/A')\n        print(f\"\\n📈 PERFORMANCE:\")\n        print(f\"   F1 Macro: {f1_macro}\")\n        print(f\"   F1 Micro: {f1_micro}\")\n        if f1_macro != 'N/A' and f1_macro > 0.6:\n            print(\"\\n🎉 SUCCESS: Achieved >60% F1-macro target!\")\n        elif f1_macro != 'N/A' and f1_macro > 0.55:\n            print(\"\\n👍 GOOD: Achieved >55% F1-macro!\")\n    except:\n        print(\"📊 Check eval_report.json for detailed results\")\n        \nelse:\n    print(\"⚠️ TRAINING MAY HAVE FAILED OR IS STILL RUNNING\")\n    print(\"📊 Check logs: tail -f logs/train_comprehensive_multidataset.log\")\n    print(\"📊 Check for results: checkpoints_comprehensive_multidataset/eval_report.json\")\n\nprint(\"\\n🎯 Target: >60% F1-macro\")\nprint(\"🏆 Baseline: 51.79% F1-macro (GoEmotions only)\")\nprint(\"☁️ Backup: Automatic Google Drive (every 2 minutes)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 **STEP 3: Results Analysis (Optional)**\n",
    "### **🎯 COMPARE WITH BASELINE**\n",
    "Analyze performance improvement from multi-dataset training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESULTS ANALYSIS\n",
    "# Compare multi-dataset performance with baseline\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"📊 MULTI-DATASET RESULTS ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Baseline from original GoEmotions training\n",
    "baseline = {\n",
    "    'f1_macro': 0.5179,\n",
    "    'f1_micro': 0.5975,\n",
    "    'model': 'BCE Extended (GoEmotions only)'\n",
    "}\n",
    "\n",
    "print(\"🏆 BASELINE PERFORMANCE:\")\n",
    "print(f\"   F1 Macro: {baseline['f1_macro']:.4f} ({baseline['f1_macro']*100:.1f}%)\")\n",
    "print(f\"   F1 Micro: {baseline['f1_micro']:.4f} ({baseline['f1_micro']*100:.1f}%)\")\n",
    "print(f\"   Model: {baseline['model']}\")\n",
    "\n",
    "# Load current results\n",
    "eval_file = Path(\"checkpoints_comprehensive_multidataset/eval_report.json\")\n",
    "\n",
    "if eval_file.exists():\n",
    "    with open(eval_file, 'r') as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    current_f1_macro = results.get('f1_macro', 0)\n",
    "    current_f1_micro = results.get('f1_micro', 0)\n",
    "    \n",
    "    print(\"\\n🎯 MULTI-DATASET RESULTS:\")\n",
    "    print(f\"   F1 Macro: {current_f1_macro:.4f} ({current_f1_macro*100:.1f}%)\")\n",
    "    print(f\"   F1 Micro: {current_f1_micro:.4f} ({current_f1_micro*100:.1f}%)\")\n",
    "    \n",
    "    # Calculate improvement\n",
    "    improvement = ((current_f1_macro - baseline['f1_macro']) / baseline['f1_macro']) * 100\n",
    "    \n",
    "    print(f\"\\n📈 IMPROVEMENT: {improvement:+.1f}%\")\n",
    "    \n",
    "    # Success assessment\n",
    "    if current_f1_macro >= 0.60:\n",
    "        print(\"🚀 EXCELLENT: Achieved >60% F1-macro target!\")\n",
    "        print(\"🎉 Multi-dataset training SUCCESSFUL!\")\n",
    "    elif current_f1_macro >= 0.55:\n",
    "        print(\"✅ GOOD: Achieved >55% F1-macro!\")\n",
    "        print(\"📈 Significant improvement from multi-dataset approach\")\n",
    "    elif current_f1_macro > baseline['f1_macro']:\n",
    "        print(\"👍 IMPROVEMENT: Better than baseline\")\n",
    "        print(\"🔧 May need more training epochs or parameter tuning\")\n",
    "    else:\n",
    "        print(\"⚠️ NO IMPROVEMENT: Check data quality or training setup\")\n",
    "        \n",
    "    print(f\"\\n📊 TARGET ACHIEVEMENT:\")\n",
    "    print(f\"   >60% F1-macro: {'✅' if current_f1_macro >= 0.60 else '❌'} (Target: 60%+)\")\n",
    "    print(f\"   >55% F1-macro: {'✅' if current_f1_macro >= 0.55 else '❌'} (Target: 55%+)\")\n",
    "    print(f\"   Beat baseline: {'✅' if current_f1_macro > baseline['f1_macro'] else '❌'}\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n⏳ RESULTS NOT AVAILABLE\")\n",
    "    print(\"🔧 Training may still be in progress or check file path\")\n",
    "    print(\"📁 Expected: checkpoints_comprehensive_multidataset/eval_report.json\")\n",
    "\n",
    "print(\"\\n🔍 MONITORING COMMANDS:\")\n",
    "print(\"   Training logs: tail -f logs/train_comprehensive_multidataset.log\")\n",
    "print(\"   GPU status: watch -n 5 'nvidia-smi'\")\n",
    "print(\"   Process status: ps aux | grep train_deberta\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# POST-TRAINING CLEANUP\n# Remove local artifacts after ensuring cloud backup\n\nimport os\nimport subprocess\nimport shutil\nfrom pathlib import Path\n\nprint(\"🧹 POST-TRAINING CLEANUP\")\nprint(\"=\" * 50)\nprint(\"🎯 Goal: Free up disk space by removing local artifacts\")\nprint(\"☁️ Prerequisite: Ensure cloud backup is complete\")\nprint(\"=\" * 50)\n\n# Change to project directory\nos.chdir('/home/user/goemotions-deberta')\n\n# Check current disk usage\ndef get_disk_usage():\n    disk_usage = shutil.disk_usage(\".\")\n    free_gb = disk_usage.free / (1024 ** 3)\n    used_gb = disk_usage.used / (1024 ** 3)\n    total_gb = disk_usage.total / (1024 ** 3)\n    used_percent = (disk_usage.used / disk_usage.total) * 100\n    return free_gb, used_gb, total_gb, used_percent\n\n# Before cleanup\nfree_before, used_before, total, used_percent_before = get_disk_usage()\nprint(f\"💾 BEFORE CLEANUP:\")\nprint(f\"   Used: {used_before:.1f}GB ({used_percent_before:.1f}%)\")\nprint(f\"   Free: {free_before:.1f}GB\")\nprint(f\"   Total: {total:.1f}GB\")\n\n# Verify cloud backup exists\nprint(f\"\\n☁️ VERIFYING CLOUD BACKUP...\")\nresult = subprocess.run(['rclone', 'lsf', 'drive:backup/goemotions-training/'], \n                       capture_output=True, text=True)\n\nif result.returncode == 0 and result.stdout.strip():\n    print(\"✅ Cloud backup found!\")\n    backup_folders = result.stdout.strip().split('\\n')\n    print(f\"   Found {len(backup_folders)} backup folder(s)\")\n    for folder in backup_folders[-3:]:  # Show latest 3\n        print(f\"   📁 {folder}\")\nelse:\n    print(\"⚠️ No cloud backup found or rclone error!\")\n    print(\"💡 Run training first to create backup before cleanup\")\n    print(\"🛑 STOPPING CLEANUP - Backup required for safety\")\n    exit()\n\n# Safe cleanup of training artifacts\nprint(f\"\\n🗑️ CLEANING LOCAL ARTIFACTS...\")\n\ncleanup_targets = [\n    'checkpoints_comprehensive_multidataset/',\n    'logs/',\n    'models/',\n    'outputs/',\n    '__pycache__/',\n    '.cache/'\n]\n\ntotal_freed = 0\n\nfor target in cleanup_targets:\n    if os.path.exists(target):\n        # Get size before deletion\n        if os.path.isdir(target):\n            size_mb = sum(os.path.getsize(os.path.join(dirpath, filename))\n                         for dirpath, dirnames, filenames in os.walk(target)\n                         for filename in filenames) / (1024 * 1024)\n        else:\n            size_mb = os.path.getsize(target) / (1024 * 1024)\n        \n        try:\n            if os.path.isdir(target):\n                # Keep only essential files in checkpoints\n                if target == 'checkpoints_comprehensive_multidataset/':\n                    # Keep config and eval_report, remove model weights\n                    keep_files = ['config.json', 'eval_report.json', 'tokenizer.json', 'tokenizer_config.json']\n                    if os.path.exists(target):\n                        for item in os.listdir(target):\n                            item_path = os.path.join(target, item)\n                            if os.path.isdir(item_path) and 'checkpoint-' in item:\n                                shutil.rmtree(item_path)\n                                print(f\"🗑️ Removed checkpoint: {item}\")\n                            elif os.path.isfile(item_path) and item not in keep_files and item.endswith(('.bin', '.safetensors')):\n                                os.remove(item_path)\n                                print(f\"🗑️ Removed model file: {item}\")\n                else:\n                    shutil.rmtree(target)\n                    print(f\"🗑️ Removed directory: {target} ({size_mb:.1f}MB)\")\n            else:\n                os.remove(target)\n                print(f\"🗑️ Removed file: {target} ({size_mb:.1f}MB)\")\n            \n            total_freed += size_mb\n            \n        except Exception as e:\n            print(f\"⚠️ Could not remove {target}: {str(e)}\")\n\n# Clean up temporary and cache files\ntemp_patterns = ['*.tmp', 'tmp_*', '.DS_Store', 'Thumbs.db']\nfor pattern in temp_patterns:\n    import glob\n    for file in glob.glob(pattern):\n        try:\n            os.remove(file)\n            print(f\"🗑️ Removed temp file: {file}\")\n        except:\n            pass\n\n# After cleanup\nfree_after, used_after, total, used_percent_after = get_disk_usage()\nspace_freed = used_before - used_after\n\nprint(f\"\\n✅ CLEANUP COMPLETE!\")\nprint(f\"💾 AFTER CLEANUP:\")\nprint(f\"   Used: {used_after:.1f}GB ({used_percent_after:.1f}%)\")\nprint(f\"   Free: {free_after:.1f}GB\")\nprint(f\"   Space freed: {space_freed:.1f}GB\")\n\nif space_freed > 1:\n    print(f\"🎉 Successfully freed {space_freed:.1f}GB of disk space!\")\n    print(f\"📈 Disk usage reduced from {used_percent_before:.1f}% to {used_percent_after:.1f}%\")\nelse:\n    print(f\"ℹ️ Minimal space freed ({space_freed:.1f}GB) - artifacts may have been cleaned during training\")\n\nprint(f\"\\n☁️ IMPORTANT: All training artifacts are safely backed up to Google Drive\")\nprint(f\"🔄 To restore: Use rclone to download from drive:backup/goemotions-training/\")\nprint(f\"📁 Local config files and eval reports retained for quick access\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 🧹 **STEP 4: Post-Training Cleanup (Optional)**\n### **🗑️ FREE UP DISK SPACE**\nRemove local artifacts after confirming cloud backup",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}