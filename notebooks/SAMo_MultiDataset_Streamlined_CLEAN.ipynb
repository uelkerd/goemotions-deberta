{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ **SAMo Multi-Dataset DeBERTa (CLEAN VERSION)**\n",
    "## **Efficient Multi-Dataset Training with Proven BCE Configuration**\n",
    "\n",
    "### **üéØ MISSION**\n",
    "- **One-Command Multi-Dataset Training**: GoEmotions + SemEval + ISEAR + MELD\n",
    "- **Proven BCE Configuration**: Use your 51.79% F1-macro winning setup\n",
    "- **No Threshold Testing**: Save time with threshold=0.2\n",
    "- **Achieve >60% F1-macro**: Through comprehensive dataset integration\n",
    "\n",
    "### **üìã SIMPLE WORKFLOW**\n",
    "1. **Run Cell 2**: Data preparation (10-15 minutes)\n",
    "2. **Run Cell 4**: Training (3-4 hours)\n",
    "3. **Monitor**: `tail -f logs/train_comprehensive_multidataset.log`\n",
    "\n",
    "### **üìä EXPECTED RESULTS**\n",
    "- **Baseline**: 51.79% F1-macro (GoEmotions BCE Extended)\n",
    "- **Target**: 60-65% F1-macro (All datasets combined)\n",
    "- **Dataset**: 38,111 samples (GoEmotions + SemEval + ISEAR + MELD)\n",
    "\n",
    "**Start with Cell 2 below!** üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì• **STEP 1: Data Preparation**\n",
    "### **üéØ ONE COMMAND - PREPARE ALL DATASETS**\n",
    "Combines GoEmotions + SemEval + ISEAR + MELD into unified training format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ MULTI-DATASET PREPARATION\n",
      "==================================================\n",
      "üìä Datasets: GoEmotions + SemEval + ISEAR + MELD\n",
      "‚è±Ô∏è Time: ~10-15 minutes\n",
      "==================================================\n",
      "üìÅ Working directory: /home/user/goemotions-deberta\n",
      "\n",
      "üîÑ Preparing datasets...\n",
      "üöÄ COMPREHENSIVE MULTI-DATASET PREPARATION\n",
      "============================================================\n",
      "üìä Datasets: GoEmotions + SemEval + ISEAR + MELD\n",
      "‚öôÔ∏è Configuration: Proven BCE setup (threshold=0.2)\n",
      "‚è±Ô∏è Time: ~10-15 minutes\n",
      "============================================================\n",
      "üìÅ Working directory: /home/user/goemotions-deberta\n",
      "üìñ Loading GoEmotions dataset...\n",
      "‚úÖ Loaded 43410 GoEmotions train samples\n",
      "‚úÖ Loaded 5426 GoEmotions val samples\n",
      "üì• Processing local SemEval-2018 EI-reg dataset...\n",
      "‚úÖ Found local SemEval zip file\n",
      "‚úÖ Copied local SemEval zip to data directory\n",
      "üì¶ Extracting SemEval-2018 zip file...\n",
      "‚úÖ Extracted SemEval-2018 data\n",
      "üìñ Processing anger data...\n",
      "üìñ Processing fear data...\n",
      "üìñ Processing joy data...\n",
      "üìñ Processing sadness data...\n",
      "‚úÖ Processed 802 SemEval samples\n",
      "üì• Downloading ISEAR dataset...\n",
      "üì• Loading ISEAR from Hugging Face...\n",
      "‚úÖ Processed 7516 ISEAR samples\n",
      "üì• Processing local MELD dataset (TEXT ONLY)...\n",
      "‚úÖ Found local MELD data directory\n",
      "üìä Found 3 CSV files\n",
      "üìñ Processing train_sent_emo.csv...\n",
      "üìñ Processing test_sent_emo.csv...\n",
      "üìñ Processing dev_sent_emo.csv...\n",
      "‚úÖ Processed 13708 MELD samples\n",
      "üîÑ Creating weighted combination of all datasets...\n",
      "üìä Target sizes:\n",
      "   GoEmotions: 29301 samples\n",
      "   Other datasets: 8810 samples\n",
      "‚úÖ Combined dataset created:\n",
      "   Train: 30489 samples\n",
      "   Val: 7622 samples\n",
      "   GoEmotions: 29301\n",
      "   Other datasets: 8810\n",
      "üíæ Saving dataset: data/combined_all_datasets/train.jsonl\n",
      "‚úÖ Saved 30489 samples\n",
      "üíæ Saving dataset: data/combined_all_datasets/val.jsonl\n",
      "‚úÖ Saved 7622 samples\n",
      "\n",
      "‚úÖ DATA PREPARATION COMPLETE!\n",
      "üìÅ Check: data/combined_all_datasets\n",
      "üöÄ Ready for training with all datasets combined!\n",
      "\n",
      "üìä FINAL SUMMARY:\n",
      "   Total train samples: 30489\n",
      "   Total val samples: 7622\n",
      "   GoEmotions samples: 48836\n",
      "   SemEval samples: 802\n",
      "   ISEAR samples: 7516\n",
      "   MELD samples: 13708\n",
      "\n",
      "‚úÖ SUCCESS: 38111 samples prepared\n",
      "   Training: 30489 samples\n",
      "   Validation: 7622 samples\n",
      "\n",
      "üöÄ Ready for training! Run Cell 4 next.\n"
     ]
    }
   ],
   "source": [
    "# MULTI-DATASET PREPARATION\n",
    "# Combines GoEmotions + SemEval + ISEAR + MELD datasets\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "print(\"üöÄ MULTI-DATASET PREPARATION\")\n",
    "print(\"=\" * 50)\n",
    "print(\"üìä Datasets: GoEmotions + SemEval + ISEAR + MELD\")\n",
    "print(\"‚è±Ô∏è Time: ~10-15 minutes\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Change to project directory\n",
    "os.chdir('/home/user/goemotions-deberta')\n",
    "print(f\"üìÅ Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Run data preparation\n",
    "print(\"\\nüîÑ Preparing datasets...\")\n",
    "result = subprocess.run(['python', './notebooks/prepare_all_datasets.py'], \n",
    "                       capture_output=False, text=True)\n",
    "\n",
    "# Verify success\n",
    "if os.path.exists('data/combined_all_datasets/train.jsonl'):\n",
    "    train_count = sum(1 for line in open('data/combined_all_datasets/train.jsonl'))\n",
    "    val_count = sum(1 for line in open('data/combined_all_datasets/val.jsonl'))\n",
    "    print(f\"\\n‚úÖ SUCCESS: {train_count + val_count} samples prepared\")\n",
    "    print(f\"   Training: {train_count} samples\")\n",
    "    print(f\"   Validation: {val_count} samples\")\n",
    "    print(\"\\nüöÄ Ready for training! Run Cell 4 next.\")\n",
    "else:\n",
    "    print(\"\\n‚ùå FAILED: Dataset preparation unsuccessful\")\n",
    "    print(\"üí° Check logs and try again\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö° **STEP 2: Training**\n",
    "### **üéØ START MULTI-DATASET TRAINING**\n",
    "Trains DeBERTa on combined dataset with proven configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ MULTI-DATASET TRAINING\n",
      "==================================================\n",
      "ü§ñ Model: DeBERTa-v3-large\n",
      "üìä Data: 38K+ samples (GoEmotions + SemEval + ISEAR + MELD)\n",
      "üéØ Loss: BCE (your proven 51.79% winner)\n",
      "‚è±Ô∏è Time: ~6-8 hours (5 epochs on larger dataset)\n",
      "==================================================\n",
      "‚úÖ Training script ready\n",
      "\n",
      "üöÄ STARTING TRAINING...\n",
      "üìä Monitor progress: tail -f logs/train_comprehensive_multidataset.log\n",
      "üìä Results will be in: checkpoints_comprehensive_multidataset/eval_report.json\n",
      "‚òÅÔ∏è Google Drive backup: Automatic (every 15 minutes during training)\n",
      "\n",
      "‚ö†Ô∏è This will take 6-8 hours. Training runs with VISIBLE progress!\n",
      "‚ö†Ô∏è DO NOT close this notebook - you'll see live progress bars!\n",
      "----------------------------------------------------------------------\n",
      "üöÄ COMPREHENSIVE MULTI-DATASET TRAINING\n",
      "========================================\n",
      "üéØ TARGET: >60% F1-macro with all datasets combined\n",
      "üìä Datasets: GoEmotions + SemEval + ISEAR + MELD\n",
      "‚ö° Configuration: BCE Extended (your proven 51.79% winner)\n",
      "==========================================\n"
     ]
    }
   ],
   "source": [
    "# MULTI-DATASET TRAINING\n",
    "# Trains DeBERTa on combined dataset with Asymmetric Loss\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"üöÄ MULTI-DATASET TRAINING\")\n",
    "print(\"=\" * 50)\n",
    "print(\"ü§ñ Model: DeBERTa-v3-large\")\n",
    "print(\"üìä Data: 38K+ samples (GoEmotions + SemEval + ISEAR + MELD)\")\n",
    "print(\"üéØ Loss: BCE (your proven 51.79% winner)\")\n",
    "print(\"‚è±Ô∏è Time: ~6-8 hours (5 epochs on larger dataset)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Change to project directory\n",
    "os.chdir('/home/user/goemotions-deberta')\n",
    "\n",
    "# Verify prerequisites\n",
    "checks_passed = True\n",
    "\n",
    "if not os.path.exists('data/combined_all_datasets/train.jsonl'):\n",
    "    print(\"‚ùå Dataset not found - run Cell 2 first\")\n",
    "    checks_passed = False\n",
    "\n",
    "if not os.path.exists('scripts/train_comprehensive_multidataset.sh'):\n",
    "    print(\"‚ùå Training script not found\")\n",
    "    checks_passed = False\n",
    "\n",
    "if not checks_passed:\n",
    "    print(\"\\nüí° Please run Cell 2 first to prepare data\")\n",
    "    exit()\n",
    "\n",
    "# Make script executable\n",
    "os.chmod('scripts/train_comprehensive_multidataset.sh', 0o755)\n",
    "print(\"‚úÖ Training script ready\")\n",
    "\n",
    "# Start training\n",
    "print(\"\\nüöÄ STARTING TRAINING...\")\n",
    "print(\"üìä Monitor progress: tail -f logs/train_comprehensive_multidataset.log\")\n",
    "print(\"üìä Results will be in: checkpoints_comprehensive_multidataset/eval_report.json\")\n",
    "print(\"‚òÅÔ∏è Google Drive backup: Automatic (every 15 minutes during training)\")\n",
    "print(\"\\n‚ö†Ô∏è This will take 6-8 hours. Training runs with VISIBLE progress!\")\n",
    "print(\"‚ö†Ô∏è DO NOT close this notebook - you'll see live progress bars!\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Run training\n",
    "training_result = subprocess.run(['bash', 'scripts/train_comprehensive_multidataset.sh'], \n",
    "                                capture_output=False, text=True)\n",
    "\n",
    "# Check results\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "if os.path.exists('checkpoints_comprehensive_multidataset/eval_report.json'):\n",
    "    print(\"‚úÖ TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"üìä Results available locally: checkpoints_comprehensive_multidataset/eval_report.json\")\n",
    "    print(\"‚òÅÔ∏è Google Drive backup: Completed automatically during training\")\n",
    "    \n",
    "    # Try to show F1 scores\n",
    "    try:\n",
    "        import json\n",
    "        with open('checkpoints_comprehensive_multidataset/eval_report.json', 'r') as f:\n",
    "            results = json.load(f)\n",
    "        f1_macro = results.get('f1_macro', 'N/A')\n",
    "        f1_micro = results.get('f1_micro', 'N/A')\n",
    "        print(f\"\\nüìà PERFORMANCE:\")\n",
    "        print(f\"   F1 Macro: {f1_macro}\")\n",
    "        print(f\"   F1 Micro: {f1_micro}\")\n",
    "        if f1_macro != 'N/A' and f1_macro > 0.6:\n",
    "            print(\"\\nüéâ SUCCESS: Achieved >60% F1-macro target!\")\n",
    "        elif f1_macro != 'N/A' and f1_macro > 0.55:\n",
    "            print(\"\\nüëç GOOD: Achieved >55% F1-macro!\")\n",
    "    except:\n",
    "        print(\"üìä Check eval_report.json for detailed results\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è TRAINING MAY HAVE FAILED OR IS STILL RUNNING\")\n",
    "    print(\"üìä Check logs: tail -f logs/train_comprehensive_multidataset.log\")\n",
    "    print(\"üìä Check for results: checkpoints_comprehensive_multidataset/eval_report.json\")\n",
    "\n",
    "print(\"\\nüéØ Target: >60% F1-macro\")\n",
    "print(\"üèÜ Baseline: 51.79% F1-macro (GoEmotions only)\")\n",
    "print(\"‚òÅÔ∏è Backup: Automatic Google Drive (timestamped folders)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä **STEP 3: Results Analysis (Optional)**\n",
    "### **üéØ COMPARE WITH BASELINE**\n",
    "Analyze performance improvement from multi-dataset training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESULTS ANALYSIS\n",
    "# Compare multi-dataset performance with baseline\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"üìä MULTI-DATASET RESULTS ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Baseline from original GoEmotions training\n",
    "baseline = {\n",
    "    'f1_macro': 0.5179,\n",
    "    'f1_micro': 0.5975,\n",
    "    'model': 'BCE Extended (GoEmotions only)'\n",
    "}\n",
    "\n",
    "print(\"üèÜ BASELINE PERFORMANCE:\")\n",
    "print(f\"   F1 Macro: {baseline['f1_macro']:.4f} ({baseline['f1_macro']*100:.1f}%)\")\n",
    "print(f\"   F1 Micro: {baseline['f1_micro']:.4f} ({baseline['f1_micro']*100:.1f}%)\")\n",
    "print(f\"   Model: {baseline['model']}\")\n",
    "\n",
    "# Load current results\n",
    "eval_file = Path(\"checkpoints_comprehensive_multidataset/eval_report.json\")\n",
    "\n",
    "if eval_file.exists():\n",
    "    with open(eval_file, 'r') as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    current_f1_macro = results.get('f1_macro', 0)\n",
    "    current_f1_micro = results.get('f1_micro', 0)\n",
    "    \n",
    "    print(\"\\nüéØ MULTI-DATASET RESULTS:\")\n",
    "    print(f\"   F1 Macro: {current_f1_macro:.4f} ({current_f1_macro*100:.1f}%)\")\n",
    "    print(f\"   F1 Micro: {current_f1_micro:.4f} ({current_f1_micro*100:.1f}%)\")\n",
    "    \n",
    "    # Calculate improvement\n",
    "    improvement = ((current_f1_macro - baseline['f1_macro']) / baseline['f1_macro']) * 100\n",
    "    \n",
    "    print(f\"\\nüìà IMPROVEMENT: {improvement:+.1f}%\")\n",
    "    \n",
    "    # Success assessment\n",
    "    if current_f1_macro >= 0.60:\n",
    "        print(\"üöÄ EXCELLENT: Achieved >60% F1-macro target!\")\n",
    "        print(\"üéâ Multi-dataset training SUCCESSFUL!\")\n",
    "    elif current_f1_macro >= 0.55:\n",
    "        print(\"‚úÖ GOOD: Achieved >55% F1-macro!\")\n",
    "        print(\"üìà Significant improvement from multi-dataset approach\")\n",
    "    elif current_f1_macro > baseline['f1_macro']:\n",
    "        print(\"üëç IMPROVEMENT: Better than baseline\")\n",
    "        print(\"üîß May need more training epochs or parameter tuning\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è NO IMPROVEMENT: Check data quality or training setup\")\n",
    "        \n",
    "    print(f\"\\nüìä TARGET ACHIEVEMENT:\")\n",
    "    print(f\"   >60% F1-macro: {'‚úÖ' if current_f1_macro >= 0.60 else '‚ùå'} (Target: 60%+)\")\n",
    "    print(f\"   >55% F1-macro: {'‚úÖ' if current_f1_macro >= 0.55 else '‚ùå'} (Target: 55%+)\")\n",
    "    print(f\"   Beat baseline: {'‚úÖ' if current_f1_macro > baseline['f1_macro'] else '‚ùå'}\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n‚è≥ RESULTS NOT AVAILABLE\")\n",
    "    print(\"üîß Training may still be in progress or check file path\")\n",
    "    print(\"üìÅ Expected: checkpoints_comprehensive_multidataset/eval_report.json\")\n",
    "\n",
    "print(\"\\nüîç MONITORING COMMANDS:\")\n",
    "print(\"   Training logs: tail -f logs/train_comprehensive_multidataset.log\")\n",
    "print(\"   GPU status: watch -n 5 'nvidia-smi'\")\n",
    "print(\"   Process status: ps aux | grep train_deberta\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
