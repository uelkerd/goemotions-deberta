{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ **SAMo Multi-Dataset DeBERTa (CLEAN VERSION)**\n",
    "## **Efficient Multi-Dataset Training with Proven BCE Configuration**\n",
    "\n",
    "### **ğŸ¯ MISSION**\n",
    "- **One-Command Multi-Dataset Training**: GoEmotions + SemEval + ISEAR + MELD\n",
    "- **Proven BCE Configuration**: Use your 51.79% F1-macro winning setup\n",
    "- **No Threshold Testing**: Save time with threshold=0.2\n",
    "- **Achieve >60% F1-macro**: Through comprehensive dataset integration\n",
    "\n",
    "### **ğŸ“‹ SIMPLE WORKFLOW**\n",
    "1. **Run Cell 2**: Data preparation (10-15 minutes)\n",
    "2. **Run Cell 4**: Training (3-4 hours)\n",
    "3. **Monitor**: `tail -f logs/train_comprehensive_multidataset.log`\n",
    "\n",
    "### **ğŸ“Š EXPECTED RESULTS**\n",
    "- **Baseline**: 51.79% F1-macro (GoEmotions BCE Extended)\n",
    "- **Target**: 60-65% F1-macro (All datasets combined)\n",
    "- **Dataset**: 38,111 samples (GoEmotions + SemEval + ISEAR + MELD)\n",
    "\n",
    "**Start with Cell 2 below!** ğŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¥ **STEP 1: Data Preparation**\n",
    "### **ğŸ¯ ONE COMMAND - PREPARE ALL DATASETS**\n",
    "Combines GoEmotions + SemEval + ISEAR + MELD into unified training format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ MULTI-DATASET PREPARATION\n",
      "==================================================\n",
      "ğŸ“Š Datasets: GoEmotions + SemEval + ISEAR + MELD\n",
      "â±ï¸ Time: ~10-15 minutes\n",
      "==================================================\n",
      "ğŸ“ Working directory: /home/user/goemotions-deberta\n",
      "\n",
      "ğŸ”„ Preparing datasets...\n",
      "ğŸš€ COMPREHENSIVE MULTI-DATASET PREPARATION\n",
      "============================================================\n",
      "ğŸ“Š Datasets: GoEmotions + SemEval + ISEAR + MELD\n",
      "âš™ï¸ Configuration: Proven BCE setup (threshold=0.2)\n",
      "â±ï¸ Time: ~10-15 minutes\n",
      "============================================================\n",
      "ğŸ“ Working directory: /home/user/goemotions-deberta\n",
      "ğŸ“– Loading GoEmotions dataset...\n",
      "âœ… Found local GoEmotions cache\n",
      "âœ… Loaded 43410 GoEmotions train samples\n",
      "âœ… Loaded 5426 GoEmotions val samples\n",
      "ğŸ“¥ Loading SemEval-2018 EI-reg dataset...\n",
      "âœ… Found SemEval zip at: data/semeval2018/SemEval2018-Task1-all-data.zip\n",
      "âœ… Found local SemEval zip file\n",
      "âœ… Copied local SemEval zip to data directory\n",
      "ğŸ“¦ Extracting SemEval-2018 zip file...\n",
      "âœ… Extracted SemEval-2018 data\n",
      "ğŸ“– Processing anger data...\n",
      "ğŸ“– Processing fear data...\n",
      "ğŸ“– Processing joy data...\n",
      "ğŸ“– Processing sadness data...\n",
      "âœ… Processed 804 SemEval samples\n",
      "ğŸ“¥ Loading ISEAR dataset...\n",
      "ğŸ“¥ Loading ISEAR from Hugging Face...\n",
      "âœ… Found ISEAR dataset on Hugging Face (gsri-18 version)\n",
      "âš ï¸ ISEAR dataset loading failed: 'str' object has no attribute 'get'\n",
      "âš ï¸ All ISEAR options failed, using emotion dataset as alternative...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'emotion' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Using emotion dataset as ISEAR alternative\n",
      "ğŸ“¥ Processing local MELD dataset (TEXT ONLY)...\n",
      "âœ… Found local MELD data directory\n",
      "ğŸ“Š Found 1 CSV files\n",
      "ğŸ“– Processing train_sent_emo.csv...\n",
      "âœ… Processed 8328 MELD samples\n",
      "ğŸ”„ Creating weighted combination of all datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/user/goemotions-deberta/./notebooks/prepare_all_datasets.py\", line 641, in <module>\n",
      "    main()\n",
      "  File \"/home/user/goemotions-deberta/./notebooks/prepare_all_datasets.py\", line 617, in main\n",
      "    combined_train, combined_val = combine_datasets(\n",
      "  File \"/home/user/goemotions-deberta/./notebooks/prepare_all_datasets.py\", line 538, in combine_datasets\n",
      "    total_other = len(semeval_data) + len(isear_data) + len(meld_data)\n",
      "TypeError: object of type 'NoneType' has no len()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… SUCCESS: 50643 samples prepared\n",
      "   Training: 43409 samples\n",
      "   Validation: 7234 samples\n",
      "\n",
      "ğŸš€ Ready for training! Run Cell 4 next.\n"
     ]
    }
   ],
   "source": [
    "# MULTI-DATASET PREPARATION\n",
    "# Combines GoEmotions + SemEval + ISEAR + MELD datasets\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "print(\"ğŸš€ MULTI-DATASET PREPARATION\")\n",
    "print(\"=\" * 50)\n",
    "print(\"ğŸ“Š Datasets: GoEmotions + SemEval + ISEAR + MELD\")\n",
    "print(\"â±ï¸ Time: ~10-15 minutes\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Change to project directory\n",
    "os.chdir('/home/user/goemotions-deberta')\n",
    "print(f\"ğŸ“ Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Run data preparation\n",
    "print(\"\\nğŸ”„ Preparing datasets...\")\n",
    "result = subprocess.run(['python', './notebooks/prepare_all_datasets.py'], \n",
    "                       capture_output=False, text=True)\n",
    "\n",
    "# Verify success\n",
    "if os.path.exists('data/combined_all_datasets/train.jsonl'):\n",
    "    train_count = sum(1 for line in open('data/combined_all_datasets/train.jsonl'))\n",
    "    val_count = sum(1 for line in open('data/combined_all_datasets/val.jsonl'))\n",
    "    print(f\"\\nâœ… SUCCESS: {train_count + val_count} samples prepared\")\n",
    "    print(f\"   Training: {train_count} samples\")\n",
    "    print(f\"   Validation: {val_count} samples\")\n",
    "    print(\"\\nğŸš€ Ready for training! Run Cell 4 next.\")\n",
    "else:\n",
    "    print(\"\\nâŒ FAILED: Dataset preparation unsuccessful\")\n",
    "    print(\"ğŸ’¡ Check logs and try again\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš¡ **STEP 2: Training**\n",
    "### **ğŸ¯ START MULTI-DATASET TRAINING**\n",
    "Trains DeBERTa on combined dataset with proven configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
<<<<<<< HEAD
   "outputs": [],
   "source": "# MULTI-DATASET TRAINING\n# Trains DeBERTa on combined dataset with Asymmetric Loss\n\nimport os\nimport subprocess\nfrom pathlib import Path\n\nprint(\"ğŸš€ MULTI-DATASET TRAINING\")\nprint(\"=\" * 50)\nprint(\"ğŸ¤– Model: DeBERTa-v3-large\")\nprint(\"ğŸ“Š Data: 38K+ samples (GoEmotions + SemEval + ISEAR + MELD)\")\nprint(\"ğŸ¯ Loss: BCE (your proven 51.79% winner)\")\nprint(\"â±ï¸ Time: ~6-8 hours (5 epochs on larger dataset)\")\nprint(\"=\" * 50)\n\n# Change to project directory\nos.chdir('/home/user/goemotions-deberta')\n\n# Set optimized cloud storage environment variables\nprint(\"\\nâš™ï¸ CONFIGURING OPTIMIZED CLOUD STORAGE...\")\nos.environ['GDRIVE_BACKUP_PATH'] = 'drive:backup/goemotions-training'\nos.environ['IMMEDIATE_CLEANUP'] = 'true'  # Clean up local files after cloud upload\nos.environ['MAX_LOCAL_CHECKPOINTS'] = '1'  # Keep only 1 checkpoint locally\nprint(\"âœ… Cloud storage optimized for minimal local storage\")\nprint(\"   ğŸ“¤ Backup every 2 minutes to Google Drive\")\nprint(\"   ğŸ—‘ï¸ Automatic cleanup after each backup\")\nprint(\"   ğŸ’¾ Keep only 1 checkpoint locally (saves ~15-25GB)\")\n\n# Verify prerequisites\nchecks_passed = True\n\nif not os.path.exists('data/combined_all_datasets/train.jsonl'):\n    print(\"âŒ Dataset not found - run Cell 2 first\")\n    checks_passed = False\n\nif not os.path.exists('scripts/train_comprehensive_multidataset.sh'):\n    print(\"âŒ Training script not found\")\n    checks_passed = False\n\nif not checks_passed:\n    print(\"\\nğŸ’¡ Please run Cell 2 first to prepare data\")\n    exit()\n\n# Make script executable\nos.chmod('scripts/train_comprehensive_multidataset.sh', 0o755)\nprint(\"âœ… Training script ready\")\n\n# Start training\nprint(\"\\nğŸš€ STARTING TRAINING...\")\nprint(\"ğŸ“Š Monitor progress: tail -f logs/train_comprehensive_multidataset.log\")\nprint(\"ğŸ“Š Results will be in: checkpoints_comprehensive_multidataset/eval_report.json\")\nprint(\"â˜ï¸ Google Drive backup: Automatic (every 2 minutes during training)\")\nprint(\"ğŸ—‘ï¸ Local cleanup: Automatic after each backup\")\nprint(\"\\nâš ï¸ This will take 6-8 hours. Training runs with VISIBLE progress!\")\nprint(\"âš ï¸ DO NOT close this notebook - you'll see live progress bars!\")\nprint(\"-\" * 70)\n\n# Run training\ntraining_result = subprocess.run(['bash', 'scripts/train_comprehensive_multidataset.sh'], \n                                capture_output=False, text=True)\n\n# Check results\nprint(\"\\n\" + \"=\" * 50)\nif os.path.exists('checkpoints_comprehensive_multidataset/eval_report.json'):\n    print(\"âœ… TRAINING COMPLETED SUCCESSFULLY!\")\n    print(\"ğŸ“Š Results available locally: checkpoints_comprehensive_multidataset/eval_report.json\")\n    print(\"â˜ï¸ Google Drive backup: Completed automatically during training\")\n    \n    # Try to show F1 scores\n    try:\n        import json\n        with open('checkpoints_comprehensive_multidataset/eval_report.json', 'r') as f:\n            results = json.load(f)\n        f1_macro = results.get('f1_macro', 'N/A')\n        f1_micro = results.get('f1_micro', 'N/A')\n        print(f\"\\nğŸ“ˆ PERFORMANCE:\")\n        print(f\"   F1 Macro: {f1_macro}\")\n        print(f\"   F1 Micro: {f1_micro}\")\n        if f1_macro != 'N/A' and f1_macro > 0.6:\n            print(\"\\nğŸ‰ SUCCESS: Achieved >60% F1-macro target!\")\n        elif f1_macro != 'N/A' and f1_macro > 0.55:\n            print(\"\\nğŸ‘ GOOD: Achieved >55% F1-macro!\")\n    except:\n        print(\"ğŸ“Š Check eval_report.json for detailed results\")\n        \nelse:\n    print(\"âš ï¸ TRAINING MAY HAVE FAILED OR IS STILL RUNNING\")\n    print(\"ğŸ“Š Check logs: tail -f logs/train_comprehensive_multidataset.log\")\n    print(\"ğŸ“Š Check for results: checkpoints_comprehensive_multidataset/eval_report.json\")\n\nprint(\"\\nğŸ¯ Target: >60% F1-macro\")\nprint(\"ğŸ† Baseline: 51.79% F1-macro (GoEmotions only)\")\nprint(\"â˜ï¸ Backup: Automatic Google Drive (every 2 minutes)\")"
=======
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ MULTI-DATASET TRAINING\n",
      "==================================================\n",
      "ğŸ¤– Model: DeBERTa-v3-large\n",
      "ğŸ“Š Data: 38K+ samples (GoEmotions + SemEval + ISEAR + MELD)\n",
      "ğŸ¯ Loss: BCE (your proven 51.79% winner)\n",
      "â±ï¸ Time: ~6-8 hours (5 epochs on larger dataset)\n",
      "==================================================\n",
      "âœ… Training script ready\n",
      "\n",
      "ğŸš€ STARTING TRAINING...\n",
      "ğŸ“Š Monitor progress: tail -f logs/train_comprehensive_multidataset.log\n",
      "ğŸ“Š Results will be in: checkpoints_comprehensive_multidataset/eval_report.json\n",
      "â˜ï¸ Google Drive backup: Automatic (every 15 minutes during training)\n",
      "\n",
      "âš ï¸ This will take 6-8 hours. Training runs with VISIBLE progress!\n",
      "âš ï¸ DO NOT close this notebook - you'll see live progress bars!\n",
      "----------------------------------------------------------------------\n",
      "ğŸš€ COMPREHENSIVE MULTI-DATASET TRAINING\n",
      "========================================\n",
      "ğŸ¯ TARGET: >60% F1-macro with all datasets combined\n",
      "ğŸ“Š Datasets: GoEmotions + SemEval + ISEAR + MELD\n",
      "âš¡ Configuration: BCE Extended (your proven 51.79% winner)\n",
      "==========================================\n",
      "ğŸ“Š Logging to: logs/train_comprehensive_multidataset.log\n",
      "[2025-09-14 13:38:32] ğŸš€ Starting comprehensive multi-dataset training...\n",
      "[2025-09-14 13:38:32] ğŸ“Š Configuration: BCE (proven winner from 51.79% baseline)\n",
      "[2025-09-14 13:38:32] ğŸ¯ Target performance: >60% F1-macro\n",
      "[2025-09-14 13:38:32] ğŸ” Checking prerequisites...\n",
      "[2025-09-14 13:38:32] âœ… Dataset ready: 43409 train, 7234 val samples\n",
      "[2025-09-14 13:38:32] ğŸ® Using GPU: 0\n",
      "[2025-09-14 13:38:32] âš™ï¸ Training parameters:\n",
      "[2025-09-14 13:38:32]    Model: deberta-v3-large\n",
      "[2025-09-14 13:38:32]    Epochs: 3\n",
      "[2025-09-14 13:38:32]    Batch size: 4\n",
      "[2025-09-14 13:38:32]    Learning rate: 3e-5\n",
      "[2025-09-14 13:38:32]    Output: checkpoints_comprehensive_multidataset\n",
      "[2025-09-14 13:38:32] ğŸš€ Starting training with Combined Loss (optimized for multi-dataset)...\n",
      "ğŸ’¾ Disk space at startup: 76.1GB free, 69.4% used\n",
      "ğŸš€ GoEmotions DeBERTa Training (SCIENTIFIC VERSION)\n",
      "============================================================\n",
      "ğŸ“ Output directory: checkpoints_comprehensive_multidataset\n",
      "ğŸ¤– Model: deberta-v3-large (from local cache)\n",
      "ğŸ“Š Dataset: GoEmotions (from local cache)\n",
      "ğŸ”¬ Scientific logging: ENABLED\n",
      "ğŸ¤– Loading deberta-v3-large...\n",
      "ğŸ“ Found local cache at models/deberta-v3-large\n",
      "âœ… deberta-v3-large tokenizer loaded from local cache\n",
      "âœ… deberta-v3-large model loaded from local cache\n",
      "ğŸ“Š Loading combined multi-dataset...\n",
      "âœ… Found combined multi-dataset!\n",
      "   Training examples: 43409\n",
      "   Validation examples: 7234\n",
      "   Total: 50643 samples\n",
      "   Datasets: GoEmotions + SemEval + ISEAR + MELD\n",
      "ğŸ”„ Creating datasets...\n",
      "âœ… Created 43409 training examples\n",
      "âœ… Created 7234 validation examples\n",
      "ğŸ”§ Disabling gradient checkpointing to prevent RuntimeError during backward pass\n",
      "ğŸš€ Using Combined Loss (ASL + Class Weighting + Focal Loss) for maximum performance\n",
      "ğŸ“Š Loss combination ratio: 0.7 ASL + 0.30000000000000004 Focal\n",
      "WARNING:accelerate.utils.other:Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "[2025-09-14 13:38:39,122] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-09-14 13:38:40,760] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\n",
      "ğŸ“Š Class weights computed: tensor([ 0.3754,  0.6660,  0.9894,  0.6277,  0.5275,  1.4263,  1.1333,  0.7076,\n",
      "         2.4187,  1.2217,  0.7667,  1.9551,  5.1167,  1.8175,  2.6013,  0.5824,\n",
      "        20.1345,  1.0677,  0.7432,  9.4534,  0.9806, 13.9672,  1.3967, 10.1331,\n",
      "         2.8447,  1.1692,  1.4626,  0.1090])\n",
      "ğŸ¯ Loss combination: 0.7 ASL + 0.30000000000000004 Focal\n",
      "ğŸ“Š Rare classes identified: [16, 21, 23, 19, 12, 24, 14, 8, 11, 13, 26, 5, 22, 9] (threshold: 1326 samples)\n",
      "ğŸ“ˆ Oversampled class grief: 77 â†’ 115\n",
      "ğŸ“ˆ Oversampled class pride: 111 â†’ 166\n",
      "ğŸ“ˆ Oversampled class relief: 153 â†’ 229\n",
      "ğŸ“ˆ Oversampled class nervousness: 164 â†’ 246\n",
      "ğŸ“ˆ Oversampled class embarrassment: 303 â†’ 454\n",
      "ğŸ“ˆ Oversampled class remorse: 545 â†’ 817\n",
      "ğŸ“ˆ Oversampled class fear: 596 â†’ 894\n",
      "ğŸ“ˆ Oversampled class desire: 641 â†’ 961\n",
      "ğŸ“ˆ Oversampled class disgust: 793 â†’ 1189\n",
      "ğŸ“ˆ Oversampled class excitement: 853 â†’ 1279\n",
      "ğŸ“ˆ Oversampled class surprise: 1060 â†’ 1590\n",
      "ğŸ“ˆ Oversampled class caring: 1087 â†’ 1630\n",
      "ğŸ“ˆ Oversampled class realization: 1110 â†’ 1665\n",
      "ğŸ“ˆ Oversampled class disappointment: 1269 â†’ 1903\n",
      "âœ… Stratified oversampling applied: 43410 â†’ 47786 samples\n",
      "âœ… Oversampling applied for rare classes\n",
      "ğŸ”„ Creating oversampled training dataset...\n",
      "âœ… Oversampled training dataset: 47786 examples\n",
      "ğŸš€ Starting training...\n",
      "  0%|          | 0/8961 [00:00<?, ?it/s]/venv/deberta-v3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2752: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "{'loss': 1.5382, 'grad_norm': 7.62939453125e-05, 'learning_rate': 1.6387959866220736e-06, 'epoch': 0.02}\n",
      "{'loss': 1.4016, 'grad_norm': 7.629393803654239e-05, 'learning_rate': 3.311036789297659e-06, 'epoch': 0.03}\n",
      "{'loss': 1.0895, 'grad_norm': 7.629395258845761e-05, 'learning_rate': 4.983277591973244e-06, 'epoch': 0.05}\n",
      "{'loss': 0.7757, 'grad_norm': 7.62939453125e-05, 'learning_rate': 6.65551839464883e-06, 'epoch': 0.07}\n",
      "{'loss': 0.5647, 'grad_norm': 7.629395986441523e-05, 'learning_rate': 8.327759197324414e-06, 'epoch': 0.08}\n",
      "{'loss': 0.4846, 'grad_norm': 7.629393803654239e-05, 'learning_rate': 9.999999999999999e-06, 'epoch': 0.1}\n",
      "{'loss': 0.4554, 'grad_norm': 0.000152587890625, 'learning_rate': 1.1638795986622074e-05, 'epoch': 0.12}\n",
      "  4%|â–         | 398/8961 [03:37<1:15:32,  1.89it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 48\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Run training\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m training_result \u001b[38;5;241m=\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbash\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mscripts/train_comprehensive_multidataset.sh\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mcapture_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Check results\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m50\u001b[39m)\n",
      "File \u001b[0;32m/venv/deberta-v3/lib/python3.10/subprocess.py:505\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Popen(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 505\u001b[0m         stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    507\u001b[0m         process\u001b[38;5;241m.\u001b[39mkill()\n",
      "File \u001b[0;32m/venv/deberta-v3/lib/python3.10/subprocess.py:1146\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1144\u001b[0m         stderr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1145\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1146\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/venv/deberta-v3/lib/python3.10/subprocess.py:1209\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1207\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m _time() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1211\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m     \u001b[38;5;66;03m# The first keyboard interrupt waits briefly for the child to\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m     \u001b[38;5;66;03m# exit under the common assumption that it also received the ^C\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m     \u001b[38;5;66;03m# generated SIGINT and will exit rapidly.\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/venv/deberta-v3/lib/python3.10/subprocess.py:1959\u001b[0m, in \u001b[0;36mPopen._wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1958\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# Another thread waited.\u001b[39;00m\n\u001b[0;32m-> 1959\u001b[0m (pid, sts) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1960\u001b[0m \u001b[38;5;66;03m# Check the pid and loop as waitpid has been known to\u001b[39;00m\n\u001b[1;32m   1961\u001b[0m \u001b[38;5;66;03m# return 0 even without WNOHANG in odd situations.\u001b[39;00m\n\u001b[1;32m   1962\u001b[0m \u001b[38;5;66;03m# http://bugs.python.org/issue14396.\u001b[39;00m\n\u001b[1;32m   1963\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pid \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid:\n",
      "File \u001b[0;32m/venv/deberta-v3/lib/python3.10/subprocess.py:1917\u001b[0m, in \u001b[0;36mPopen._try_wait\u001b[0;34m(self, wait_flags)\u001b[0m\n\u001b[1;32m   1915\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001b[39;00m\n\u001b[1;32m   1916\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1917\u001b[0m     (pid, sts) \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitpid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait_flags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1918\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mChildProcessError\u001b[39;00m:\n\u001b[1;32m   1919\u001b[0m     \u001b[38;5;66;03m# This happens if SIGCLD is set to be ignored or waiting\u001b[39;00m\n\u001b[1;32m   1920\u001b[0m     \u001b[38;5;66;03m# for child processes has otherwise been disabled for our\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m     \u001b[38;5;66;03m# process.  This child is dead, we can't get the status.\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m     pid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# MULTI-DATASET TRAINING\n",
    "# Trains DeBERTa on combined dataset with Asymmetric Loss\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"ğŸš€ MULTI-DATASET TRAINING\")\n",
    "print(\"=\" * 50)\n",
    "print(\"ğŸ¤– Model: DeBERTa-v3-large\")\n",
    "print(\"ğŸ“Š Data: 38K+ samples (GoEmotions + SemEval + ISEAR + MELD)\")\n",
    "print(\"ğŸ¯ Loss: BCE (your proven 51.79% winner)\")\n",
    "print(\"â±ï¸ Time: ~6-8 hours (5 epochs on larger dataset)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Change to project directory\n",
    "os.chdir('/home/user/goemotions-deberta')\n",
    "\n",
    "# Verify prerequisites\n",
    "checks_passed = True\n",
    "\n",
    "if not os.path.exists('data/combined_all_datasets/train.jsonl'):\n",
    "    print(\"âŒ Dataset not found - run Cell 2 first\")\n",
    "    checks_passed = False\n",
    "\n",
    "if not os.path.exists('scripts/train_comprehensive_multidataset.sh'):\n",
    "    print(\"âŒ Training script not found\")\n",
    "    checks_passed = False\n",
    "\n",
    "if not checks_passed:\n",
    "    print(\"\\nğŸ’¡ Please run Cell 2 first to prepare data\")\n",
    "    exit()\n",
    "\n",
    "# Make script executable\n",
    "os.chmod('scripts/train_comprehensive_multidataset.sh', 0o755)\n",
    "print(\"âœ… Training script ready\")\n",
    "\n",
    "# Start training\n",
    "print(\"\\nğŸš€ STARTING TRAINING...\")\n",
    "print(\"ğŸ“Š Monitor progress: tail -f logs/train_comprehensive_multidataset.log\")\n",
    "print(\"ğŸ“Š Results will be in: checkpoints_comprehensive_multidataset/eval_report.json\")\n",
    "print(\"â˜ï¸ Google Drive backup: Automatic (every 15 minutes during training)\")\n",
    "print(\"\\nâš ï¸ This will take 6-8 hours. Training runs with VISIBLE progress!\")\n",
    "print(\"âš ï¸ DO NOT close this notebook - you'll see live progress bars!\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Run training\n",
    "training_result = subprocess.run(['bash', 'scripts/train_comprehensive_multidataset.sh'], \n",
    "                                capture_output=False, text=True)\n",
    "\n",
    "# Check results\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "if os.path.exists('checkpoints_comprehensive_multidataset/eval_report.json'):\n",
    "    print(\"âœ… TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"ğŸ“Š Results available locally: checkpoints_comprehensive_multidataset/eval_report.json\")\n",
    "    print(\"â˜ï¸ Google Drive backup: Completed automatically during training\")\n",
    "    \n",
    "    # Try to show F1 scores\n",
    "    try:\n",
    "        import json\n",
    "        with open('checkpoints_comprehensive_multidataset/eval_report.json', 'r') as f:\n",
    "            results = json.load(f)\n",
    "        f1_macro = results.get('f1_macro', 'N/A')\n",
    "        f1_micro = results.get('f1_micro', 'N/A')\n",
    "        print(f\"\\nğŸ“ˆ PERFORMANCE:\")\n",
    "        print(f\"   F1 Macro: {f1_macro}\")\n",
    "        print(f\"   F1 Micro: {f1_micro}\")\n",
    "        if f1_macro != 'N/A' and f1_macro > 0.6:\n",
    "            print(\"\\nğŸ‰ SUCCESS: Achieved >60% F1-macro target!\")\n",
    "        elif f1_macro != 'N/A' and f1_macro > 0.55:\n",
    "            print(\"\\nğŸ‘ GOOD: Achieved >55% F1-macro!\")\n",
    "    except:\n",
    "        print(\"ğŸ“Š Check eval_report.json for detailed results\")\n",
    "        \n",
    "else:\n",
    "    print(\"âš ï¸ TRAINING MAY HAVE FAILED OR IS STILL RUNNING\")\n",
    "    print(\"ğŸ“Š Check logs: tail -f logs/train_comprehensive_multidataset.log\")\n",
    "    print(\"ğŸ“Š Check for results: checkpoints_comprehensive_multidataset/eval_report.json\")\n",
    "\n",
    "print(\"\\nğŸ¯ Target: >60% F1-macro\")\n",
    "print(\"ğŸ† Baseline: 51.79% F1-macro (GoEmotions only)\")\n",
    "print(\"â˜ï¸ Backup: Automatic Google Drive (timestamped folders)\")"
   ]
>>>>>>> 8beb01759a2a4cf387ad8c6ca2700757edd20f0e
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š **STEP 3: Results Analysis (Optional)**\n",
    "### **ğŸ¯ COMPARE WITH BASELINE**\n",
    "Analyze performance improvement from multi-dataset training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESULTS ANALYSIS\n",
    "# Compare multi-dataset performance with baseline\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"ğŸ“Š MULTI-DATASET RESULTS ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Baseline from original GoEmotions training\n",
    "baseline = {\n",
    "    'f1_macro': 0.5179,\n",
    "    'f1_micro': 0.5975,\n",
    "    'model': 'BCE Extended (GoEmotions only)'\n",
    "}\n",
    "\n",
    "print(\"ğŸ† BASELINE PERFORMANCE:\")\n",
    "print(f\"   F1 Macro: {baseline['f1_macro']:.4f} ({baseline['f1_macro']*100:.1f}%)\")\n",
    "print(f\"   F1 Micro: {baseline['f1_micro']:.4f} ({baseline['f1_micro']*100:.1f}%)\")\n",
    "print(f\"   Model: {baseline['model']}\")\n",
    "\n",
    "# Load current results\n",
    "eval_file = Path(\"checkpoints_comprehensive_multidataset/eval_report.json\")\n",
    "\n",
    "if eval_file.exists():\n",
    "    with open(eval_file, 'r') as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    current_f1_macro = results.get('f1_macro', 0)\n",
    "    current_f1_micro = results.get('f1_micro', 0)\n",
    "    \n",
    "    print(\"\\nğŸ¯ MULTI-DATASET RESULTS:\")\n",
    "    print(f\"   F1 Macro: {current_f1_macro:.4f} ({current_f1_macro*100:.1f}%)\")\n",
    "    print(f\"   F1 Micro: {current_f1_micro:.4f} ({current_f1_micro*100:.1f}%)\")\n",
    "    \n",
    "    # Calculate improvement\n",
    "    improvement = ((current_f1_macro - baseline['f1_macro']) / baseline['f1_macro']) * 100\n",
    "    \n",
    "    print(f\"\\nğŸ“ˆ IMPROVEMENT: {improvement:+.1f}%\")\n",
    "    \n",
    "    # Success assessment\n",
    "    if current_f1_macro >= 0.60:\n",
    "        print(\"ğŸš€ EXCELLENT: Achieved >60% F1-macro target!\")\n",
    "        print(\"ğŸ‰ Multi-dataset training SUCCESSFUL!\")\n",
    "    elif current_f1_macro >= 0.55:\n",
    "        print(\"âœ… GOOD: Achieved >55% F1-macro!\")\n",
    "        print(\"ğŸ“ˆ Significant improvement from multi-dataset approach\")\n",
    "    elif current_f1_macro > baseline['f1_macro']:\n",
    "        print(\"ğŸ‘ IMPROVEMENT: Better than baseline\")\n",
    "        print(\"ğŸ”§ May need more training epochs or parameter tuning\")\n",
    "    else:\n",
    "        print(\"âš ï¸ NO IMPROVEMENT: Check data quality or training setup\")\n",
    "        \n",
    "    print(f\"\\nğŸ“Š TARGET ACHIEVEMENT:\")\n",
    "    print(f\"   >60% F1-macro: {'âœ…' if current_f1_macro >= 0.60 else 'âŒ'} (Target: 60%+)\")\n",
    "    print(f\"   >55% F1-macro: {'âœ…' if current_f1_macro >= 0.55 else 'âŒ'} (Target: 55%+)\")\n",
    "    print(f\"   Beat baseline: {'âœ…' if current_f1_macro > baseline['f1_macro'] else 'âŒ'}\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\nâ³ RESULTS NOT AVAILABLE\")\n",
    "    print(\"ğŸ”§ Training may still be in progress or check file path\")\n",
    "    print(\"ğŸ“ Expected: checkpoints_comprehensive_multidataset/eval_report.json\")\n",
    "\n",
    "print(\"\\nğŸ” MONITORING COMMANDS:\")\n",
    "print(\"   Training logs: tail -f logs/train_comprehensive_multidataset.log\")\n",
    "print(\"   GPU status: watch -n 5 'nvidia-smi'\")\n",
    "print(\"   Process status: ps aux | grep train_deberta\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}