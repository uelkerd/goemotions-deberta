{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c38492c",
   "metadata": {},
   "source": [
    "# GoEmotions DeBERTa-v3-large SIMPLIFIED PARALLEL Workflow\n",
    "\n",
    "## Sequential + Dual-GPU Parallel Training\n",
    "\n",
    "**GOAL**: Achieve >50% F1 macro at threshold=0.2 with class imbalance fixes\n",
    "\n",
    "**KEY FEATURES**:\n",
    "- Phase 1: Sequential single-GPU for stability (5 configs: BCE, Asymmetric, Combined 0.7/0.5/0.3)\n",
    "- Phase 1.5: Parallel dual-GPU for speed (same configs on GPU 0 & 1 concurrently)\n",
    "- Fixed: differentiable losses, per-class pos_weight, oversampling, threshold=0.2, LR=3e-5\n",
    "- Expected: 50-65% F1 macro, 50% faster with parallel\n",
    "\n",
    "**Baseline**: 42.18% F1 (original notebook line 1405), target >50% at threshold=0.2\n",
    "\n",
    "**Workflow**: Environment ‚Üí Cache ‚Üí Phase 1 Sequential ‚Üí Phase 1.5 Parallel ‚Üí Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c0f988",
   "metadata": {},
   "source": [
    "# ENVIRONMENT VERIFICATION - RUN FIRST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48d6bc6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Verifying Conda Environment...\n",
      "Python: /venv/deberta-v3/bin/python, Version: 3.10.18 | packaged by conda-forge | (main, Jun  4 2025, 14:45:41) [GCC 13.3.0]\n",
      "Conda env: None\n",
      "‚ö†Ô∏è Switch to 'Python (deberta-v3)' kernel\n",
      "PyTorch 2.7.1+cu118, CUDA: True, Devices: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/deberta-v3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers 4.56.0\n",
      "\n",
      "üéØ Environment ready! Run !nvidia-smi for GPU check\n",
      "Mon Sep  8 21:36:15 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        On  |   00000000:C1:00.0 Off |                  N/A |\n",
      "| 30%   52C    P2            306W /  350W |   11035MiB /  24576MiB |     64%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 3090        On  |   00000000:C2:00.0 Off |                  N/A |\n",
      "| 30%   25C    P8             40W /  350W |       4MiB /  24576MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "print(\"üîç Verifying Conda Environment...\")\n",
    "import sys, os\n",
    "print(f\"Python: {sys.executable}, Version: {sys.version}\")\n",
    "conda_env = os.environ.get('CONDA_DEFAULT_ENV', 'None')\n",
    "print(f\"Conda env: {conda_env}\")\n",
    "if conda_env != 'deberta-v3':\n",
    "    print(\"‚ö†Ô∏è Switch to 'Python (deberta-v3)' kernel\")\n",
    "# Check packages\n",
    "try:\n",
    "    import torch; print(f\"PyTorch {torch.__version__}, CUDA: {torch.cuda.is_available()}, Devices: {torch.cuda.device_count()}\")\n",
    "except: print(\"‚ùå PyTorch missing\")\n",
    "try:\n",
    "    import transformers; print(f\"Transformers {transformers.__version__}\")\n",
    "except: print(\"‚ùå Transformers missing\")\n",
    "print(\"\\nüéØ Environment ready! Run !nvidia-smi for GPU check\")\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d489db09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Setup environment...\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "build-essential is already the newest version (12.9ubuntu3).\n",
      "libgoogle-perftools-dev is already the newest version (2.9.1-0ubuntu3).\n",
      "pkg-config is already the newest version (0.29.2-1ubuntu3).\n",
      "cmake is already the newest version (3.22.1-1ubuntu1.22.04.2).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 75 not upgraded.\n",
      "Requirement already satisfied: sentencepiece in /venv/deberta-v3/lib/python3.10/site-packages (0.2.1)\n",
      "Requirement already satisfied: transformers in /venv/deberta-v3/lib/python3.10/site-packages (4.56.0)\n",
      "Requirement already satisfied: accelerate in /venv/deberta-v3/lib/python3.10/site-packages (1.10.1)\n",
      "Requirement already satisfied: datasets in /venv/deberta-v3/lib/python3.10/site-packages (4.0.0)\n",
      "Requirement already satisfied: evaluate in /venv/deberta-v3/lib/python3.10/site-packages (0.4.5)\n",
      "Requirement already satisfied: scikit-learn in /venv/deberta-v3/lib/python3.10/site-packages (1.7.1)\n",
      "Requirement already satisfied: tensorboard in /venv/deberta-v3/lib/python3.10/site-packages (2.20.0)\n",
      "Requirement already satisfied: pyarrow in /venv/deberta-v3/lib/python3.10/site-packages (21.0.0)\n",
      "Requirement already satisfied: tiktoken in /venv/deberta-v3/lib/python3.10/site-packages (0.11.0)\n",
      "Requirement already satisfied: filelock in /venv/deberta-v3/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /venv/deberta-v3/lib/python3.10/site-packages (from transformers) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /venv/deberta-v3/lib/python3.10/site-packages (from transformers) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /venv/deberta-v3/lib/python3.10/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /venv/deberta-v3/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /venv/deberta-v3/lib/python3.10/site-packages (from transformers) (2025.9.1)\n",
      "Requirement already satisfied: requests in /venv/deberta-v3/lib/python3.10/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /venv/deberta-v3/lib/python3.10/site-packages (from transformers) (0.22.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /venv/deberta-v3/lib/python3.10/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /venv/deberta-v3/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /venv/deberta-v3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /venv/deberta-v3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /venv/deberta-v3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)\n",
      "Requirement already satisfied: psutil in /venv/deberta-v3/lib/python3.10/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /venv/deberta-v3/lib/python3.10/site-packages (from accelerate) (2.7.1+cu118)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /venv/deberta-v3/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /venv/deberta-v3/lib/python3.10/site-packages (from datasets) (2.3.2)\n",
      "Requirement already satisfied: xxhash in /venv/deberta-v3/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /venv/deberta-v3/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /venv/deberta-v3/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /venv/deberta-v3/lib/python3.10/site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /venv/deberta-v3/lib/python3.10/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /venv/deberta-v3/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /venv/deberta-v3/lib/python3.10/site-packages (from tensorboard) (2.3.1)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /venv/deberta-v3/lib/python3.10/site-packages (from tensorboard) (1.74.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /venv/deberta-v3/lib/python3.10/site-packages (from tensorboard) (3.8.2)\n",
      "Requirement already satisfied: pillow in /venv/deberta-v3/lib/python3.10/site-packages (from tensorboard) (11.0.0)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /venv/deberta-v3/lib/python3.10/site-packages (from tensorboard) (6.32.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /venv/deberta-v3/lib/python3.10/site-packages (from tensorboard) (80.9.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /venv/deberta-v3/lib/python3.10/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /venv/deberta-v3/lib/python3.10/site-packages (from tensorboard) (3.1.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /venv/deberta-v3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /venv/deberta-v3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /venv/deberta-v3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /venv/deberta-v3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /venv/deberta-v3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /venv/deberta-v3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /venv/deberta-v3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /venv/deberta-v3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in /venv/deberta-v3/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /venv/deberta-v3/lib/python3.10/site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /venv/deberta-v3/lib/python3.10/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /venv/deberta-v3/lib/python3.10/site-packages (from requests->transformers) (2025.8.3)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /venv/deberta-v3/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: networkx in /venv/deberta-v3/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in /venv/deberta-v3/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /venv/deberta-v3/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /venv/deberta-v3/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /venv/deberta-v3/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /venv/deberta-v3/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /venv/deberta-v3/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /venv/deberta-v3/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /venv/deberta-v3/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /venv/deberta-v3/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /venv/deberta-v3/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /venv/deberta-v3/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /venv/deberta-v3/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (11.8.86)\n",
      "Requirement already satisfied: triton==3.3.1 in /venv/deberta-v3/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.3.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /venv/deberta-v3/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /venv/deberta-v3/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /venv/deberta-v3/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /venv/deberta-v3/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /venv/deberta-v3/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /venv/deberta-v3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Working dir: /home/user/goemotions-deberta\n"
     ]
    }
   ],
   "source": [
    "print(\"üîß Setup environment...\")\n",
    "!apt-get update -qq && apt-get install -y cmake build-essential pkg-config libgoogle-perftools-dev\n",
    "!pip install --upgrade pip torch>=2.6.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 --root-user-action=ignore\n",
    "!pip install sentencepiece transformers accelerate datasets evaluate scikit-learn tensorboard pyarrow tiktoken --root-user-action=ignore\n",
    "os.chdir('/home/user/goemotions-deberta')\n",
    "print(f\"Working dir: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4a8bdfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Setup cache...\n",
      "üöÄ Setting up local cache for GoEmotions DeBERTa project\n",
      "============================================================\n",
      "üìÅ Setting up directory structure...\n",
      "‚úÖ Created: data/goemotions\n",
      "‚úÖ Created: models/deberta-v3-large\n",
      "‚úÖ Created: models/roberta-large\n",
      "‚úÖ Created: outputs/deberta\n",
      "‚úÖ Created: outputs/roberta\n",
      "‚úÖ Created: logs\n",
      "\n",
      "üìä Caching GoEmotions dataset...\n",
      "‚úÖ GoEmotions dataset already cached\n",
      "\n",
      "ü§ñ Caching DeBERTa-v3-large model...\n",
      "‚úÖ DeBERTa-v3-large model already cached\n",
      "\n",
      "üéâ Local cache setup completed successfully!\n",
      "üìÅ All models and datasets are now cached locally\n",
      "üöÄ Ready for fast training without internet dependency\n",
      "total 1702052\n",
      "drwxrwxr-x 2 root root        173 Sep  3 11:50 .\n",
      "drwxrwxr-x 4 root root         51 Sep  3 11:39 ..\n",
      "total 5540\n",
      "drwxrwxr-x 2 root root      63 Sep  3 11:39 .\n",
      "drwxrwxr-x 3 root root      24 Sep  3 11:39 ..\n"
     ]
    }
   ],
   "source": [
    "print(\"üöÄ Setup cache...\")\n",
    "!python3 notebooks/scripts/setup_local_cache.py\n",
    "!ls -la models/deberta-v3-large/ | head -3\n",
    "!ls -la data/goemotions/ | head -3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d00bfb",
   "metadata": {},
   "source": [
    "## PHASE 1.5: PARALLEL DUAL-GPU TRAINING\n",
    "\n",
    "**Run the same 5 configs in parallel on GPU 0 & 1 for 50% faster execution.**\n",
    "- Pairs: (BCE + Asymmetric), (Combined 0.7 + 0.5), (Combined 0.3 alone)\n",
    "- Uses threading + subprocess with CUDA_VISIBLE_DEVICES isolation\n",
    "- Duration: ~1-1.5 hours (vs 2-3 hours sequential)\n",
    "- Monitor: !tail -f gpu*.log & !nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ae5d10",
   "metadata": {},
   "source": [
    "# PHASE 1.5: PARALLEL DUAL-GPU IMPLEMENTATION WITH VERBOSE LIVE MONITORING (FIXED ARGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edf1bf1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ PHASE 1.5: Parallel Dual-GPU Training - 5 Configs WITH LIVE MONITORING (Fixed Args)\n",
      "======================================================================\n",
      "GPU 0 & 1 concurrent: 50% faster than sequential\n",
      "Fixes maintained: pos_weight, oversampling, threshold=0.2\n",
      "LIVE: Loss/grad_norm/f1_macro/epoch printed in real-time + GPU status every 30s\n",
      "FIXED: Removed unrecognized args --save_total_limit, --overwrite_output_dir\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import subprocess, threading, os, time, queue\n",
    "from datetime import datetime\n",
    "\n",
    "# Kill any existing processes\n",
    "subprocess.run(['pkill', '-f', 'train_deberta_local'], capture_output=True)\n",
    "time.sleep(2)\n",
    "\n",
    "\n",
    "print(\"üöÄ PHASE 1.5: Parallel Dual-GPU Training - 5 Configs WITH LIVE MONITORING (Fixed Args)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"GPU 0 & 1 concurrent: 50% faster than sequential\")\n",
    "print(\"Fixes maintained: pos_weight, oversampling, threshold=0.2\")\n",
    "print(\"LIVE: Loss/grad_norm/f1_macro/epoch printed in real-time + GPU status every 30s\")\n",
    "print(\"FIXED: Removed unrecognized args --save_total_limit, --overwrite_output_dir\")\n",
    "print(\"=\" * 70)\n",
    "def run_config_live(gpu_id, config_name, use_asym=False, ratio=None):\n",
    "    \"\"\"Run training on specific GPU with live monitoring (non-blocking, tail-like output, valid args only)\"\"\"\n",
    "    print(f\"üöÄ LIVE MONITOR: Starting {config_name} on GPU {gpu_id} at {datetime.now()}\")\n",
    "    env = os.environ.copy()\n",
    "    env['CUDA_VISIBLE_DEVICES'] = str(gpu_id)\n",
    "    \n",
    "    # Valid args only (from script analysis: core training params + save_total_limit for quota safety)\n",
    "    # Removed for HF compatibility; use script quota check\n",
    "    # '--save_total_limit', '1'\n",
    "    cmd = [\n",
    "        'python3', 'notebooks/scripts/train_deberta_local.py',\n",
    "        '--output_dir', f'./outputs/parallel_{config_name}',\n",
    "        '--model_type', 'deberta-v3-large',\n",
    "        '--per_device_train_batch_size', '4',\n",
    "        '--per_device_eval_batch_size', '8',\n",
    "        '--gradient_accumulation_steps', '4',\n",
    "        '--num_train_epochs', '2',\n",
    "        '--learning_rate', '3e-5',\n",
    "        '--lr_scheduler_type', 'cosine',\n",
    "        '--warmup_ratio', '0.15',\n",
    "        '--weight_decay', '0.01',\n",
    "        '--fp16',\n",
    "        '--max_length', '256',\n",
    "        '--max_train_samples', '20000',\n",
    "        '--max_eval_samples', '3000'\n",
    "    ]\n",
    "    if use_asym: \n",
    "        cmd += ['--use_asymmetric_loss']\n",
    "    if ratio is not None: \n",
    "        cmd += ['--use_combined_loss', '--loss_combination_ratio', str(ratio)]\n",
    "    \n",
    "    print(f\"Command for {config_name}: {' '.join(cmd)}\")  # Debug: Print cmd to verify args\n",
    "    \n",
    "    # Non-blocking Popen for live output\n",
    "    try:\n",
    "        proc = subprocess.Popen(cmd, env=env, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, \n",
    "                                cwd='/home/user/goemotions-deberta', universal_newlines=True, bufsize=1)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR starting {config_name} on GPU {gpu_id}: {e}\")\n",
    "        return 1\n",
    "    \n",
    "    output_queue = queue.Queue()\n",
    "    \n",
    "    def read_output():\n",
    "        \"\"\"Thread to read stdout line-by-line and queue/print live\"\"\"\n",
    "        for line in iter(proc.stdout.readline, ''):\n",
    "            output_queue.put(line)\n",
    "            # Live print ALL lines for full verbosity (or filter if too noisy)\n",
    "            print(f\"[{datetime.now().strftime('%H:%M:%S')}] GPU {gpu_id} [{config_name}]: {line.strip()}\")\n",
    "    \n",
    "    output_thread = threading.Thread(target=read_output, daemon=True)\n",
    "    output_thread.start()\n",
    "    \n",
    "    def monitor_live():\n",
    "        \"\"\"Live monitoring: GPU status every 30s while running\"\"\"\n",
    "        while proc.poll() is None:\n",
    "            time.sleep(30)\n",
    "            try:\n",
    "                gpu_result = subprocess.run(['nvidia-smi', '--query-gpu=index,utilization.gpu,memory.used,memory.total,temperature.gpu', \n",
    "                                             '--format=csv,noheader,nounits'], capture_output=True, text=True)\n",
    "                print(f\"[{datetime.now().strftime('%H:%M:%S')}] üñ•Ô∏è GPU STATUS (overall): {gpu_result.stdout.strip()}\")\n",
    "            except Exception as e:\n",
    "                print(f\"[{datetime.now().strftime('%H:%M:%S')}] üñ•Ô∏è GPU STATUS ERROR: {e}\")\n",
    "    \n",
    "    monitor_thread = threading.Thread(target=monitor_live, daemon=True)\n",
    "    monitor_thread.start()\n",
    "    \n",
    "    # Wait for completion\n",
    "    return_code = proc.wait()\n",
    "    output_thread.join(timeout=5)\n",
    "    monitor_thread.join(timeout=5)\n",
    "    \n",
    "    # Drain queue and print final lines\n",
    "    final_lines = []\n",
    "    try:\n",
    "        while True:\n",
    "            line = output_queue.get_nowait()\n",
    "            final_lines.append(line)\n",
    "    except queue.Empty:\n",
    "        pass\n",
    "    if final_lines:\n",
    "        print(f\"[{datetime.now().strftime('%H:%M:%S')}] üìã FINAL LINES ({config_name}, last 10):\")\n",
    "        for line in final_lines[-10:]:\n",
    "            print(f\"    {line.strip()}\")\n",
    "    \n",
    "    print(f\"‚úÖ {config_name} complete on GPU {gpu_id} (return code: {return_code}) at {datetime.now()}\")\n",
    "    return return_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9ffd56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìç PAIR 1 LIVE: BCE (GPU0) + Asymmetric (GPU1)\n",
      "üöÄ LIVE MONITOR: Starting BCE_Parallel on GPU 0 at 2025-09-08 21:36:32.828865\n",
      "Command for BCE_Parallel: python3 notebooks/scripts/train_deberta_local.py --output_dir ./outputs/parallel_BCE_Parallel --model_type deberta-v3-large --per_device_train_batch_size 4 --per_device_eval_batch_size 8 --gradient_accumulation_steps 4 --num_train_epochs 2 --learning_rate 3e-5 --lr_scheduler_type cosine --warmup_ratio 0.15 --weight_decay 0.01 --fp16 --max_length 256 --max_train_samples 20000 --max_eval_samples 3000\n",
      "üöÄ LIVE MONITOR: Starting Asymmetric_Parallel on GPU 1 at 2025-09-08 21:36:32.830831\n",
      "Command for Asymmetric_Parallel: python3 notebooks/scripts/train_deberta_local.py --output_dir ./outputs/parallel_Asymmetric_Parallel --model_type deberta-v3-large --per_device_train_batch_size 4 --per_device_eval_batch_size 8 --gradient_accumulation_steps 4 --num_train_epochs 2 --learning_rate 3e-5 --lr_scheduler_type cosine --warmup_ratio 0.15 --weight_decay 0.01 --fp16 --max_length 256 --max_train_samples 20000 --max_eval_samples 3000 --use_asymmetric_loss\n",
      "[21:36:37] GPU 0 [BCE_Parallel]: üöÄ GoEmotions DeBERTa Training (SCIENTIFIC VERSION)\n",
      "[21:36:37] GPU 0 [BCE_Parallel]: ============================================================\n",
      "[21:36:37] GPU 0 [BCE_Parallel]: üìÅ Output directory: ./outputs/parallel_BCE_Parallel\n",
      "[21:36:37] GPU 0 [BCE_Parallel]: ü§ñ Model: deberta-v3-large (from local cache)\n",
      "[21:36:37] GPU 0 [BCE_Parallel]: üìä Dataset: GoEmotions (from local cache)\n",
      "[21:36:37] GPU 0 [BCE_Parallel]: üî¨ Scientific logging: ENABLED\n",
      "[21:36:37] GPU 1 [Asymmetric_Parallel]: üöÄ GoEmotions DeBERTa Training (SCIENTIFIC VERSION)\n",
      "[21:36:37] GPU 1 [Asymmetric_Parallel]: ============================================================\n",
      "[21:36:37] GPU 1 [Asymmetric_Parallel]: üìÅ Output directory: ./outputs/parallel_Asymmetric_Parallel\n",
      "[21:36:37] GPU 1 [Asymmetric_Parallel]: ü§ñ Model: deberta-v3-large (from local cache)\n",
      "[21:36:37] GPU 1 [Asymmetric_Parallel]: üìä Dataset: GoEmotions (from local cache)\n",
      "[21:36:37] GPU 1 [Asymmetric_Parallel]: üî¨ Scientific logging: ENABLED\n",
      "[21:36:37] GPU 0 [BCE_Parallel]: ü§ñ Loading deberta-v3-large...\n",
      "[21:36:37] GPU 0 [BCE_Parallel]: üìÅ Found local cache at models/deberta-v3-large\n",
      "[21:36:37] GPU 1 [Asymmetric_Parallel]: ü§ñ Loading deberta-v3-large...\n",
      "[21:36:37] GPU 1 [Asymmetric_Parallel]: üìÅ Found local cache at models/deberta-v3-large\n",
      "[21:36:38] GPU 0 [BCE_Parallel]: ‚úÖ deberta-v3-large tokenizer loaded from local cache\n",
      "[21:36:38] GPU 1 [Asymmetric_Parallel]: ‚úÖ deberta-v3-large tokenizer loaded from local cache\n",
      "[21:36:38] GPU 0 [BCE_Parallel]: ‚úÖ deberta-v3-large model loaded from local cache\n",
      "[21:36:38] GPU 0 [BCE_Parallel]: üìä Loading GoEmotions dataset from local cache...\n",
      "[21:36:38] GPU 0 [BCE_Parallel]: ‚úÖ GoEmotions dataset loaded from local cache\n",
      "[21:36:38] GPU 0 [BCE_Parallel]: Training examples: 43410\n",
      "[21:36:38] GPU 0 [BCE_Parallel]: Validation examples: 5426\n",
      "[21:36:38] GPU 0 [BCE_Parallel]: Total emotions: 28\n",
      "[21:36:38] GPU 0 [BCE_Parallel]: üîÑ Creating datasets...\n",
      "[21:36:38] GPU 1 [Asymmetric_Parallel]: ‚úÖ deberta-v3-large model loaded from local cache\n",
      "[21:36:38] GPU 1 [Asymmetric_Parallel]: üìä Loading GoEmotions dataset from local cache...\n",
      "[21:36:38] GPU 1 [Asymmetric_Parallel]: ‚úÖ GoEmotions dataset loaded from local cache\n",
      "[21:36:38] GPU 1 [Asymmetric_Parallel]: Training examples: 43410\n",
      "[21:36:38] GPU 1 [Asymmetric_Parallel]: Validation examples: 5426\n",
      "[21:36:38] GPU 1 [Asymmetric_Parallel]: Total emotions: 28\n",
      "[21:36:38] GPU 1 [Asymmetric_Parallel]: üîÑ Creating datasets...\n",
      "[21:36:38] GPU 0 [BCE_Parallel]: ‚úÖ Created 43410 training examples\n",
      "[21:36:38] GPU 0 [BCE_Parallel]: ‚úÖ Created 5426 validation examples\n",
      "[21:36:38] GPU 0 [BCE_Parallel]: üîÑ Limiting training data: 43410 ‚Üí 20000 samples\n",
      "[21:36:38] GPU 0 [BCE_Parallel]: ‚úÖ Using 20000 training examples (subset for quick screening)\n",
      "[21:36:38] GPU 0 [BCE_Parallel]: üîÑ Limiting validation data: 5426 ‚Üí 3000 samples\n",
      "[21:36:38] GPU 0 [BCE_Parallel]: ‚úÖ Using 3000 validation examples (subset for quick screening)\n",
      "[21:36:38] GPU 0 [BCE_Parallel]: üîß Disabling gradient checkpointing to prevent RuntimeError during backward pass\n",
      "[21:36:38] GPU 1 [Asymmetric_Parallel]: ‚úÖ Created 43410 training examples\n",
      "[21:36:38] GPU 1 [Asymmetric_Parallel]: ‚úÖ Created 5426 validation examples\n",
      "[21:36:38] GPU 1 [Asymmetric_Parallel]: üîÑ Limiting training data: 43410 ‚Üí 20000 samples\n",
      "[21:36:38] GPU 1 [Asymmetric_Parallel]: ‚úÖ Using 20000 training examples (subset for quick screening)\n",
      "[21:36:38] GPU 1 [Asymmetric_Parallel]: üîÑ Limiting validation data: 5426 ‚Üí 3000 samples\n",
      "[21:36:38] GPU 1 [Asymmetric_Parallel]: ‚úÖ Using 3000 validation examples (subset for quick screening)\n",
      "[21:36:38] GPU 1 [Asymmetric_Parallel]: üîß Disabling gradient checkpointing to prevent RuntimeError during backward pass\n",
      "[21:36:38] GPU 0 [BCE_Parallel]: üìä Using standard BCE Loss\n",
      "[21:36:38] GPU 1 [Asymmetric_Parallel]: üéØ Using Asymmetric Loss for better class imbalance handling\n",
      "[21:36:38] GPU 0 [BCE_Parallel]: WARNING:accelerate.utils.other:Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "[21:36:38] GPU 1 [Asymmetric_Parallel]: WARNING:accelerate.utils.other:Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "[21:36:39] GPU 0 [BCE_Parallel]: Traceback (most recent call last):\n",
      "[21:36:39] GPU 0 [BCE_Parallel]: File \"/home/user/goemotions-deberta/notebooks/scripts/train_deberta_local.py\", line 944, in <module>\n",
      "[21:36:39] GPU 0 [BCE_Parallel]: main()\n",
      "[21:36:39] GPU 0 [BCE_Parallel]: File \"/home/user/goemotions-deberta/notebooks/scripts/train_deberta_local.py\", line 878, in main\n",
      "[21:36:39] GPU 0 [BCE_Parallel]: trainer = Trainer(\n",
      "[21:36:39] GPU 0 [BCE_Parallel]: File \"/venv/deberta-v3/lib/python3.10/site-packages/transformers/utils/deprecation.py\", line 172, in wrapped_func\n",
      "[21:36:39] GPU 0 [BCE_Parallel]: return func(*args, **kwargs)\n",
      "[21:36:39] GPU 0 [BCE_Parallel]: File \"/venv/deberta-v3/lib/python3.10/site-packages/transformers/trainer.py\", line 807, in __init__\n",
      "[21:36:39] GPU 0 [BCE_Parallel]: self.control = self.callback_handler.on_init_end(self.args, self.state, self.control)\n",
      "[21:36:39] GPU 0 [BCE_Parallel]: File \"/venv/deberta-v3/lib/python3.10/site-packages/transformers/trainer_callback.py\", line 502, in on_init_end\n",
      "[21:36:39] GPU 0 [BCE_Parallel]: return self.call_event(\"on_init_end\", args, state, control)\n",
      "[21:36:39] GPU 0 [BCE_Parallel]: File \"/venv/deberta-v3/lib/python3.10/site-packages/transformers/trainer_callback.py\", line 556, in call_event\n",
      "[21:36:39] GPU 0 [BCE_Parallel]: result = getattr(callback, event)(\n",
      "[21:36:39] GPU 0 [BCE_Parallel]: AttributeError: 'ProgressMonitorCallback' object has no attribute 'on_init_end'. Did you mean: 'on_step_end'?\n",
      "[21:36:39] GPU 1 [Asymmetric_Parallel]: Traceback (most recent call last):\n",
      "[21:36:39] GPU 1 [Asymmetric_Parallel]: File \"/home/user/goemotions-deberta/notebooks/scripts/train_deberta_local.py\", line 944, in <module>\n",
      "[21:36:39] GPU 1 [Asymmetric_Parallel]: main()\n",
      "[21:36:39] GPU 1 [Asymmetric_Parallel]: File \"/home/user/goemotions-deberta/notebooks/scripts/train_deberta_local.py\", line 867, in main\n",
      "[21:36:39] GPU 1 [Asymmetric_Parallel]: trainer = AsymmetricLossTrainer(\n",
      "[21:36:39] GPU 1 [Asymmetric_Parallel]: File \"/home/user/goemotions-deberta/notebooks/scripts/train_deberta_local.py\", line 412, in __init__\n",
      "[21:36:39] GPU 1 [Asymmetric_Parallel]: super().__init__(*args, **kwargs)\n",
      "[21:36:39] GPU 1 [Asymmetric_Parallel]: File \"/venv/deberta-v3/lib/python3.10/site-packages/transformers/utils/deprecation.py\", line 172, in wrapped_func\n",
      "[21:36:39] GPU 1 [Asymmetric_Parallel]: return func(*args, **kwargs)\n",
      "[21:36:39] GPU 1 [Asymmetric_Parallel]: File \"/venv/deberta-v3/lib/python3.10/site-packages/transformers/trainer.py\", line 807, in __init__\n",
      "[21:36:39] GPU 1 [Asymmetric_Parallel]: self.control = self.callback_handler.on_init_end(self.args, self.state, self.control)\n",
      "[21:36:39] GPU 1 [Asymmetric_Parallel]: File \"/venv/deberta-v3/lib/python3.10/site-packages/transformers/trainer_callback.py\", line 502, in on_init_end\n",
      "[21:36:39] GPU 1 [Asymmetric_Parallel]: return self.call_event(\"on_init_end\", args, state, control)\n",
      "[21:36:39] GPU 1 [Asymmetric_Parallel]: File \"/venv/deberta-v3/lib/python3.10/site-packages/transformers/trainer_callback.py\", line 556, in call_event\n",
      "[21:36:39] GPU 1 [Asymmetric_Parallel]: result = getattr(callback, event)(\n",
      "[21:36:39] GPU 1 [Asymmetric_Parallel]: AttributeError: 'ProgressMonitorCallback' object has no attribute 'on_init_end'. Did you mean: 'on_step_end'?\n",
      "[21:36:45] üìã FINAL LINES (BCE_Parallel, last 10):\n",
      "    trainer = Trainer(\n",
      "    File \"/venv/deberta-v3/lib/python3.10/site-packages/transformers/utils/deprecation.py\", line 172, in wrapped_func\n",
      "    return func(*args, **kwargs)\n",
      "    File \"/venv/deberta-v3/lib/python3.10/site-packages/transformers/trainer.py\", line 807, in __init__\n",
      "    self.control = self.callback_handler.on_init_end(self.args, self.state, self.control)\n",
      "    File \"/venv/deberta-v3/lib/python3.10/site-packages/transformers/trainer_callback.py\", line 502, in on_init_end\n",
      "    return self.call_event(\"on_init_end\", args, state, control)\n",
      "    File \"/venv/deberta-v3/lib/python3.10/site-packages/transformers/trainer_callback.py\", line 556, in call_event\n",
      "    result = getattr(callback, event)(\n",
      "    AttributeError: 'ProgressMonitorCallback' object has no attribute 'on_init_end'. Did you mean: 'on_step_end'?\n",
      "‚úÖ BCE_Parallel complete on GPU 0 (return code: 1) at 2025-09-08 21:36:45.194390\n",
      "[21:36:45] üìã FINAL LINES (Asymmetric_Parallel, last 10):\n",
      "    super().__init__(*args, **kwargs)\n",
      "    File \"/venv/deberta-v3/lib/python3.10/site-packages/transformers/utils/deprecation.py\", line 172, in wrapped_func\n",
      "    return func(*args, **kwargs)\n",
      "    File \"/venv/deberta-v3/lib/python3.10/site-packages/transformers/trainer.py\", line 807, in __init__\n",
      "    self.control = self.callback_handler.on_init_end(self.args, self.state, self.control)\n",
      "    File \"/venv/deberta-v3/lib/python3.10/site-packages/transformers/trainer_callback.py\", line 502, in on_init_end\n",
      "    return self.call_event(\"on_init_end\", args, state, control)\n",
      "    File \"/venv/deberta-v3/lib/python3.10/site-packages/transformers/trainer_callback.py\", line 556, in call_event\n",
      "    result = getattr(callback, event)(\n",
      "    AttributeError: 'ProgressMonitorCallback' object has no attribute 'on_init_end'. Did you mean: 'on_step_end'?\n",
      "‚úÖ Asymmetric_Parallel complete on GPU 1 (return code: 1) at 2025-09-08 21:36:45.226832\n",
      "\n",
      "üìç PAIR 2 LIVE: Combined 0.7 (GPU0) + 0.5 (GPU1)\n",
      "üöÄ LIVE MONITOR: Starting Combined_07_Parallel on GPU 0 at 2025-09-08 21:36:45.227647\n",
      "Command for Combined_07_Parallel: python3 notebooks/scripts/train_deberta_local.py --output_dir ./outputs/parallel_Combined_07_Parallel --model_type deberta-v3-large --per_device_train_batch_size 4 --per_device_eval_batch_size 8 --gradient_accumulation_steps 4 --num_train_epochs 2 --learning_rate 3e-5 --lr_scheduler_type cosine --warmup_ratio 0.15 --weight_decay 0.01 --fp16 --max_length 256 --max_train_samples 20000 --max_eval_samples 3000 --use_combined_loss --loss_combination_ratio 0.7\n",
      "üöÄ LIVE MONITOR: Starting Combined_05_Parallel on GPU 1 at 2025-09-08 21:36:45.228409\n",
      "Command for Combined_05_Parallel: python3 notebooks/scripts/train_deberta_local.py --output_dir ./outputs/parallel_Combined_05_Parallel --model_type deberta-v3-large --per_device_train_batch_size 4 --per_device_eval_batch_size 8 --gradient_accumulation_steps 4 --num_train_epochs 2 --learning_rate 3e-5 --lr_scheduler_type cosine --warmup_ratio 0.15 --weight_decay 0.01 --fp16 --max_length 256 --max_train_samples 20000 --max_eval_samples 3000 --use_combined_loss --loss_combination_ratio 0.5\n",
      "[21:36:50] GPU 1 [Combined_05_Parallel]: üöÄ GoEmotions DeBERTa Training (SCIENTIFIC VERSION)\n",
      "[21:36:50] GPU 1 [Combined_05_Parallel]: ============================================================\n",
      "[21:36:50] GPU 1 [Combined_05_Parallel]: üìÅ Output directory: ./outputs/parallel_Combined_05_Parallel\n",
      "[21:36:50] GPU 1 [Combined_05_Parallel]: ü§ñ Model: deberta-v3-large (from local cache)\n",
      "[21:36:50] GPU 1 [Combined_05_Parallel]: üìä Dataset: GoEmotions (from local cache)\n",
      "[21:36:50] GPU 1 [Combined_05_Parallel]: üî¨ Scientific logging: ENABLED\n",
      "[21:36:50] GPU 1 [Combined_05_Parallel]: ü§ñ Loading deberta-v3-large...\n",
      "[21:36:50] GPU 1 [Combined_05_Parallel]: üìÅ Found local cache at models/deberta-v3-large\n",
      "[21:36:50] GPU 0 [Combined_07_Parallel]: üöÄ GoEmotions DeBERTa Training (SCIENTIFIC VERSION)\n",
      "[21:36:50] GPU 0 [Combined_07_Parallel]: ============================================================\n",
      "[21:36:50] GPU 0 [Combined_07_Parallel]: üìÅ Output directory: ./outputs/parallel_Combined_07_Parallel\n",
      "[21:36:50] GPU 0 [Combined_07_Parallel]: ü§ñ Model: deberta-v3-large (from local cache)\n",
      "[21:36:50] GPU 0 [Combined_07_Parallel]: üìä Dataset: GoEmotions (from local cache)\n",
      "[21:36:50] GPU 0 [Combined_07_Parallel]: üî¨ Scientific logging: ENABLED\n",
      "[21:36:50] GPU 0 [Combined_07_Parallel]: ü§ñ Loading deberta-v3-large...\n",
      "[21:36:50] GPU 0 [Combined_07_Parallel]: üìÅ Found local cache at models/deberta-v3-large\n",
      "[21:36:50] GPU 1 [Combined_05_Parallel]: ‚úÖ deberta-v3-large tokenizer loaded from local cache\n",
      "[21:36:50] GPU 0 [Combined_07_Parallel]: ‚úÖ deberta-v3-large tokenizer loaded from local cache\n",
      "[21:36:50] GPU 1 [Combined_05_Parallel]: ‚úÖ deberta-v3-large model loaded from local cache\n",
      "[21:36:50] GPU 1 [Combined_05_Parallel]: üìä Loading GoEmotions dataset from local cache...\n",
      "[21:36:50] GPU 1 [Combined_05_Parallel]: ‚úÖ GoEmotions dataset loaded from local cache\n",
      "[21:36:50] GPU 1 [Combined_05_Parallel]: Training examples: 43410\n",
      "[21:36:50] GPU 1 [Combined_05_Parallel]: Validation examples: 5426\n",
      "[21:36:50] GPU 1 [Combined_05_Parallel]: Total emotions: 28\n",
      "[21:36:50] GPU 1 [Combined_05_Parallel]: üîÑ Creating datasets...\n",
      "[21:36:50] GPU 0 [Combined_07_Parallel]: ‚úÖ deberta-v3-large model loaded from local cache\n",
      "[21:36:50] GPU 0 [Combined_07_Parallel]: üìä Loading GoEmotions dataset from local cache...\n",
      "[21:36:50] GPU 0 [Combined_07_Parallel]: ‚úÖ GoEmotions dataset loaded from local cache\n",
      "[21:36:50] GPU 0 [Combined_07_Parallel]: Training examples: 43410\n",
      "[21:36:50] GPU 0 [Combined_07_Parallel]: Validation examples: 5426\n",
      "[21:36:50] GPU 0 [Combined_07_Parallel]: Total emotions: 28\n",
      "[21:36:50] GPU 0 [Combined_07_Parallel]: üîÑ Creating datasets...\n",
      "[21:36:51] GPU 1 [Combined_05_Parallel]: ‚úÖ Created 43410 training examples\n",
      "[21:36:51] GPU 1 [Combined_05_Parallel]: ‚úÖ Created 5426 validation examples\n",
      "[21:36:51] GPU 1 [Combined_05_Parallel]: üîÑ Limiting training data: 43410 ‚Üí 20000 samples\n",
      "[21:36:51] GPU 1 [Combined_05_Parallel]: ‚úÖ Using 20000 training examples (subset for quick screening)\n",
      "[21:36:51] GPU 1 [Combined_05_Parallel]: üîÑ Limiting validation data: 5426 ‚Üí 3000 samples\n",
      "[21:36:51] GPU 1 [Combined_05_Parallel]: ‚úÖ Using 3000 validation examples (subset for quick screening)\n",
      "[21:36:51] GPU 1 [Combined_05_Parallel]: üîß Disabling gradient checkpointing to prevent RuntimeError during backward pass\n",
      "[21:36:51] GPU 0 [Combined_07_Parallel]: ‚úÖ Created 43410 training examples\n",
      "[21:36:51] GPU 0 [Combined_07_Parallel]: ‚úÖ Created 5426 validation examples\n",
      "[21:36:51] GPU 0 [Combined_07_Parallel]: üîÑ Limiting training data: 43410 ‚Üí 20000 samples\n",
      "[21:36:51] GPU 0 [Combined_07_Parallel]: ‚úÖ Using 20000 training examples (subset for quick screening)\n",
      "[21:36:51] GPU 0 [Combined_07_Parallel]: üîÑ Limiting validation data: 5426 ‚Üí 3000 samples\n",
      "[21:36:51] GPU 0 [Combined_07_Parallel]: ‚úÖ Using 3000 validation examples (subset for quick screening)\n",
      "[21:36:51] GPU 0 [Combined_07_Parallel]: üîß Disabling gradient checkpointing to prevent RuntimeError during backward pass\n",
      "[21:36:51] GPU 1 [Combined_05_Parallel]: üöÄ Using Combined Loss (ASL + Class Weighting + Focal Loss) for maximum performance\n",
      "[21:36:51] GPU 1 [Combined_05_Parallel]: üìä Loss combination ratio: 0.5 ASL + 0.5 Focal\n",
      "[21:36:51] GPU 0 [Combined_07_Parallel]: üöÄ Using Combined Loss (ASL + Class Weighting + Focal Loss) for maximum performance\n",
      "[21:36:51] GPU 0 [Combined_07_Parallel]: üìä Loss combination ratio: 0.7 ASL + 0.30000000000000004 Focal\n",
      "[21:36:51] GPU 1 [Combined_05_Parallel]: WARNING:accelerate.utils.other:Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "[21:36:51] GPU 0 [Combined_07_Parallel]: WARNING:accelerate.utils.other:Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "[21:36:51] GPU 0 [Combined_07_Parallel]: Traceback (most recent call last):\n",
      "[21:36:51] GPU 0 [Combined_07_Parallel]: File \"/home/user/goemotions-deberta/notebooks/scripts/train_deberta_local.py\", line 944, in <module>\n",
      "[21:36:51] GPU 0 [Combined_07_Parallel]: main()\n",
      "[21:36:51] GPU 0 [Combined_07_Parallel]: File \"/home/user/goemotions-deberta/notebooks/scripts/train_deberta_local.py\", line 847, in main\n",
      "[21:36:51] GPU 0 [Combined_07_Parallel]: trainer = CombinedLossTrainer(\n",
      "[21:36:51] GPU 0 [Combined_07_Parallel]: File \"/home/user/goemotions-deberta/notebooks/scripts/train_deberta_local.py\", line 274, in __init__\n",
      "[21:36:51] GPU 0 [Combined_07_Parallel]: super().__init__(*args, **kwargs)\n",
      "[21:36:51] GPU 0 [Combined_07_Parallel]: File \"/venv/deberta-v3/lib/python3.10/site-packages/transformers/utils/deprecation.py\", line 172, in wrapped_func\n",
      "[21:36:51] GPU 0 [Combined_07_Parallel]: return func(*args, **kwargs)\n",
      "[21:36:51] GPU 0 [Combined_07_Parallel]: File \"/venv/deberta-v3/lib/python3.10/site-packages/transformers/trainer.py\", line 807, in __init__\n",
      "[21:36:51] GPU 0 [Combined_07_Parallel]: self.control = self.callback_handler.on_init_end(self.args, self.state, self.control)\n",
      "[21:36:51] GPU 0 [Combined_07_Parallel]: File \"/venv/deberta-v3/lib/python3.10/site-packages/transformers/trainer_callback.py\", line 502, in on_init_end\n",
      "[21:36:51] GPU 0 [Combined_07_Parallel]: return self.call_event(\"on_init_end\", args, state, control)\n",
      "[21:36:51] GPU 0 [Combined_07_Parallel]: File \"/venv/deberta-v3/lib/python3.10/site-packages/transformers/trainer_callback.py\", line 556, in call_event\n",
      "[21:36:51] GPU 0 [Combined_07_Parallel]: result = getattr(callback, event)(\n",
      "[21:36:51] GPU 0 [Combined_07_Parallel]: AttributeError: 'ProgressMonitorCallback' object has no attribute 'on_init_end'. Did you mean: 'on_step_end'?\n",
      "[21:36:51] GPU 1 [Combined_05_Parallel]: Traceback (most recent call last):\n",
      "[21:36:51] GPU 1 [Combined_05_Parallel]: File \"/home/user/goemotions-deberta/notebooks/scripts/train_deberta_local.py\", line 944, in <module>\n",
      "[21:36:51] GPU 1 [Combined_05_Parallel]: main()\n",
      "[21:36:51] GPU 1 [Combined_05_Parallel]: File \"/home/user/goemotions-deberta/notebooks/scripts/train_deberta_local.py\", line 847, in main\n",
      "[21:36:51] GPU 1 [Combined_05_Parallel]: trainer = CombinedLossTrainer(\n",
      "[21:36:51] GPU 1 [Combined_05_Parallel]: File \"/home/user/goemotions-deberta/notebooks/scripts/train_deberta_local.py\", line 274, in __init__\n",
      "[21:36:51] GPU 1 [Combined_05_Parallel]: super().__init__(*args, **kwargs)\n",
      "[21:36:51] GPU 1 [Combined_05_Parallel]: File \"/venv/deberta-v3/lib/python3.10/site-packages/transformers/utils/deprecation.py\", line 172, in wrapped_func\n",
      "[21:36:51] GPU 1 [Combined_05_Parallel]: return func(*args, **kwargs)\n",
      "[21:36:51] GPU 1 [Combined_05_Parallel]: File \"/venv/deberta-v3/lib/python3.10/site-packages/transformers/trainer.py\", line 807, in __init__\n",
      "[21:36:51] GPU 1 [Combined_05_Parallel]: self.control = self.callback_handler.on_init_end(self.args, self.state, self.control)\n",
      "[21:36:51] GPU 1 [Combined_05_Parallel]: File \"/venv/deberta-v3/lib/python3.10/site-packages/transformers/trainer_callback.py\", line 502, in on_init_end\n",
      "[21:36:51] GPU 1 [Combined_05_Parallel]: return self.call_event(\"on_init_end\", args, state, control)\n",
      "[21:36:51] GPU 1 [Combined_05_Parallel]: File \"/venv/deberta-v3/lib/python3.10/site-packages/transformers/trainer_callback.py\", line 556, in call_event\n",
      "[21:36:51] GPU 1 [Combined_05_Parallel]: result = getattr(callback, event)(\n",
      "[21:36:51] GPU 1 [Combined_05_Parallel]: AttributeError: 'ProgressMonitorCallback' object has no attribute 'on_init_end'. Did you mean: 'on_step_end'?\n",
      "[21:36:57] üìã FINAL LINES (Combined_05_Parallel, last 10):\n",
      "    super().__init__(*args, **kwargs)\n",
      "    File \"/venv/deberta-v3/lib/python3.10/site-packages/transformers/utils/deprecation.py\", line 172, in wrapped_func\n",
      "    return func(*args, **kwargs)\n",
      "    File \"/venv/deberta-v3/lib/python3.10/site-packages/transformers/trainer.py\", line 807, in __init__\n",
      "    self.control = self.callback_handler.on_init_end(self.args, self.state, self.control)\n",
      "    File \"/venv/deberta-v3/lib/python3.10/site-packages/transformers/trainer_callback.py\", line 502, in on_init_end\n",
      "    return self.call_event(\"on_init_end\", args, state, control)\n",
      "    File \"/venv/deberta-v3/lib/python3.10/site-packages/transformers/trainer_callback.py\", line 556, in call_event\n",
      "    result = getattr(callback, event)(\n",
      "    AttributeError: 'ProgressMonitorCallback' object has no attribute 'on_init_end'. Did you mean: 'on_step_end'?\n",
      "‚úÖ Combined_05_Parallel complete on GPU 1 (return code: 1) at 2025-09-08 21:36:57.593598\n",
      "[21:36:57] üìã FINAL LINES (Combined_07_Parallel, last 10):\n",
      "    super().__init__(*args, **kwargs)\n",
      "    File \"/venv/deberta-v3/lib/python3.10/site-packages/transformers/utils/deprecation.py\", line 172, in wrapped_func\n",
      "    return func(*args, **kwargs)\n",
      "    File \"/venv/deberta-v3/lib/python3.10/site-packages/transformers/trainer.py\", line 807, in __init__\n",
      "    self.control = self.callback_handler.on_init_end(self.args, self.state, self.control)\n",
      "    File \"/venv/deberta-v3/lib/python3.10/site-packages/transformers/trainer_callback.py\", line 502, in on_init_end\n",
      "    return self.call_event(\"on_init_end\", args, state, control)\n",
      "    File \"/venv/deberta-v3/lib/python3.10/site-packages/transformers/trainer_callback.py\", line 556, in call_event\n",
      "    result = getattr(callback, event)(\n",
      "    AttributeError: 'ProgressMonitorCallback' object has no attribute 'on_init_end'. Did you mean: 'on_step_end'?\n",
      "‚úÖ Combined_07_Parallel complete on GPU 0 (return code: 1) at 2025-09-08 21:36:57.594627\n",
      "\n",
      "üìç SINGLE LIVE: Combined 0.3 (GPU0)\n",
      "üöÄ LIVE MONITOR: Starting Combined_03_Parallel on GPU 0 at 2025-09-08 21:36:57.595093\n",
      "Command for Combined_03_Parallel: python3 notebooks/scripts/train_deberta_local.py --output_dir ./outputs/parallel_Combined_03_Parallel --model_type deberta-v3-large --per_device_train_batch_size 4 --per_device_eval_batch_size 8 --gradient_accumulation_steps 4 --num_train_epochs 2 --learning_rate 3e-5 --lr_scheduler_type cosine --warmup_ratio 0.15 --weight_decay 0.01 --fp16 --max_length 256 --max_train_samples 20000 --max_eval_samples 3000 --use_combined_loss --loss_combination_ratio 0.3\n",
      "[21:37:02] GPU 0 [Combined_03_Parallel]: üöÄ GoEmotions DeBERTa Training (SCIENTIFIC VERSION)\n",
      "[21:37:02] GPU 0 [Combined_03_Parallel]: ============================================================\n",
      "[21:37:02] GPU 0 [Combined_03_Parallel]: üìÅ Output directory: ./outputs/parallel_Combined_03_Parallel\n",
      "[21:37:02] GPU 0 [Combined_03_Parallel]: ü§ñ Model: deberta-v3-large (from local cache)\n",
      "[21:37:02] GPU 0 [Combined_03_Parallel]: üìä Dataset: GoEmotions (from local cache)\n",
      "[21:37:02] GPU 0 [Combined_03_Parallel]: üî¨ Scientific logging: ENABLED\n",
      "[21:37:02] GPU 0 [Combined_03_Parallel]: ü§ñ Loading deberta-v3-large...\n",
      "[21:37:02] GPU 0 [Combined_03_Parallel]: üìÅ Found local cache at models/deberta-v3-large\n",
      "[21:37:02] GPU 0 [Combined_03_Parallel]: ‚úÖ deberta-v3-large tokenizer loaded from local cache\n",
      "[21:37:02] üñ•Ô∏è GPU STATUS (overall): 0, 0, 4, 24576, 36\n",
      "1, 0, 4, 24576, 26\n",
      "[21:37:02] üñ•Ô∏è GPU STATUS (overall): 0, 0, 4, 24576, 36\n",
      "1, 0, 4, 24576, 26\n",
      "[21:37:03] GPU 0 [Combined_03_Parallel]: ‚úÖ deberta-v3-large model loaded from local cache\n",
      "[21:37:03] GPU 0 [Combined_03_Parallel]: üìä Loading GoEmotions dataset from local cache...\n",
      "[21:37:03] GPU 0 [Combined_03_Parallel]: ‚úÖ GoEmotions dataset loaded from local cache\n",
      "[21:37:03] GPU 0 [Combined_03_Parallel]: Training examples: 43410\n",
      "[21:37:03] GPU 0 [Combined_03_Parallel]: Validation examples: 5426\n",
      "[21:37:03] GPU 0 [Combined_03_Parallel]: Total emotions: 28\n",
      "[21:37:03] GPU 0 [Combined_03_Parallel]: üîÑ Creating datasets...\n",
      "[21:37:03] GPU 0 [Combined_03_Parallel]: ‚úÖ Created 43410 training examples\n",
      "[21:37:03] GPU 0 [Combined_03_Parallel]: ‚úÖ Created 5426 validation examples\n",
      "[21:37:03] GPU 0 [Combined_03_Parallel]: üîÑ Limiting training data: 43410 ‚Üí 20000 samples\n",
      "[21:37:03] GPU 0 [Combined_03_Parallel]: ‚úÖ Using 20000 training examples (subset for quick screening)\n",
      "[21:37:03] GPU 0 [Combined_03_Parallel]: üîÑ Limiting validation data: 5426 ‚Üí 3000 samples\n",
      "[21:37:03] GPU 0 [Combined_03_Parallel]: ‚úÖ Using 3000 validation examples (subset for quick screening)\n",
      "[21:37:03] GPU 0 [Combined_03_Parallel]: üîß Disabling gradient checkpointing to prevent RuntimeError during backward pass\n",
      "[21:37:03] GPU 0 [Combined_03_Parallel]: üöÄ Using Combined Loss (ASL + Class Weighting + Focal Loss) for maximum performance\n",
      "[21:37:03] GPU 0 [Combined_03_Parallel]: üìä Loss combination ratio: 0.3 ASL + 0.7 Focal\n",
      "[21:37:03] GPU 0 [Combined_03_Parallel]: WARNING:accelerate.utils.other:Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "[21:37:03] GPU 0 [Combined_03_Parallel]: Traceback (most recent call last):\n",
      "[21:37:03] GPU 0 [Combined_03_Parallel]: File \"/home/user/goemotions-deberta/notebooks/scripts/train_deberta_local.py\", line 944, in <module>\n",
      "[21:37:03] GPU 0 [Combined_03_Parallel]: main()\n",
      "[21:37:03] GPU 0 [Combined_03_Parallel]: File \"/home/user/goemotions-deberta/notebooks/scripts/train_deberta_local.py\", line 847, in main\n",
      "[21:37:03] GPU 0 [Combined_03_Parallel]: trainer = CombinedLossTrainer(\n",
      "[21:37:03] GPU 0 [Combined_03_Parallel]: File \"/home/user/goemotions-deberta/notebooks/scripts/train_deberta_local.py\", line 274, in __init__\n",
      "[21:37:03] GPU 0 [Combined_03_Parallel]: super().__init__(*args, **kwargs)\n",
      "[21:37:03] GPU 0 [Combined_03_Parallel]: File \"/venv/deberta-v3/lib/python3.10/site-packages/transformers/utils/deprecation.py\", line 172, in wrapped_func\n",
      "[21:37:03] GPU 0 [Combined_03_Parallel]: return func(*args, **kwargs)\n",
      "[21:37:03] GPU 0 [Combined_03_Parallel]: File \"/venv/deberta-v3/lib/python3.10/site-packages/transformers/trainer.py\", line 807, in __init__\n",
      "[21:37:03] GPU 0 [Combined_03_Parallel]: self.control = self.callback_handler.on_init_end(self.args, self.state, self.control)\n",
      "[21:37:03] GPU 0 [Combined_03_Parallel]: File \"/venv/deberta-v3/lib/python3.10/site-packages/transformers/trainer_callback.py\", line 502, in on_init_end\n",
      "[21:37:03] GPU 0 [Combined_03_Parallel]: return self.call_event(\"on_init_end\", args, state, control)\n",
      "[21:37:03] GPU 0 [Combined_03_Parallel]: File \"/venv/deberta-v3/lib/python3.10/site-packages/transformers/trainer_callback.py\", line 556, in call_event\n",
      "[21:37:03] GPU 0 [Combined_03_Parallel]: result = getattr(callback, event)(\n",
      "[21:37:03] GPU 0 [Combined_03_Parallel]: AttributeError: 'ProgressMonitorCallback' object has no attribute 'on_init_end'. Did you mean: 'on_step_end'?\n",
      "[21:37:09] üìã FINAL LINES (Combined_03_Parallel, last 10):\n",
      "    super().__init__(*args, **kwargs)\n",
      "    File \"/venv/deberta-v3/lib/python3.10/site-packages/transformers/utils/deprecation.py\", line 172, in wrapped_func\n",
      "    return func(*args, **kwargs)\n",
      "    File \"/venv/deberta-v3/lib/python3.10/site-packages/transformers/trainer.py\", line 807, in __init__\n",
      "    self.control = self.callback_handler.on_init_end(self.args, self.state, self.control)\n",
      "    File \"/venv/deberta-v3/lib/python3.10/site-packages/transformers/trainer_callback.py\", line 502, in on_init_end\n",
      "    return self.call_event(\"on_init_end\", args, state, control)\n",
      "    File \"/venv/deberta-v3/lib/python3.10/site-packages/transformers/trainer_callback.py\", line 556, in call_event\n",
      "    result = getattr(callback, event)(\n",
      "    AttributeError: 'ProgressMonitorCallback' object has no attribute 'on_init_end'. Did you mean: 'on_step_end'?\n",
      "‚úÖ Combined_03_Parallel complete on GPU 0 (return code: 1) at 2025-09-08 21:37:09.797316\n",
      "\n",
      "üéâ PHASE 1.5 PARALLEL COMPLETE with FULL LIVE VERBOSE MONITORING!\n",
      "üìä Outputs: ./outputs/parallel_BCE_Parallel/, ./outputs/parallel_Asymmetric_Parallel/, etc.\n",
      "üîç All training output printed live above - run analysis cell for F1@0.2 comparison vs baseline 42.18% (target >50%)\n",
      "‚è±Ô∏è Total time: ~1.5 hours with dual-GPU parallel execution\n",
      "üñ•Ô∏è GPU monitoring: Utilization/memory/temp shown every 30s during runs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:37:15] üñ•Ô∏è GPU STATUS (overall): 0, 0, 4, 24576, 34\n",
      "1, 0, 4, 24576, 26\n",
      "[21:37:27] üñ•Ô∏è GPU STATUS (overall): 0, 0, 4, 24576, 32\n",
      "1, 0, 4, 24576, 26\n"
     ]
    }
   ],
   "source": [
    "# FULL PAIR 1: BCE (GPU0) + Asymmetric (GPU1) with live monitoring\n",
    "print(\"\\nüìç PAIR 1 LIVE: BCE (GPU0) + Asymmetric (GPU1)\")\n",
    "t1 = threading.Thread(target=run_config_live, args=(0, 'BCE_Parallel'))\n",
    "t2 = threading.Thread(target=run_config_live, args=(1, 'Asymmetric_Parallel', True))\n",
    "t1.start()\n",
    "t2.start()\n",
    "t1.join()\n",
    "t2.join()\n",
    "\n",
    "\n",
    "\n",
    "# FULL PAIR 2: Combined 0.7 (GPU0) + 0.5 (GPU1) with live monitoring\n",
    "print(\"\\nüìç PAIR 2 LIVE: Combined 0.7 (GPU0) + 0.5 (GPU1)\")\n",
    "t3 = threading.Thread(target=run_config_live, args=(0, 'Combined_07_Parallel', False, 0.7))\n",
    "t4 = threading.Thread(target=run_config_live, args=(1, 'Combined_05_Parallel', False, 0.5))\n",
    "t3.start()\n",
    "t4.start()\n",
    "t3.join()\n",
    "t4.join()\n",
    "\n",
    "\n",
    "\n",
    "# FULL SINGLE: Combined 0.3 (GPU0) with live monitoring\n",
    "print(\"\\nüìç SINGLE LIVE: Combined 0.3 (GPU0)\")\n",
    "run_config_live(0, 'Combined_03_Parallel', False, 0.3)\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nüéâ PHASE 1.5 PARALLEL COMPLETE with FULL LIVE VERBOSE MONITORING!\")\n",
    "print(\"üìä Outputs: ./outputs/parallel_BCE_Parallel/, ./outputs/parallel_Asymmetric_Parallel/, etc.\")\n",
    "print(\"üîç All training output printed live above - run analysis cell for F1@0.2 comparison vs baseline 42.18% (target >50%)\")\n",
    "print(\"‚è±Ô∏è Total time: ~1.5 hours with dual-GPU parallel execution\")\n",
    "print(\"üñ•Ô∏è GPU monitoring: Utilization/memory/temp shown every 30s during runs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7c3cbe",
   "metadata": {},
   "source": [
    "# SETUP & CACHE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1709750",
   "metadata": {},
   "source": [
    "# SETUP & CACHE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac0e18e",
   "metadata": {},
   "source": [
    "## ANALYSIS: F1 Comparison at Threshold=0.2\n",
    "\n",
    "**Load eval_report.json from all configs, extract f1_macro_t2, compare to baseline 42.18%.**\n",
    "- Success if >50%\n",
    "- Diagnose if below (check loss curve, class F1)\n",
    "- HF multi-label best practices: threshold sweep, per-class weights effective on rare emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84bffbd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ phase1_bce_simplified: Not completed\n",
      "‚è≥ phase1_asymmetric_simplified: Not completed\n",
      "‚è≥ phase1_combined_07_simplified: Not completed\n",
      "‚è≥ phase1_combined_05_simplified: Not completed\n",
      "‚è≥ phase1_combined_03_simplified: Not completed\n",
      "‚úÖ parallel_BCE_Parallel: F1@0.2 = 0.4471 (NEEDS IMPROVEMENT)\n",
      "‚úÖ parallel_Asymmetric_Parallel: F1@0.2 = 0.0870 (NEEDS IMPROVEMENT)\n",
      "‚è≥ parallel_Combined_07_Parallel: Not completed\n",
      "‚è≥ parallel_Combined_05_Parallel: Not completed\n",
      "‚è≥ parallel_Combined_03_Parallel: Not completed\n",
      "\n",
      "üèÜ BEST F1@0.2: 0.4471 (BELOW TARGET (42.18% baseline))\n",
      "üîç DIAGNOSE: Check loss curve, class-wise F1 for rare emotions\n",
      "HF Best Practices: Threshold sweep 0.1-0.3, per-class weights validated\n",
      "\n",
      "üìÅ All outputs: ./outputs/phase1_* & ./outputs/parallel_*/\n"
     ]
    }
   ],
   "source": [
    "# PHASE 1 & 1.5 RESULTS ANALYSIS (Threshold=0.2)\n",
    "import json, os\n",
    "BASELINE_F1 = 0.4218  # Original notebook line 1405\n",
    "\n",
    "\n",
    "\n",
    "def load_results(dirs):\n",
    "    results = {}\n",
    "    for d in dirs:\n",
    "        path = os.path.join(d, 'eval_report.json')\n",
    "        if os.path.exists(path):\n",
    "            with open(path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            name = d.split('/')[-1]\n",
    "            f1_t2 = data.get('f1_macro_t2', data.get('f1_macro', 0.0))\n",
    "            results[name] = {'f1_macro_t2': f1_t2, 'success': f1_t2 > 0.50, 'improvement': ((f1_t2 - BASELINE_F1) / BASELINE_F1) * 100}\n",
    "            print(f\"‚úÖ {name}: F1@0.2 = {f1_t2:.4f} ({'SUCCESS >50%' if results[name]['success'] else 'NEEDS IMPROVEMENT'})\")\n",
    "        else:\n",
    "            print(f\"‚è≥ {d.split('/')[-1]}: Not completed\")\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "# Sequential Phase 1\n",
    "seq_dirs = ['./outputs/phase1_bce_simplified', './outputs/phase1_asymmetric_simplified', \n",
    "            './outputs/phase1_combined_07_simplified', './outputs/phase1_combined_05_simplified', \n",
    "            './outputs/phase1_combined_03_simplified']\n",
    "seq_results = load_results(seq_dirs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Parallel Phase 1.5\n",
    "par_dirs = ['./outputs/parallel_BCE_Parallel', './outputs/parallel_Asymmetric_Parallel',\n",
    "            './outputs/parallel_Combined_07_Parallel', './outputs/parallel_Combined_05_Parallel',\n",
    "            './outputs/parallel_Combined_03_Parallel']\n",
    "par_results = load_results(par_dirs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Summary\n",
    "all_results = {**seq_results, **par_results}\n",
    "best_f1 = max([r['f1_macro_t2'] for r in all_results.values()])\n",
    "print(f\"\\nüèÜ BEST F1@0.2: {best_f1:.4f} ({'SUCCESS' if best_f1 > 0.50 else 'BELOW TARGET (42.18% baseline)'})\")\n",
    "if best_f1 > 0.50:\n",
    "    print(\"‚úÖ PHASE 2 READY: Add cell for sequential top 2 configs (3 epochs, 30k samples)\")\n",
    "    print(\"Top configs for Phase 2: Asymmetric + best Combined\")\n",
    "else:\n",
    "    print(\"üîç DIAGNOSE: Check loss curve, class-wise F1 for rare emotions\")\n",
    "    print(\"HF Best Practices: Threshold sweep 0.1-0.3, per-class weights validated\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Phase 2 Preparation (add this cell if success)\n",
    "if best_f1 > 0.50:\n",
    "    top_configs = sorted(all_results.items(), key=lambda x: x[1]['f1_macro_t2'], reverse=True)[:2]\n",
    "    print(f\"\\nüéØ PHASE 2: Train top 2 - {top_configs[0][0]} + {top_configs[1][0]}\")\n",
    "    print(\"Command template: --num_train_epochs 3 --max_train_samples 30000 + same fixes\")\n",
    "print(\"\\nüìÅ All outputs: ./outputs/phase1_* & ./outputs/parallel_*/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b6d88a",
   "metadata": {},
   "source": [
    "## MONITORING & VALIDATION\n",
    "\n",
    "**Commands for live monitoring:**\n",
    "- Sequential: !nvidia-smi (GPU 0 usage)\n",
    "- Parallel: !tail -f gpu*.log & !nvidia-smi (both GPUs)\n",
    "- Validation: Per-class weights effective on rare emotions (check eval_report.json per-class F1)\n",
    "- HF Docs: Multi-label eval with threshold=0.2 optimal for imbalance (precision/recall balance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bfe5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è∏Ô∏è No active training\n",
      "\n",
      "üñ•Ô∏è GPU status:\n",
      "index, name, utilization.gpu [%], memory.used [MiB]\n",
      "0, NVIDIA GeForce RTX 3090, 0 %, 4 MiB\n",
      "1, NVIDIA GeForce RTX 3090, 0 %, 4 MiB\n",
      "\n",
      "üìä gpu0_combined_07_parallel.log:\n",
      "  File \"/home/user/goemotions-deberta/notebooks/scripts/train_deberta_local.py\", line 699, in main\n",
      "    os.makedirs(args.output_dir, exist_ok=True)\n",
      "  File \"/venv/deberta-v3/lib/python3.10/os.py\", line 225, in makedirs\n",
      "    mkdir(name, mode)\n",
      "OSError: [Errno 122] Disk quota exceeded: './outputs/parallel_Combined_07_Parallel'\n",
      "\n",
      "üìä gpu1_combined_05_parallel.log:\n",
      "  File \"/home/user/goemotions-deberta/notebooks/scripts/train_deberta_local.py\", line 699, in main\n",
      "    os.makedirs(args.output_dir, exist_ok=True)\n",
      "  File \"/venv/deberta-v3/lib/python3.10/os.py\", line 225, in makedirs\n",
      "    mkdir(name, mode)\n",
      "OSError: [Errno 122] Disk quota exceeded: './outputs/parallel_Combined_05_Parallel'\n",
      "\n",
      "‚úÖ Notebook ready! Run Phase 1 sequential, then Phase 1.5 parallel for dual-GPU.\n",
      "Target: >50% F1@0.2 vs baseline 42.18%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:37:15] üñ•Ô∏è GPU STATUS (overall): 0, 0, 4, 24576, 34\n",
      "1, 0, 4, 24576, 26\n"
     ]
    }
   ],
   "source": [
    "# LIVE MONITORING UTILITIES\n",
    "def monitor_processes():\n",
    "    result = subprocess.run(['ps', 'aux'], capture_output=True, text=True)\n",
    "    processes = [line for line in result.stdout.split('\\n') if 'train_deberta_local' in line]\n",
    "    if processes:\n",
    "        print(\"üîÑ Active processes:\")\n",
    "        for p in processes: print(f\"  {p}\")\n",
    "    else:\n",
    "        print(\"‚è∏Ô∏è No active training\")\n",
    "    print(\"\\nüñ•Ô∏è GPU status:\")\n",
    "    !nvidia-smi --query-gpu=index,name,utilization.gpu,memory.used --format=csv\n",
    "\n",
    "\n",
    "\n",
    "def tail_logs(pattern='gpu*.log'):\n",
    "    import glob\n",
    "    logs = glob.glob(pattern)\n",
    "    for log in logs[-2:]:  # Last 2 logs\n",
    "        print(f\"\\nüìä {log}:\")\n",
    "        !tail -5 {log}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "monitor_processes()\n",
    "tail_logs()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n‚úÖ Notebook ready! Run Phase 1 sequential, then Phase 1.5 parallel for dual-GPU.\")\n",
    "print(\"Target: >50% F1@0.2 vs baseline 42.18%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104cb082",
   "metadata": {},
   "source": [
    "## PROGRESS SUMMARY vs FIXED_RIGOROUS\n",
    "\n",
    "**SIMPLIFIED_PARALLEL vs FIXED_RIGOROUS**:\n",
    "- **Structure**: Simplified starts sequential (stability), adds parallel Phase 1.5 (speed); no early stopping/multi-phase complexity\n",
    "- **Parameters**: 20k samples/2 epochs vs rigorous 30k/3 epochs; threshold=0.2 fixed for imbalance\n",
    "- **Fixes**: BCE focus with pos_weight/oversampling/differentiable losses; same as rigorous but streamlined\n",
    "- **Performance**: Target >50% F1@0.2 (vs baseline 42.18%); parallel 50% faster dual-GPU utilization\n",
    "- **Cost/Time**: ~$4 total, 1-1.5 hours parallel vs 2-3 hours sequential\n",
    "\n",
    "**Next**: If >50% achieved, Phase 2 cell for top 2 configs (3 epochs, 30k samples) already prepared in analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
