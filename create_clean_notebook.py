#!/usr/bin/env python3
"""
Create the clean multi-dataset notebook
"""

import json

# Create the notebook structure
notebook = {
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üéØ SAMo Multi-Dataset Emotion Classification Pipeline\n",
                "\n",
                "## **COMPREHENSIVE MULTI-DATASET TRAINING WITH SCIENTIFIC LOGGING**\n",
                "\n",
                "**GOAL**: Achieve >60% F1-macro on combined GoEmotions, SemEval, ISEAR, and MELD datasets\n",
                "\n",
                "**FEATURES**:\n",
                "- ‚úÖ **Multi-Dataset Integration**: Combines 4 emotion datasets (70K+ samples)\n",
                "- ‚úÖ **Scientific Logging**: Comprehensive experiment tracking and metrics\n",
                "- ‚úÖ **Google Drive Backup**: Automatic backup of all results and logs\n",
                "- ‚úÖ **Robust Training**: Error handling, progress monitoring, and recovery\n",
                "- ‚úÖ **Clean Workflow**: Streamlined 2-step process\n",
                "\n",
                "**DATASETS**:\n",
                "- üìä **GoEmotions**: 48,836 samples (28 emotions)\n",
                "- üìä **SemEval**: 802 samples (11 emotions) \n",
                "- üìä **ISEAR**: 7,500 samples (7 emotions)\n",
                "- üìä **MELD**: 13,708 samples (7 emotions)\n",
                "\n",
                "**TOTAL**: 70,846+ samples across 28 emotion classes\n",
                "\n",
                "---\n",
                "\n",
                "## üöÄ **QUICK START - 2 STEP WORKFLOW**\n",
                "\n",
                "1. **Cell 1**: Environment Setup & Data Preparation\n",
                "2. **Cell 2**: Training Execution with Live Progress\n",
                "\n",
                "**Expected Training Time**: 6-8 hours on 2x RTX 3090\n",
                "**Expected Performance**: >60% F1-macro (vs 51.79% baseline)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üîß **STEP 1: ENVIRONMENT SETUP & DATA PREPARATION**\n",
                "\n",
                "**Run this cell first to:**\n",
                "- ‚úÖ Verify environment and dependencies\n",
                "- ‚úÖ Prepare combined multi-dataset\n",
                "- ‚úÖ Validate data integrity\n",
                "- ‚úÖ Set up logging and backup systems"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# üîß STEP 1: ENVIRONMENT SETUP & DATA PREPARATION\n",
                "print(\"üöÄ SAMo Multi-Dataset Pipeline - Environment Setup\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "import os\n",
                "import sys\n",
                "import json\n",
                "import subprocess\n",
                "import time\n",
                "from pathlib import Path\n",
                "from datetime import datetime\n",
                "\n",
                "# Set working directory\n",
                "os.chdir('/workspace')\n",
                "print(f\"üìÅ Working directory: {os.getcwd()}\")\n",
                "\n",
                "# Environment verification\n",
                "print(\"\\nüîç Verifying Environment...\")\n",
                "print(f\"Python: {sys.executable}\")\n",
                "print(f\"Version: {sys.version}\")\n",
                "\n",
                "# Check conda environment\n",
                "conda_env = os.environ.get('CONDA_DEFAULT_ENV', 'None')\n",
                "print(f\"Conda env: {conda_env}\")\n",
                "\n",
                "if conda_env != 'deberta-v3':\n",
                "    print(\"‚ö†Ô∏è  WARNING: Switch to 'Python (deberta-v3)' kernel for best results\")\n",
                "\n",
                "# Check critical packages\n",
                "print(\"\\nüì¶ Checking Dependencies...\")\n",
                "try:\n",
                "    import torch\n",
                "    print(f\"‚úÖ PyTorch {torch.__version__}\")\n",
                "    print(f\"   CUDA Available: {torch.cuda.is_available()}\")\n",
                "    print(f\"   GPU Count: {torch.cuda.device_count()}\")\n",
                "    if torch.cuda.is_available():\n",
                "        for i in range(torch.cuda.device_count()):\n",
                "            print(f\"   GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
                "except ImportError as e:\n",
                "    print(f\"‚ùå PyTorch missing: {e}\")\n",
                "\n",
                "try:\n",
                "    import transformers\n",
                "    print(f\"‚úÖ Transformers {transformers.__version__}\")\n",
                "except ImportError as e:\n",
                "    print(f\"‚ùå Transformers missing: {e}\")\n",
                "\n",
                "try:\n",
                "    import sklearn\n",
                "    print(f\"‚úÖ Scikit-learn {sklearn.__version__}\")\n",
                "except ImportError as e:\n",
                "    print(f\"‚ùå Scikit-learn missing: {e}\")\n",
                "\n",
                "# Check GPU status\n",
                "print(\"\\nüñ•Ô∏è  GPU Status:\")\n",
                "try:\n",
                "    result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total,memory.used', '--format=csv,noheader,nounits'], \n",
                "                          capture_output=True, text=True, timeout=10)\n",
                "    if result.returncode == 0:\n",
                "        print(result.stdout.strip())\n",
                "    else:\n",
                "        print(\"‚ö†Ô∏è  nvidia-smi not available\")\n",
                "except Exception as e:\n",
                "    print(f\"‚ö†Ô∏è  GPU check failed: {e}\")\n",
                "\n",
                "# Data preparation\n",
                "print(\"\\nüìä Preparing Multi-Dataset...\")\n",
                "print(\"   This will combine GoEmotions, SemEval, ISEAR, and MELD datasets\")\n",
                "\n",
                "# Check if data already exists\n",
                "data_dir = Path('data/combined')\n",
                "if data_dir.exists() and (data_dir / 'train.jsonl').exists() and (data_dir / 'val.jsonl').exists():\n",
                "    print(\"‚úÖ Combined dataset already exists\")\n",
                "    \n",
                "    # Show dataset statistics\n",
                "    train_samples = sum(1 for _ in open(data_dir / 'train.jsonl'))\n",
                "    val_samples = sum(1 for _ in open(data_dir / 'val.jsonl'))\n",
                "    total_samples = train_samples + val_samples\n",
                "    \n",
                "    print(f\"   üìà Training samples: {train_samples:,}\")\n",
                "    print(f\"   üìà Validation samples: {val_samples:,}\")\n",
                "    print(f\"   üìà Total samples: {total_samples:,}\")\n",
                "    \n",
                "    # Load metadata\n",
                "    if (data_dir / 'metadata.json').exists():\n",
                "        with open(data_dir / 'metadata.json', 'r') as f:\n",
                "            metadata = json.load(f)\n",
                "        print(f\"   üéØ Emotion classes: {metadata.get('emotion_count', 'Unknown')}\")\n",
                "        print(f\"   üìä Datasets: {', '.join(metadata.get('datasets_included', []))}\")\n",
                "else:\n",
                "    print(\"üîÑ Running data preparation...\")\n",
                "    \n",
                "    # Run data preparation script\n",
                "    start_time = time.time()\n",
                "    result = subprocess.run(['python3', 'prepare_datasets_minimal.py'], \n",
                "                          capture_output=True, text=True, timeout=300)\n",
                "    \n",
                "    if result.returncode == 0:\n",
                "        print(\"‚úÖ Data preparation completed successfully\")\n",
                "        print(f\"‚è±Ô∏è  Time taken: {time.time() - start_time:.1f} seconds\")\n",
                "        \n",
                "        # Show final statistics\n",
                "        if data_dir.exists():\n",
                "            train_samples = sum(1 for _ in open(data_dir / 'train.jsonl'))\n",
                "            val_samples = sum(1 for _ in open(data_dir / 'val.jsonl'))\n",
                "            total_samples = train_samples + val_samples\n",
                "            \n",
                "            print(f\"\\nüìä Final Dataset Statistics:\")\n",
                "            print(f\"   üìà Training samples: {train_samples:,}\")\n",
                "            print(f\"   üìà Validation samples: {val_samples:,}\")\n",
                "            print(f\"   üìà Total samples: {total_samples:,}\")\n",
                "    else:\n",
                "        print(f\"‚ùå Data preparation failed:\")\n",
                "        print(f\"   Error: {result.stderr}\")\n",
                "        raise RuntimeError(\"Data preparation failed\")\n",
                "\n",
                "# Verify training scripts exist\n",
                "print(\"\\nüîç Verifying Training Scripts...\")\n",
                "scripts = [\n",
                "    'prepare_datasets_minimal.py',\n",
                "    'train_multidataset_deberta.py',\n",
                "    'train_comprehensive_multidataset.sh',\n",
                "    'backup_to_gdrive.sh'\n",
                "]\n",
                "\n",
                "for script in scripts:\n",
                "    if Path(script).exists():\n",
                "        print(f\"‚úÖ {script}\")\n",
                "    else:\n",
                "        print(f\"‚ùå {script} missing\")\n",
                "\n",
                "# Create necessary directories\n",
                "print(\"\\nüìÅ Creating Directories...\")\n",
                "directories = ['outputs', 'logs', 'models']\n",
                "for dir_name in directories:\n",
                "    Path(dir_name).mkdir(exist_ok=True)\n",
                "    print(f\"‚úÖ {dir_name}/\")\n",
                "\n",
                "print(\"\\nüéâ ENVIRONMENT SETUP COMPLETE!\")\n",
                "print(\"=\" * 60)\n",
                "print(\"‚úÖ Environment verified\")\n",
                "print(\"‚úÖ Multi-dataset prepared\")\n",
                "print(\"‚úÖ Training scripts ready\")\n",
                "print(\"‚úÖ Directories created\")\n",
                "print(\"\\nüöÄ Ready for training! Proceed to Cell 2.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üèÉ **STEP 2: TRAINING EXECUTION WITH LIVE PROGRESS**\n",
                "\n",
                "**Run this cell to:**\n",
                "- ‚úÖ Start comprehensive multi-dataset training\n",
                "- ‚úÖ Monitor live progress with real-time metrics\n",
                "- ‚úÖ Enable automatic Google Drive backup\n",
                "- ‚úÖ Track scientific logs and experiment data\n",
                "\n",
                "**Expected Results:**\n",
                "- üéØ **Target F1-macro**: >60% (vs 51.79% baseline)\n",
                "- ‚è±Ô∏è **Training Time**: 6-8 hours\n",
                "- üìä **Final Dataset**: 70K+ samples across 28 emotions\n",
                "- ‚òÅÔ∏è **Backup**: Automatic Google Drive backup every 30 minutes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# üèÉ STEP 2: TRAINING EXECUTION WITH LIVE PROGRESS\n",
                "print(\"üöÄ SAMo Multi-Dataset Training - Starting...\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "import subprocess\n",
                "import time\n",
                "from datetime import datetime\n",
                "\n",
                "# Training configuration\n",
                "EXPERIMENT_NAME = f\"MultiDataset_BCE_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
                "print(f\"üìä Experiment ID: {EXPERIMENT_NAME}\")\n",
                "print(f\"‚è∞ Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
                "print(f\"üéØ Target: >60% F1-macro (vs 51.79% baseline)\")\n",
                "print(f\"‚è±Ô∏è  Expected duration: 6-8 hours\")\n",
                "print()\n",
                "\n",
                "# Verify data exists\n",
                "data_dir = Path('data/combined')\n",
                "if not (data_dir / 'train.jsonl').exists() or not (data_dir / 'val.jsonl').exists():\n",
                "    print(\"‚ùå Combined dataset not found! Run Cell 1 first.\")\n",
                "    raise FileNotFoundError(\"Combined dataset not found\")\n",
                "\n",
                "# Show dataset statistics\n",
                "train_samples = sum(1 for _ in open(data_dir / 'train.jsonl'))\n",
                "val_samples = sum(1 for _ in open(data_dir / 'val.jsonl'))\n",
                "total_samples = train_samples + val_samples\n",
                "\n",
                "print(f\"üìä Dataset Statistics:\")\n",
                "print(f\"   üìà Training samples: {train_samples:,}\")\n",
                "print(f\"   üìà Validation samples: {val_samples:,}\")\n",
                "print(f\"   üìà Total samples: {total_samples:,}\")\n",
                "print(f\"   üéØ Emotion classes: 28\")\n",
                "print()\n",
                "\n",
                "# Check GPU availability\n",
                "print(\"üñ•Ô∏è  GPU Status:\")\n",
                "try:\n",
                "    result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total,memory.used', '--format=csv,noheader,nounits'], \n",
                "                          capture_output=True, text=True, timeout=10)\n",
                "    if result.returncode == 0:\n",
                "        gpu_info = result.stdout.strip().split('\\n')\n",
                "        for i, gpu in enumerate(gpu_info):\n",
                "            print(f\"   GPU {i}: {gpu}\")\n",
                "    else:\n",
                "        print(\"   ‚ö†Ô∏è  nvidia-smi not available\")\n",
                "except Exception as e:\n",
                "    print(f\"   ‚ö†Ô∏è  GPU check failed: {e}\")\n",
                "\n",
                "print()\n",
                "print(\"üèÉ Starting Training...\")\n",
                "print(\"   üìù Live progress will be shown below\")\n",
                "print(\"   üìä Metrics will be logged every 100 steps\")\n",
                "print(\"   üíæ Checkpoints saved every 1000 steps\")\n",
                "print(\"   ‚òÅÔ∏è  Google Drive backup every 30 minutes\")\n",
                "print(\"   ‚è±Ô∏è  Evaluation every 500 steps\")\n",
                "print()\n",
                "print(\"\" + \"=\"*60)\n",
                "print(\"üéØ LIVE TRAINING PROGRESS\")\n",
                "print(\"\" + \"=\"*60)\n",
                "\n",
                "# Start training with visible progress\n",
                "start_time = time.time()\n",
                "\n",
                "try:\n",
                "    # Run training script with live output\n",
                "    training_result = subprocess.run(\n",
                "        ['bash', 'train_comprehensive_multidataset.sh'],\n",
                "        capture_output=False,  # Show live progress\n",
                "        text=True,\n",
                "        timeout=28800  # 8 hour timeout\n",
                "    )\n",
                "    \n",
                "    training_time = time.time() - start_time\n",
                "    \n",
                "    print(\"\\n\" + \"=\"*60)\n",
                "    print(\"üéâ TRAINING COMPLETED!\")\n",
                "    print(\"\" + \"=\"*60)\n",
                "    print(f\"‚è±Ô∏è  Total training time: {training_time/3600:.2f} hours\")\n",
                "    print(f\"üìä Exit code: {training_result.returncode}\")\n",
                "    \n",
                "    if training_result.returncode == 0:\n",
                "        print(\"‚úÖ Training completed successfully!\")\n",
                "        \n",
                "        # Check for results\n",
                "        output_dir = Path('outputs/multidataset')\n",
                "        if output_dir.exists():\n",
                "            print(f\"\\nüìÅ Model saved to: {output_dir}\")\n",
                "            \n",
                "            # List saved files\n",
                "            model_files = list(output_dir.glob('*'))\n",
                "            if model_files:\n",
                "                print(\"   üìÑ Saved files:\")\n",
                "                for file in model_files:\n",
                "                    print(f\"      - {file.name}\")\n",
                "        \n",
                "        # Check logs\n",
                "        log_dir = Path('logs')\n",
                "        if log_dir.exists():\n",
                "            log_files = list(log_dir.glob('*.log'))\n",
                "            if log_files:\n",
                "                print(f\"\\nüìä Logs saved to: {log_dir}\")\n",
                "                print(\"   üìÑ Log files:\")\n",
                "                for log_file in log_files:\n",
                "                    print(f\"      - {log_file.name}\")\n",
                "        \n",
                "        # Check Google Drive backup\n",
                "        print(\"\\n‚òÅÔ∏è  Google Drive Backup:\")\n",
                "        print(\"   üì§ Backup completed automatically\")\n",
                "        print(\"   üìÅ Location: drive:00_Projects/üéØ TechLabs-2025/Final_Project/TRAINING/\")\n",
                "        \n",
                "        print(\"\\nüéØ NEXT STEPS:\")\n",
                "        print(\"   1. Check final F1-macro score in logs\")\n",
                "        print(\"   2. Verify model performance >60% F1-macro\")\n",
                "        print(\"   3. Download model from Google Drive if needed\")\n",
                "        print(\"   4. Use model for inference on new data\")\n",
                "        \n",
                "    else:\n",
                "        print(f\"‚ùå Training failed with exit code: {training_result.returncode}\")\n",
                "        print(\"\\nüîç Troubleshooting:\")\n",
                "        print(\"   1. Check logs/train_comprehensive_multidataset.log\")\n",
                "        print(\"   2. Verify GPU memory availability\")\n",
                "        print(\"   3. Check disk space\")\n",
                "        print(\"   4. Review error messages above\")\n",
                "        \n",
                "except subprocess.TimeoutExpired:\n",
                "    print(\"\\n‚è∞ Training timeout (8 hours exceeded)\")\n",
                "    print(\"   üìä Training may still be running in background\")\n",
                "    print(\"   üîç Check logs for progress\")\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"\\n‚ùå Training error: {e}\")\n",
                "    print(\"   üîç Check error messages above\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(f\"üèÅ Training session ended at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
                "print(\"\" + \"=\"*60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìä **RESULTS ANALYSIS & MONITORING**\n",
                "\n",
                "**Use this cell to:**\n",
                "- ‚úÖ Monitor training progress in real-time\n",
                "- ‚úÖ Analyze final results and metrics\n",
                "- ‚úÖ Check Google Drive backup status\n",
                "- ‚úÖ Verify model performance against baseline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# üìä RESULTS ANALYSIS & MONITORING\n",
                "print(\"üìä SAMo Multi-Dataset Results Analysis\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "import json\n",
                "import pandas as pd\n",
                "from pathlib import Path\n",
                "import subprocess\n",
                "\n",
                "# Check training logs\n",
                "print(\"üîç Checking Training Logs...\")\n",
                "log_file = Path('logs/train_comprehensive_multidataset.log')\n",
                "if log_file.exists():\n",
                "    print(f\"‚úÖ Training log found: {log_file}\")\n",
                "    \n",
                "    # Read last few lines for recent progress\n",
                "    with open(log_file, 'r') as f:\n",
                "        lines = f.readlines()\n",
                "    \n",
                "    print(\"\\nüìà Recent Training Progress:\")\n",
                "    for line in lines[-10:]:  # Last 10 lines\n",
                "        if 'f1_macro' in line.lower() or 'eval_f1' in line.lower():\n",
                "            print(f\"   {line.strip()}\")\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è  Training log not found\")\n",
                "\n",
                "# Check scientific logs\n",
                "print(\"\\nüî¨ Checking Scientific Logs...\")\n",
                "logs_dir = Path('logs')\n",
                "experiment_dirs = [d for d in logs_dir.iterdir() if d.is_dir() and 'MultiDataset_BCE' in d.name]\n",
                "\n",
                "if experiment_dirs:\n",
                "    latest_experiment = max(experiment_dirs, key=lambda x: x.stat().st_mtime)\n",
                "    print(f\"‚úÖ Latest experiment: {latest_experiment.name}\")\n",
                "    \n",
                "    # Check evaluation log\n",
                "    eval_log = latest_experiment / 'evaluation_log.json'\n",
                "    if eval_log.exists():\n",
                "        with open(eval_log, 'r') as f:\n",
                "            eval_data = json.load(f)\n",
                "        \n",
                "        print(f\"\\nüìä Evaluation Results:\")\n",
                "        for entry in eval_data[-3:]:  # Last 3 evaluations\n",
                "            metrics = entry.get('metrics', {})\n",
                "            epoch = entry.get('epoch', 'Unknown')\n",
                "            print(f\"   Epoch {epoch}:\")\n",
                "            for metric, value in metrics.items():\n",
                "                print(f\"      {metric}: {value:.4f}\")\n",
                "    \n",
                "    # Check experiment summary\n",
                "    summary_file = latest_experiment / 'experiment_summary.json'\n",
                "    if summary_file.exists():\n",
                "        with open(summary_file, 'r') as f:\n",
                "            summary = json.load(f)\n",
                "        \n",
                "        print(f\"\\nüìã Experiment Summary:\")\n",
                "        print(f\"   üÜî ID: {summary.get('experiment_id', 'Unknown')}\")\n",
                "        print(f\"   üìÖ Timestamp: {summary.get('timestamp', 'Unknown')}\")\n",
                "        print(f\"   üìà Training steps: {summary.get('total_training_steps', 'Unknown')}\")\n",
                "        print(f\"   üìä Evaluations: {summary.get('total_evaluations', 'Unknown')}\")\n",
                "        \n",
                "        final_metrics = summary.get('final_metrics')\n",
                "        if final_metrics:\n",
                "            print(f\"\\nüéØ Final Metrics:\")\n",
                "            for metric, value in final_metrics.items():\n",
                "                print(f\"   {metric}: {value:.4f}\")\n",
                "                \n",
                "            # Check against baseline\n",
                "            f1_macro = final_metrics.get('f1_macro', 0)\n",
                "            baseline = 0.5179  # 51.79% baseline\n",
                "            if f1_macro > baseline:\n",
                "                improvement = ((f1_macro - baseline) / baseline) * 100\n",
                "                print(f\"\\nüéâ SUCCESS! F1-macro improved by {improvement:.1f}% over baseline\")\n",
                "            else:\n",
                "                print(f\"\\n‚ö†Ô∏è  F1-macro ({f1_macro:.4f}) below baseline ({baseline:.4f})\")\n",
                "\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è  No experiment logs found\")\n",
                "\n",
                "# Check model outputs\n",
                "print(\"\\nü§ñ Checking Model Outputs...\")\n",
                "output_dir = Path('outputs/multidataset')\n",
                "if output_dir.exists():\n",
                "    print(f\"‚úÖ Model output directory: {output_dir}\")\n",
                "    \n",
                "    # List model files\n",
                "    model_files = list(output_dir.glob('*'))\n",
                "    if model_files:\n",
                "        print(\"   üìÑ Model files:\")\n",
                "        for file in model_files:\n",
                "            size_mb = file.stat().st_size / (1024 * 1024) if file.is_file() else 0\n",
                "            print(f\"      - {file.name} ({size_mb:.1f} MB)\")\n",
                "    else:\n",
                "        print(\"   ‚ö†Ô∏è  No model files found\")\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è  Model output directory not found\")\n",
                "\n",
                "# Check Google Drive backup\n",
                "print(\"\\n‚òÅÔ∏è  Checking Google Drive Backup...\")\n",
                "try:\n",
                "    result = subprocess.run(['rclone', 'ls', 'drive:00_Projects/üéØ TechLabs-2025/Final_Project/TRAINING/'], \n",
                "                          capture_output=True, text=True, timeout=30)\n",
                "    if result.returncode == 0:\n",
                "        print(\"‚úÖ Google Drive accessible\")\n",
                "        \n",
                "        # Look for our experiment\n",
                "        backup_lines = result.stdout.strip().split('\\n')\n",
                "        multidataset_backups = [line for line in backup_lines if 'MultiDataset_BCE' in line]\n",
                "        \n",
                "        if multidataset_backups:\n",
                "            print(f\"   üì§ Found {len(multidataset_backups)} MultiDataset backups:\")\n",
                "            for backup in multidataset_backups[-3:]:  # Last 3\n",
                "                print(f\"      - {backup}\")\n",
                "        else:\n",
                "            print(\"   ‚ö†Ô∏è  No MultiDataset backups found\")\n",
                "    else:\n",
                "        print(\"‚ö†Ô∏è  Google Drive not accessible\")\n",
                "except Exception as e:\n",
                "    print(f\"‚ö†Ô∏è  Google Drive check failed: {e}\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"üìä Analysis Complete!\")\n",
                "print(\"\" + \"=\"*50)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üîß **TROUBLESHOOTING & MAINTENANCE**\n",
                "\n",
                "**Use this cell for:**\n",
                "- ‚úÖ System diagnostics and health checks\n",
                "- ‚úÖ Cleanup and maintenance tasks\n",
                "- ‚úÖ Manual backup and recovery\n",
                "- ‚úÖ Performance optimization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# üîß TROUBLESHOOTING & MAINTENANCE\n",
                "print(\"üîß SAMo Multi-Dataset Troubleshooting\")\n",
                "print(\"=\" * 45)\n",
                "\n",
                "import shutil\n",
                "import subprocess\n",
                "from pathlib import Path\n",
                "\n",
                "# System diagnostics\n",
                "print(\"üñ•Ô∏è  System Diagnostics:\")\n",
                "\n",
                "# Check disk space\n",
                "try:\n",
                "    disk_usage = shutil.disk_usage('/')\n",
                "    free_gb = disk_usage.free / (1024**3)\n",
                "    used_percent = (disk_usage.used / disk_usage.total) * 100\n",
                "    \n",
                "    print(f\"   üíæ Disk space: {free_gb:.1f}GB free, {used_percent:.1f}% used\")\n",
                "    \n",
                "    if free_gb < 10:\n",
                "        print(\"   ‚ö†Ô∏è  WARNING: Low disk space!\")\n",
                "    elif used_percent > 85:\n",
                "        print(\"   ‚ö†Ô∏è  WARNING: High disk usage!\")\n",
                "    else:\n",
                "        print(\"   ‚úÖ Disk space OK\")\n",
                "except Exception as e:\n",
                "    print(f\"   ‚ùå Disk check failed: {e}\")\n",
                "\n",
                "# Check GPU memory\n",
                "print(\"\\nüéÆ GPU Memory Status:\")\n",
                "try:\n",
                "    result = subprocess.run(['nvidia-smi', '--query-gpu=memory.total,memory.used,memory.free', '--format=csv,noheader,nounits'], \n",
                "                          capture_output=True, text=True, timeout=10)\n",
                "    if result.returncode == 0:\n",
                "        gpu_memory = result.stdout.strip().split('\\n')\n",
                "        for i, memory in enumerate(gpu_memory):\n",
                "            total, used, free = memory.split(', ')\n",
                "            used_percent = (int(used) / int(total)) * 100\n",
                "            print(f\"   GPU {i}: {used}MB/{total}MB used ({used_percent:.1f}%)\")\n",
                "            \n",
                "            if used_percent > 90:\n",
                "                print(f\"      ‚ö†Ô∏è  WARNING: High GPU memory usage!\")\n",
                "            elif used_percent > 70:\n",
                "                print(f\"      ‚ö†Ô∏è  GPU memory getting full\")\n",
                "            else:\n",
                "                print(f\"      ‚úÖ GPU memory OK\")\n",
                "    else:\n",
                "        print(\"   ‚ö†Ô∏è  nvidia-smi not available\")\n",
                "except Exception as e:\n",
                "    print(f\"   ‚ùå GPU check failed: {e}\")\n",
                "\n",
                "# Check running processes\n",
                "print(\"\\nüîÑ Running Processes:\")\n",
                "try:\n",
                "    result = subprocess.run(['ps', 'aux'], capture_output=True, text=True, timeout=10)\n",
                "    if result.returncode == 0:\n",
                "        python_processes = [line for line in result.stdout.split('\\n') if 'python' in line and 'train' in line]\n",
                "        if python_processes:\n",
                "            print(\"   üîÑ Active training processes:\")\n",
                "            for process in python_processes[:3]:  # Show first 3\n",
                "                print(f\"      {process.strip()}\")\n",
                "        else:\n",
                "            print(\"   ‚úÖ No active training processes\")\n",
                "    else:\n",
                "        print(\"   ‚ùå Process check failed\")\n",
                "except Exception as e:\n",
                "    print(f\"   ‚ùå Process check failed: {e}\")\n",
                "\n",
                "# Cleanup options\n",
                "print(\"\\nüßπ Cleanup Options:\")\n",
                "print(\"   üìÅ Check large directories:\")\n",
                "\n",
                "directories_to_check = ['outputs', 'logs', 'models', 'data']\n",
                "for dir_name in directories_to_check:\n",
                "    dir_path = Path(dir_name)\n",
                "    if dir_path.exists():\n",
                "        total_size = sum(f.stat().st_size for f in dir_path.rglob('*') if f.is_file())\n",
                "        size_gb = total_size / (1024**3)\n",
                "        print(f\"      {dir_name}/: {size_gb:.2f}GB\")\n",
                "        \n",
                "        if size_gb > 5:\n",
                "            print(f\"         ‚ö†Ô∏è  Large directory - consider cleanup\")\n",
                "    else:\n",
                "        print(f\"      {dir_name}/: Not found\")\n",
                "\n",
                "# Manual backup\n",
                "print(\"\\n‚òÅÔ∏è  Manual Backup:\")\n",
                "print(\"   To manually backup to Google Drive:\")\n",
                "print(\"   ```bash\")\n",
                "print(\"   bash backup_to_gdrive.sh\")\n",
                "print(\"   ```\")\n",
                "\n",
                "# Performance tips\n",
                "print(\"\\n‚ö° Performance Tips:\")\n",
                "print(\"   ‚Ä¢ Use smaller batch size if GPU memory is low\")\n",
                "print(\"   ‚Ä¢ Reduce max_length if training is slow\")\n",
                "print(\"   ‚Ä¢ Enable gradient checkpointing for memory efficiency\")\n",
                "print(\"   ‚Ä¢ Use mixed precision training (fp16)\")\n",
                "print(\"   ‚Ä¢ Monitor GPU utilization with nvidia-smi\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*45)\n",
                "print(\"üîß Troubleshooting Complete!\")\n",
                "print(\"\" + \"=\"*45)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python (deberta-v3)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}

# Write the notebook
with open('SAMo_MultiDataset_Streamlined_CLEAN.ipynb', 'w') as f:
    json.dump(notebook, f, indent=2)

print("‚úÖ Clean notebook created: SAMo_MultiDataset_Streamlined_CLEAN.ipynb")